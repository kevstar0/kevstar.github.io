/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
!function(e,t){"object"==typeof exports&&"undefined"!=typeof module?t(exports):"function"==typeof define&&define.amd?define(["exports"],t):t((e=e||self).tf=e.tf||{})}(this,(function(e){"use strict";class t{constructor(e,t){this.backend=e,this.dataMover=t,this.data=new WeakMap,this.dataIdsCount=0}get(e){return this.data.has(e)||this.dataMover.moveData(this.backend,e),this.data.get(e)}set(e,t){this.dataIdsCount++,this.data.set(e,t)}has(e){return this.data.has(e)}delete(e){return this.dataIdsCount--,this.data.delete(e)}numDataIds(){return this.dataIdsCount}}class n{refCount(e){return s("refCount")}incRef(e){return s("incRef")}timerAvailable(){return!0}time(e){return s("time")}read(e){return s("read")}readSync(e){return s("readSync")}numDataIds(){return s("numDataIds")}disposeData(e,t){return s("disposeData")}write(e,t,n){return s("write")}move(e,t,n,r,a){return s("move")}memory(){return s("memory")}floatPrecision(){return s("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return s("dispose")}}function s(e){throw new Error(`'${e}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function r(e){let t=e.length,n=0,s=0;for(;t>0;)s=Math.random()*t|0,t--,n=e[t],e[t]=e[s],e[s]=n}function a(e,t,n){return Math.max(e,Math.min(t,n))}function i(e){return e%2==0?e:e+1}function o(e){let t=0;for(let n=0;n<e.length;n++)t+=e[n];return t}function l(e,t){if(!e)throw new Error("string"==typeof t?t:t())}function u(e,t,n=""){l(d(e,t),(()=>n+` Shapes ${e} and ${t} must match`))}function c(e){l(null!=e,(()=>"The input to the tensor constructor must be a non-null value."))}function h(e,t=[],n=!1){if(null==t&&(t=[]),Array.isArray(e)||$(e)&&!n)for(let s=0;s<e.length;++s)h(e[s],t,n);else t.push(e);return t}function p(e){if(0===e.length)return 1;let t=e[0];for(let n=1;n<e.length;n++)t*=e[n];return t}function d(e,t){if(e===t)return!0;if(null==e||null==t)return!1;if(e.length!==t.length)return!1;for(let n=0;n<e.length;n++)if(e[n]!==t[n])return!1;return!0}function f(e){return e%1==0}function m(e){const t=Math.ceil(Math.sqrt(e));return[t,Math.ceil(e/t)]}function g(e,t){return t<=e.length?e:e+" ".repeat(t-e.length)}function y(e,t=(e=>0),n){return new Promise(((s,r)=>{let a=0;const i=()=>{if(e())return void s();a++;const o=t(a);null!=n&&a>=n?r():setTimeout(i,o)};i()}))}function b(e,t){let n=1,s=-1;for(let t=0;t<e.length;++t)if(e[t]>=0)n*=e[t];else if(-1===e[t]){if(-1!==s)throw Error(`Shapes can only have 1 implicit size. Found -1 at dim ${s} and dim ${t}`);s=t}else if(e[t]<0)throw Error(`Shapes can not be < 0. Found ${e[t]} at dim ${t}`);if(-1===s){if(t>0&&t!==n)throw Error(`Size(${t}) must match the product of shape ${e}`);return e}if(0===n)throw Error(`Cannot infer the missing size in [${e}] when there are 0 elements`);if(t%n!=0)throw Error(`The implicit shape can't be a fractional number. Got ${t} / ${n}`);const r=e.slice();return r[s]=t/n,r}function x(e,t){const n=t.length;return l((e=null==e?t.map(((e,t)=>t)):[].concat(e)).every((e=>e>=-n&&e<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${e}`)),l(e.every((e=>f(e))),(()=>`All values in axis param must be integers but got axis ${e}`)),e.map((e=>e<0?n+e:e))}function w(e,t){const n=[],s=[],r=null!=t&&Array.isArray(t)&&0===t.length,a=null==t||r?null:x(t,e).sort();let i=0;for(let t=0;t<e.length;++t){if(null!=a){if(a[i]===t&&1!==e[t])throw new Error(`Can't squeeze axis ${t} since its dim '${e[t]}' is not 1`);(null==a[i]||a[i]>t)&&1===e[t]&&(n.push(e[t]),s.push(t)),a[i]<=t&&i++}1!==e[t]&&(n.push(e[t]),s.push(t))}return{newShape:n,keptDims:s}}function k(e,t){let n=null;if(null==e||"float32"===e)n=new Float32Array(t);else if("int32"===e)n=new Int32Array(t);else{if("bool"!==e)throw new Error(`Unknown data type ${e}`);n=new Uint8Array(t)}return n}function v(e,t){let n=null;if(null==e||"float32"===e)n=new Float32Array(t);else if("int32"===e)n=new Int32Array(t);else if("bool"===e)n=new Uint8Array(t);else{if("string"!==e)throw new Error(`Unknown data type ${e}`);n=new Array(t)}return n}function N(e,t){for(let n=0;n<e.length;n++){const s=e[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${t} being uploaded contains ${s}.`)}}function I(e){return"bool"===e||"complex64"===e||"float32"===e||"int32"===e||"string"===e}function S(e,t){return"complex64"!==t&&(("float32"!==t||"complex64"===e)&&(("int32"!==t||"float32"===e||"complex64"===e)&&("bool"!==t||"bool"!==e)))}function $(e){return e instanceof Float32Array||e instanceof Int32Array||e instanceof Uint8Array}function C(e){if("float32"===e||"int32"===e)return 4;if("complex64"===e)return 8;if("bool"===e)return 1;throw new Error(`Unknown dtype ${e}`)}function T(e){if(null==e)return 0;let t=0;return e.forEach((e=>t+=e.length)),t}function E(e){return"string"==typeof e||e instanceof String}function A(e){return"boolean"==typeof e}function R(e){return"number"==typeof e}function F(e){return Array.isArray(e)?F(e[0]):e instanceof Float32Array?"float32":e instanceof Int32Array||e instanceof Uint8Array?"int32":R(e)?"float32":E(e)?"string":A(e)?"bool":"float32"}function _(e){return!!(e&&e.constructor&&e.call&&e.apply)}function D(e,t){for(let n=t;n<e;++n)if(e%n==0)return n;return e}function O(e){const t=e.length;if(t<2)return[];const n=new Array(t-1);n[t-2]=e[t-1];for(let s=t-3;s>=0;--s)n[s]=n[s+1]*e[s+1];return n}function M(e,t,n,s=!1){const r=new Array;if(1===t.length){const a=t[0]*(s?2:1);for(let t=0;t<a;t++)r[t]=n[e+t]}else{const a=t[0],i=t.slice(1),o=i.reduce(((e,t)=>e*t))*(s?2:1);for(let t=0;t<a;t++)r[t]=M(e+t*o,i,n,s)}return r}function L(e,t,n=!1){if(0===e.length)return t[0];const s=e.reduce(((e,t)=>e*t))*(n?2:1);if(0===s)return[];if(s!==t.length)throw new Error(`[${e}] does not match the input size ${t.length}${n?" for a complex tensor":""}.`);return M(0,e,t,n)}function z(e,t){const n=B(e,t);for(let e=0;e<n.length;e++)n[e]=1;return n}function B(e,t){if(null==t||"float32"===t||"complex64"===t)return new Float32Array(e);if("int32"===t)return new Int32Array(e);if("bool"===t)return new Uint8Array(e);throw new Error(`Unknown data type ${t}`)}function P(e,t){const n=e.reduce(((e,t)=>e*t),1);if(null==t||"float32"===t)return L(e,new Float32Array(n));if("int32"===t)return L(e,new Int32Array(n));if("bool"===t)return L(e,new Uint8Array(n));throw new Error(`Unknown data type ${t}`)}function W(e){e.forEach((t=>{l(Number.isInteger(t)&&t>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${e}].`))}))}function V(e,t,n){if(0===t)return 0;if(1===t)return e[0];let s=e[e.length-1];for(let t=0;t<e.length-1;++t)s+=n[t]*e[t];return s}function U(e,t,n){if(0===t)return[];if(1===t)return[e];const s=new Array(t);for(let t=0;t<s.length-1;++t)s[t]=Math.floor(e/n[t]),e-=s[t]*n[t];return s[s.length-1]=e,s}function G(e){return e&&e.then&&"function"==typeof e.then}const H="tfjsflags";class j{constructor(e){this.global=e,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=q,this.populateURLFlags()}setPlatform(e,t){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${t}.`),this.platformName=e,this.platform=t}registerFlag(e,t,n){if(this.flagRegistry[e]={evaluationFn:t,setHook:n},null!=this.urlFlags[e]){const t=this.urlFlags[e];console.warn(`Setting feature override from URL ${e}: ${t}.`),this.set(e,t)}}async getAsync(e){return e in this.flags||(this.flags[e]=await this.evaluateFlag(e)),this.flags[e]}get(e){if(e in this.flags)return this.flags[e];const t=this.evaluateFlag(e);if(G(t))throw new Error(`Flag ${e} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[e]=t,this.flags[e]}getNumber(e){return this.get(e)}getBool(e){return this.get(e)}getFlags(){return this.flags}get features(){return this.flags}set(e,t){if(null==this.flagRegistry[e])throw new Error(`Cannot set flag ${e} as it has not been registered.`);this.flags[e]=t,null!=this.flagRegistry[e].setHook&&this.flagRegistry[e].setHook(t)}evaluateFlag(e){if(null==this.flagRegistry[e])throw new Error(`Cannot evaluate flag '${e}': no evaluation function found.`);return this.flagRegistry[e].evaluationFn()}setFlags(e){this.flags=Object.assign({},e)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const e=this.getQueryParams(this.global.location.search);if(H in e){e.tfjsflags.split(",").forEach((e=>{const[t,n]=e.split(":");this.urlFlags[t]=function(e,t){if("true"===(t=t.toLowerCase())||"false"===t)return"true"===t;if(""+ +t===t)return+t;throw new Error(`Could not parse value flag value ${t} for flag ${e}.`)}(t,n)}))}}}function q(e){const t={};return e.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((e,...n)=>(function(e,t,n){e[decodeURIComponent(t)]=decodeURIComponent(n||"")}(t,n[0],n[1]),n.join("=")))),t}function K(){return e.ENV}let X;function Y(){if(null==X){let e;if("undefined"!=typeof window)e=window;else if("undefined"!=typeof global)e=global;else if("undefined"!=typeof process)e=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");e=self}X=e}return X}function Z(e,t){const n=function(){const e=Y();return null==e._tfGlobals&&(e._tfGlobals=new Map),e._tfGlobals}();if(n.has(e))return n.get(e);{const s=t();return n.set(e,s),n.get(e)}}e.ENV=null;const J="Abs",Q="Acos",ee="Acosh",te="Add",ne="AddN",se="All",re="Any",ae="ArgMax",ie="ArgMin",oe="Asin",le="Asinh",ue="Atan",ce="Atanh",he="Atan2",pe="AvgPool",de="AvgPoolGrad",fe="AvgPool3D",me="AvgPool3DGrad",ge="BatchMatMul",ye="BatchToSpaceND",be="Bincount",xe="BroadcastTo",we="Cast",ke="Ceil",ve="ClipByValue",Ne="Complex",Ie="ComplexAbs",Se="Concat",$e="Conv2D",Ce="Conv2DBackpropFilter",Te="Conv2DBackpropInput",Ee="Conv3D",Ae="Conv3DBackpropFilterV2",Re="Conv3DBackpropInputV2",Fe="Cos",_e="Cosh",De="Cumsum",Oe="CropAndResize",Me="DenseBincount",Le="DepthToSpace",ze="DepthwiseConv2dNative",Be="DepthwiseConv2dNativeBackpropFilter",Pe="DepthwiseConv2dNativeBackpropInput",We="Diag",Ve="Dilation2D",Ue="Dilation2DBackpropInput",Ge="Dilation2DBackpropFilter",He="RealDiv",je="Einsum",qe="Elu",Ke="EluGrad",Xe="Erf",Ye="Equal",Ze="Exp",Je="ExpandDims",Qe="Expm1",et="FFT",tt="Fill",nt="FlipLeftRight",st="Floor",rt="FloorDiv",at="FusedBatchNorm",it="GatherV2",ot="GatherNd",lt="Greater",ut="GreaterEqual",ct="Identity",ht="IFFT",pt="Imag",dt="IsFinite",ft="IsInf",mt="IsNan",gt="LeakyRelu",yt="Less",bt="LessEqual",xt="LinSpace",wt="Log",kt="Log1p",vt="LogicalAnd",Nt="LogicalNot",It="LogicalOr",St="LogSoftmax",$t="LRN",Ct="LRNGrad",Tt="Max",Et="Maximum",At="MaxPool",Rt="MaxPoolGrad",Ft="MaxPool3D",_t="MaxPool3DGrad",Dt="MaxPoolWithArgmax",Ot="Mean",Mt="Min",Lt="Minimum",zt="MirrorPad",Bt="Mod",Pt="Multinomial",Wt="Multiply",Vt="Neg",Ut="NotEqual",Gt="NonMaxSuppressionV3",Ht="NonMaxSuppressionV4",jt="NonMaxSuppressionV5",qt="OnesLike",Kt="OneHot",Xt="Pack",Yt="PadV2",Zt="Pow",Jt="Prelu",Qt="Prod",en="Range",tn="Real",nn="Reciprocal",sn="Relu",rn="Reshape",an="ResizeNearestNeighbor",on="ResizeNearestNeighborGrad",ln="ResizeBilinear",un="ResizeBilinearGrad",cn="Relu6",hn="Reverse",pn="Round",dn="Rsqrt",fn="ScatterNd",mn="Select",gn="Selu",yn="Slice",bn="Sin",xn="Sinh",wn="Sign",kn="Sigmoid",vn="Softplus",Nn="Sqrt",In="Sum",Sn="SpaceToBatchND",$n="SplitV",Cn="Softmax",Tn="SparseFillEmptyRows",En="SparseReshape",An="SparseSegmentMean",Rn="SparseSegmentSum",Fn="SparseToDense",_n="SquaredDifference",Dn="Square",On="StridedSlice",Mn="StringNGrams",Ln="StringSplit",zn="StringToHashBucketFast",Bn="Sub",Pn="Tan",Wn="Tanh",Vn="Tile",Un="TopK",Gn="Transform",Hn="Transpose",jn="Unique",qn="Unpack",Kn="UnsortedSegmentSum",Xn="ZerosLike",Yn="Step",Zn="FromPixels",Jn="RotateWithOffset",Qn="_FusedMatMul",es="FusedConv2D",ts="FusedDepthwiseConv2D",ns=Z("kernelRegistry",(()=>new Map)),ss=Z("gradRegistry",(()=>new Map));function rs(e,t){const n=us(e,t);return ns.get(n)}function as(e){return ss.get(e)}function is(e){const t=ns.entries(),n=[];for(;;){const{done:s,value:r}=t.next();if(s)break;const[a,i]=r,[o]=a.split("_");o===e&&n.push(i)}return n}function os(e){const{kernelName:t,backendName:n}=e,s=us(t,n);ns.has(s)&&console.warn(`The kernel '${t}' for backend '${n}' is already registered`),ns.set(s,e)}function ls(e){const{kernelName:t}=e;ss.has(t)&&K().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${t}'`),ss.set(t,e)}function us(e,t){return`${t}_${e}`}var cs=ps,hs=null;try{hs=new WebAssembly.Instance(new WebAssembly.Module(new Uint8Array([0,97,115,109,1,0,0,0,1,13,2,96,0,1,127,96,4,127,127,127,127,1,127,3,7,6,0,1,1,1,1,1,6,6,1,127,1,65,0,11,7,50,6,3,109,117,108,0,1,5,100,105,118,95,115,0,2,5,100,105,118,95,117,0,3,5,114,101,109,95,115,0,4,5,114,101,109,95,117,0,5,8,103,101,116,95,104,105,103,104,0,0,10,191,1,6,4,0,35,0,11,36,1,1,126,32,0,173,32,1,173,66,32,134,132,32,2,173,32,3,173,66,32,134,132,126,34,4,66,32,135,167,36,0,32,4,167,11,36,1,1,126,32,0,173,32,1,173,66,32,134,132,32,2,173,32,3,173,66,32,134,132,127,34,4,66,32,135,167,36,0,32,4,167,11,36,1,1,126,32,0,173,32,1,173,66,32,134,132,32,2,173,32,3,173,66,32,134,132,128,34,4,66,32,135,167,36,0,32,4,167,11,36,1,1,126,32,0,173,32,1,173,66,32,134,132,32,2,173,32,3,173,66,32,134,132,129,34,4,66,32,135,167,36,0,32,4,167,11,36,1,1,126,32,0,173,32,1,173,66,32,134,132,32,2,173,32,3,173,66,32,134,132,130,34,4,66,32,135,167,36,0,32,4,167,11])),{}).exports}catch(e){}function ps(e,t,n){this.low=0|e,this.high=0|t,this.unsigned=!!n}function ds(e){return!0===(e&&e.__isLong__)}ps.prototype.__isLong__,Object.defineProperty(ps.prototype,"__isLong__",{value:!0}),ps.isLong=ds;var fs={},ms={};function gs(e,t){var n,s,r;return t?(r=0<=(e>>>=0)&&e<256)&&(s=ms[e])?s:(n=bs(e,(0|e)<0?-1:0,!0),r&&(ms[e]=n),n):(r=-128<=(e|=0)&&e<128)&&(s=fs[e])?s:(n=bs(e,e<0?-1:0,!1),r&&(fs[e]=n),n)}function ys(e,t){if(isNaN(e))return t?Cs:$s;if(t){if(e<0)return Cs;if(e>=Ns)return Fs}else{if(e<=-Is)return _s;if(e+1>=Is)return Rs}return e<0?ys(-e,t).neg():bs(e%vs|0,e/vs|0,t)}function bs(e,t,n){return new ps(e,t,n)}ps.fromInt=gs,ps.fromNumber=ys,ps.fromBits=bs;var xs=Math.pow;function ws(e,t,n){if(0===e.length)throw Error("empty string");if("NaN"===e||"Infinity"===e||"+Infinity"===e||"-Infinity"===e)return $s;if("number"==typeof t?(n=t,t=!1):t=!!t,(n=n||10)<2||36<n)throw RangeError("radix");var s;if((s=e.indexOf("-"))>0)throw Error("interior hyphen");if(0===s)return ws(e.substring(1),t,n).neg();for(var r=ys(xs(n,8)),a=$s,i=0;i<e.length;i+=8){var o=Math.min(8,e.length-i),l=parseInt(e.substring(i,i+o),n);if(o<8){var u=ys(xs(n,o));a=a.mul(u).add(ys(l))}else a=(a=a.mul(r)).add(ys(l))}return a.unsigned=t,a}function ks(e,t){return"number"==typeof e?ys(e,t):"string"==typeof e?ws(e,t):bs(e.low,e.high,"boolean"==typeof t?t:e.unsigned)}ps.fromString=ws,ps.fromValue=ks;var vs=4294967296,Ns=vs*vs,Is=Ns/2,Ss=gs(1<<24),$s=gs(0);ps.ZERO=$s;var Cs=gs(0,!0);ps.UZERO=Cs;var Ts=gs(1);ps.ONE=Ts;var Es=gs(1,!0);ps.UONE=Es;var As=gs(-1);ps.NEG_ONE=As;var Rs=bs(-1,2147483647,!1);ps.MAX_VALUE=Rs;var Fs=bs(-1,-1,!0);ps.MAX_UNSIGNED_VALUE=Fs;var _s=bs(0,-2147483648,!1);ps.MIN_VALUE=_s;var Ds=ps.prototype;Ds.toInt=function(){return this.unsigned?this.low>>>0:this.low},Ds.toNumber=function(){return this.unsigned?(this.high>>>0)*vs+(this.low>>>0):this.high*vs+(this.low>>>0)},Ds.toString=function(e){if((e=e||10)<2||36<e)throw RangeError("radix");if(this.isZero())return"0";if(this.isNegative()){if(this.eq(_s)){var t=ys(e),n=this.div(t),s=n.mul(t).sub(this);return n.toString(e)+s.toInt().toString(e)}return"-"+this.neg().toString(e)}for(var r=ys(xs(e,6),this.unsigned),a=this,i="";;){var o=a.div(r),l=(a.sub(o.mul(r)).toInt()>>>0).toString(e);if((a=o).isZero())return l+i;for(;l.length<6;)l="0"+l;i=""+l+i}},Ds.getHighBits=function(){return this.high},Ds.getHighBitsUnsigned=function(){return this.high>>>0},Ds.getLowBits=function(){return this.low},Ds.getLowBitsUnsigned=function(){return this.low>>>0},Ds.getNumBitsAbs=function(){if(this.isNegative())return this.eq(_s)?64:this.neg().getNumBitsAbs();for(var e=0!=this.high?this.high:this.low,t=31;t>0&&0==(e&1<<t);t--);return 0!=this.high?t+33:t+1},Ds.isZero=function(){return 0===this.high&&0===this.low},Ds.eqz=Ds.isZero,Ds.isNegative=function(){return!this.unsigned&&this.high<0},Ds.isPositive=function(){return this.unsigned||this.high>=0},Ds.isOdd=function(){return 1==(1&this.low)},Ds.isEven=function(){return 0==(1&this.low)},Ds.equals=function(e){return ds(e)||(e=ks(e)),(this.unsigned===e.unsigned||this.high>>>31!=1||e.high>>>31!=1)&&(this.high===e.high&&this.low===e.low)},Ds.eq=Ds.equals,Ds.notEquals=function(e){return!this.eq(e)},Ds.neq=Ds.notEquals,Ds.ne=Ds.notEquals,Ds.lessThan=function(e){return this.comp(e)<0},Ds.lt=Ds.lessThan,Ds.lessThanOrEqual=function(e){return this.comp(e)<=0},Ds.lte=Ds.lessThanOrEqual,Ds.le=Ds.lessThanOrEqual,Ds.greaterThan=function(e){return this.comp(e)>0},Ds.gt=Ds.greaterThan,Ds.greaterThanOrEqual=function(e){return this.comp(e)>=0},Ds.gte=Ds.greaterThanOrEqual,Ds.ge=Ds.greaterThanOrEqual,Ds.compare=function(e){if(ds(e)||(e=ks(e)),this.eq(e))return 0;var t=this.isNegative(),n=e.isNegative();return t&&!n?-1:!t&&n?1:this.unsigned?e.high>>>0>this.high>>>0||e.high===this.high&&e.low>>>0>this.low>>>0?-1:1:this.sub(e).isNegative()?-1:1},Ds.comp=Ds.compare,Ds.negate=function(){return!this.unsigned&&this.eq(_s)?_s:this.not().add(Ts)},Ds.neg=Ds.negate,Ds.add=function(e){ds(e)||(e=ks(e));var t=this.high>>>16,n=65535&this.high,s=this.low>>>16,r=65535&this.low,a=e.high>>>16,i=65535&e.high,o=e.low>>>16,l=0,u=0,c=0,h=0;return c+=(h+=r+(65535&e.low))>>>16,u+=(c+=s+o)>>>16,l+=(u+=n+i)>>>16,l+=t+a,bs((c&=65535)<<16|(h&=65535),(l&=65535)<<16|(u&=65535),this.unsigned)},Ds.subtract=function(e){return ds(e)||(e=ks(e)),this.add(e.neg())},Ds.sub=Ds.subtract,Ds.multiply=function(e){if(this.isZero())return $s;if(ds(e)||(e=ks(e)),hs)return bs(hs.mul(this.low,this.high,e.low,e.high),hs.get_high(),this.unsigned);if(e.isZero())return $s;if(this.eq(_s))return e.isOdd()?_s:$s;if(e.eq(_s))return this.isOdd()?_s:$s;if(this.isNegative())return e.isNegative()?this.neg().mul(e.neg()):this.neg().mul(e).neg();if(e.isNegative())return this.mul(e.neg()).neg();if(this.lt(Ss)&&e.lt(Ss))return ys(this.toNumber()*e.toNumber(),this.unsigned);var t=this.high>>>16,n=65535&this.high,s=this.low>>>16,r=65535&this.low,a=e.high>>>16,i=65535&e.high,o=e.low>>>16,l=65535&e.low,u=0,c=0,h=0,p=0;return h+=(p+=r*l)>>>16,c+=(h+=s*l)>>>16,h&=65535,c+=(h+=r*o)>>>16,u+=(c+=n*l)>>>16,c&=65535,u+=(c+=s*o)>>>16,c&=65535,u+=(c+=r*i)>>>16,u+=t*l+n*o+s*i+r*a,bs((h&=65535)<<16|(p&=65535),(u&=65535)<<16|(c&=65535),this.unsigned)},Ds.mul=Ds.multiply,Ds.divide=function(e){if(ds(e)||(e=ks(e)),e.isZero())throw Error("division by zero");var t,n,s;if(hs)return this.unsigned||-2147483648!==this.high||-1!==e.low||-1!==e.high?bs((this.unsigned?hs.div_u:hs.div_s)(this.low,this.high,e.low,e.high),hs.get_high(),this.unsigned):this;if(this.isZero())return this.unsigned?Cs:$s;if(this.unsigned){if(e.unsigned||(e=e.toUnsigned()),e.gt(this))return Cs;if(e.gt(this.shru(1)))return Es;s=Cs}else{if(this.eq(_s))return e.eq(Ts)||e.eq(As)?_s:e.eq(_s)?Ts:(t=this.shr(1).div(e).shl(1)).eq($s)?e.isNegative()?Ts:As:(n=this.sub(e.mul(t)),s=t.add(n.div(e)));if(e.eq(_s))return this.unsigned?Cs:$s;if(this.isNegative())return e.isNegative()?this.neg().div(e.neg()):this.neg().div(e).neg();if(e.isNegative())return this.div(e.neg()).neg();s=$s}for(n=this;n.gte(e);){t=Math.max(1,Math.floor(n.toNumber()/e.toNumber()));for(var r=Math.ceil(Math.log(t)/Math.LN2),a=r<=48?1:xs(2,r-48),i=ys(t),o=i.mul(e);o.isNegative()||o.gt(n);)o=(i=ys(t-=a,this.unsigned)).mul(e);i.isZero()&&(i=Ts),s=s.add(i),n=n.sub(o)}return s},Ds.div=Ds.divide,Ds.modulo=function(e){return ds(e)||(e=ks(e)),hs?bs((this.unsigned?hs.rem_u:hs.rem_s)(this.low,this.high,e.low,e.high),hs.get_high(),this.unsigned):this.sub(this.div(e).mul(e))},Ds.mod=Ds.modulo,Ds.rem=Ds.modulo,Ds.not=function(){return bs(~this.low,~this.high,this.unsigned)},Ds.and=function(e){return ds(e)||(e=ks(e)),bs(this.low&e.low,this.high&e.high,this.unsigned)},Ds.or=function(e){return ds(e)||(e=ks(e)),bs(this.low|e.low,this.high|e.high,this.unsigned)},Ds.xor=function(e){return ds(e)||(e=ks(e)),bs(this.low^e.low,this.high^e.high,this.unsigned)},Ds.shiftLeft=function(e){return ds(e)&&(e=e.toInt()),0==(e&=63)?this:e<32?bs(this.low<<e,this.high<<e|this.low>>>32-e,this.unsigned):bs(0,this.low<<e-32,this.unsigned)},Ds.shl=Ds.shiftLeft,Ds.shiftRight=function(e){return ds(e)&&(e=e.toInt()),0==(e&=63)?this:e<32?bs(this.low>>>e|this.high<<32-e,this.high>>e,this.unsigned):bs(this.high>>e-32,this.high>=0?0:-1,this.unsigned)},Ds.shr=Ds.shiftRight,Ds.shiftRightUnsigned=function(e){if(ds(e)&&(e=e.toInt()),0===(e&=63))return this;var t=this.high;return e<32?bs(this.low>>>e|t<<32-e,t>>>e,this.unsigned):bs(32===e?t:t>>>e-32,0,this.unsigned)},Ds.shru=Ds.shiftRightUnsigned,Ds.shr_u=Ds.shiftRightUnsigned,Ds.toSigned=function(){return this.unsigned?bs(this.low,this.high,!1):this},Ds.toUnsigned=function(){return this.unsigned?this:bs(this.low,this.high,!0)},Ds.toBytes=function(e){return e?this.toBytesLE():this.toBytesBE()},Ds.toBytesLE=function(){var e=this.high,t=this.low;return[255&t,t>>>8&255,t>>>16&255,t>>>24,255&e,e>>>8&255,e>>>16&255,e>>>24]},Ds.toBytesBE=function(){var e=this.high,t=this.low;return[e>>>24,e>>>16&255,e>>>8&255,255&e,t>>>24,t>>>16&255,t>>>8&255,255&t]},ps.fromBytes=function(e,t,n){return n?ps.fromBytesLE(e,t):ps.fromBytesBE(e,t)},ps.fromBytesLE=function(e,t){return new ps(e[0]|e[1]<<8|e[2]<<16|e[3]<<24,e[4]|e[5]<<8|e[6]<<16|e[7]<<24,t)},ps.fromBytesBE=function(e,t){return new ps(e[4]<<24|e[5]<<16|e[6]<<8|e[7],e[0]<<24|e[1]<<16|e[2]<<8|e[3],t)};const Os=cs||Object.freeze({__proto__:null,default:cs,__moduleExports:cs});function Ms(e){return Os.fromString(e,!0,16)}const Ls=Ms("c3a5c85c97cb3127"),zs=Ms("b492b66fbe98f273"),Bs=Ms("9ae16a3b2f90404f");function Ps(e){return e.xor(e.shru(47))}function Ws(e,t,n){const s=e.slice(t,t+n);return Os.fromBytes(Array.from(s),!0,!0)}function Vs(e,t){return Ws(e,t,8)}function Us(e,t){return Ws(e,t,4)}function Gs(e,t){return 0===t?e:e.shru(t).or(e.shl(64-t))}function Hs(e,t,n=Ms("9ddfea08eb382d69")){let s=e.xor(t).mul(n);s=s.xor(s.shru(47));let r=t.xor(s).mul(n);return r=r.xor(r.shru(47)),r=r.mul(n),r}function js(e,t,n,s){return function(e,t,n,s,r,a){r=r.add(e),a=Gs(a.add(r).add(s),21);const i=r;return r=(r=r.add(t)).add(n),a=a.add(Gs(r,44)),[r.add(s),a.add(i)]}(Vs(e,t),Vs(e,t+8),Vs(e,t+16),Vs(e,t+24),n,s)}function qs(e,t=e.length){const n=Os.fromNumber(81,!0);if(t<=32)return t<=16?function(e,t=e.length){if(t>=8){const n=Bs.add(2*t),s=Vs(e,0).add(Bs),r=Vs(e,t-8);return Hs(Gs(r,37).mul(n).add(s),Gs(s,25).add(r).mul(n),n)}if(t>=4){const n=Bs.add(2*t);return Hs(Us(e,0).shl(3).add(t),Us(e,t-4),n)}if(t>0){const n=e[0]+(e[t>>1]<<8),s=t+(e[t-1]<<2);return Ps(Bs.mul(n).xor(Ls.mul(s))).mul(Bs)}return Bs}(e,t):function(e,t=e.length){const n=Bs.add(2*t),s=Vs(e,0).mul(zs),r=Vs(e,8),a=Vs(e,t-8).mul(n),i=Vs(e,t-16).mul(Bs);return Hs(Gs(s.add(r),43).add(Gs(a,30)).add(i),s.add(Gs(r.add(Bs),18)).add(a),n)}(e,t);if(t<=64)return function(e,t=e.length){const n=Bs.add(2*t),s=Vs(e,0).mul(Bs),r=Vs(e,8),a=Vs(e,t-8).mul(n),i=Vs(e,t-16).mul(Bs),o=Gs(s.add(r),43).add(Gs(a,30)).add(i),l=Hs(o,s.add(Gs(r.add(Bs),18)).add(a),n),u=Vs(e,16).mul(n),c=Vs(e,24),h=o.add(Vs(e,t-32)).mul(n),p=l.add(Vs(e,t-24)).mul(n);return Hs(Gs(u.add(c),43).add(Gs(h,30)).add(p),u.add(Gs(c.add(s),18)).add(h),n)}(e,t);let s=n,r=n.mul(zs).add(113),a=Ps(r.mul(Bs).add(113)).mul(Bs),i=[Os.UZERO,Os.UZERO],o=[Os.UZERO,Os.UZERO];s=s.mul(Bs).add(Vs(e,0));let l=0;const u=64*(t-1>>6),c=u+(t-1&63)-63;do{s=Gs(s.add(r).add(i[0]).add(Vs(e,l+8)),37).mul(zs),r=Gs(r.add(i[1]).add(Vs(e,l+48)),42).mul(zs),s=s.xor(o[1]),r=r.add(i[0]).add(Vs(e,l+40)),a=Gs(a.add(o[0]),33).mul(zs),i=js(e,l,i[1].mul(zs),s.add(o[0])),o=js(e,l+32,a.add(o[1]),r.add(Vs(e,l+16))),[a,s]=[s,a],l+=64}while(l!==u);const h=zs.add(a.and(255).shl(1));return l=c,o[0]=o[0].add(t-1&63),i[0]=i[0].add(o[0]),o[0]=o[0].add(i[0]),s=Gs(s.add(r).add(i[0]).add(Vs(e,l+8)),37).mul(h),r=Gs(r.add(i[1]).add(Vs(e,l+48)),42).mul(h),s=s.xor(o[1].mul(9)),r=r.add(i[0].mul(9).add(Vs(e,l+40))),a=Gs(a.add(o[0]),33).mul(h),i=js(e,l,i[1].mul(h),s.add(o[0])),o=js(e,l+32,a.add(o[1]),r.add(Vs(e,l+16))),[a,s]=[s,a],Hs(Hs(i[0],o[0],h).add(Ps(r).mul(Ls)).add(a),Hs(i[1],o[1],h).add(s),h)}function Ks(e,t){return"string"===t?Js(e):Xs([e],t)}function Xs(e,t){if("string"===t)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(e)&&(e=h(e)),K().getBool("DEBUG")&&N(e,t),function(e,t){return e instanceof Float32Array&&"float32"===t||e instanceof Int32Array&&"int32"===t||e instanceof Uint8Array&&"bool"===t}(e,t))return e;if(null==t||"float32"===t||"complex64"===t)return new Float32Array(e);if("int32"===t)return new Int32Array(e);if("bool"===t){const t=new Uint8Array(e.length);for(let n=0;n<t.length;++n)0!==Math.round(e[n])&&(t[n]=1);return t}throw new Error(`Unknown data type ${t}`)}function Ys(){return K().platform.now()}function Zs(e,t){return K().platform.fetch(e,t)}function Js(e,t="utf-8"){return t=t||"utf-8",K().platform.encode(e,t)}function Qs(e,t="utf-8"){return t=t||"utf-8",K().platform.decode(e,t)}var er=Object.freeze({__proto__:null,createScalarValue:Ks,toTypedArray:Xs,now:Ys,fetch:Zs,encodeString:Js,decodeString:Qs,shuffle:r,shuffleCombo:function(e,t){if(e.length!==t.length)throw new Error(`Array sizes must match to be shuffled together First array length was ${e.length}Second array length was ${t.length}`);let n,s,r=e.length,a=0;for(;r>0;)a=Math.random()*r|0,r--,n=e[r],s=t[r],e[r]=e[a],t[r]=t[a],e[a]=n,t[a]=s},clamp:a,nearestLargerEven:i,sum:o,randUniform:function(e,t){const n=Math.random();return t*n+(1-n)*e},distSquared:function(e,t){let n=0;for(let s=0;s<e.length;s++){const r=Number(e[s])-Number(t[s]);n+=r*r}return n},assert:l,assertShapesMatch:u,assertNonNull:c,flatten:h,sizeFromShape:p,isScalarShape:function(e){return 0===e.length},arraysEqual:d,isInt:f,tanh:function(e){if(null!=Math.tanh)return Math.tanh(e);if(e===1/0)return 1;if(e===-1/0)return-1;{const t=Math.exp(2*e);return(t-1)/(t+1)}},sizeToSquarishShape:m,createShuffledIndices:function(e){const t=new Uint32Array(e);for(let n=0;n<e;++n)t[n]=n;return r(t),t},rightPad:g,repeatedTry:y,inferFromImplicitShape:b,parseAxisParam:x,squeezeShape:w,getTypedArrayFromDType:k,getArrayFromDType:v,checkConversionForErrors:N,isValidDtype:I,hasEncodingLoss:S,isTypedArray:$,bytesPerElement:C,bytesFromStringArray:T,isString:E,isBoolean:A,isNumber:R,inferDtype:F,isFunction:_,nearestDivisor:D,computeStrides:O,toNestedArray:L,makeOnesTypedArray:z,makeZerosTypedArray:B,makeZerosNestedTypedArray:P,assertNonNegativeIntegerDimensions:W,locToIndex:V,indexToLoc:U,isPromise:G,hexToLong:Ms,fingerPrint64:qs});class tr{constructor(e,t){this.backendTimer=e,this.logger=t,null==t&&(this.logger=new sr)}profileKernel(e,t,n){let s;const r=()=>{s=n()};let a;const i=Ys();if(this.backendTimer.timerAvailable())a=this.backendTimer.time(r);else{r();for(const e of s)e.dataSync();a=Promise.resolve({kernelMs:Ys()-i})}if(K().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let t=0;t<s.length;t++){const n=s[t];n.data().then((t=>{nr(t,n.dtype,e)}))}return{kernelName:e,outputs:s,inputs:t,timeMs:a.then((e=>e.kernelMs)),extraInfo:a.then((e=>null!=e.getExtraProfileInfo?e.getExtraProfileInfo():""))}}logKernelProfile(e){const{kernelName:t,outputs:n,timeMs:s,inputs:r,extraInfo:a}=e;n.forEach((e=>{Promise.all([e.data(),s,a]).then((n=>{this.logger.logKernelProfile(t,e,n[0],n[1],r,n[2])}))}))}}function nr(e,t,n){if("float32"!==t)return!1;for(let t=0;t<e.length;t++){const s=e[t];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class sr{logKernelProfile(e,t,n,s,r,a){const i="number"==typeof s?g(`${s}ms`,9):s.error,o=g(e,25),l=t.rank,u=t.size,c=g(t.shape.toString(),14);let h="";for(const e in r){const n=r[e];if(null!=n){const s=n.shape||t.shape,r=s.length;h+=`${e}: ${r}D ${r>0?s:""} `}}console.log(`%c${o}\t%c${i}\t%c${l}D ${c}\t%c${u}\t%c${h}\t%c${a}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function rr(e,t,n,s){const r=O(t),a=function(e,t,n,s){const r=p(t),a=s[s.length-1],i=new Array(a).fill(0),o=t.length,l="complex64"===n?lr(e):e;if(o>1)for(let e=0;e<r/a;e++){const t=e*a;for(let e=0;e<a;e++)i[e]=Math.max(i[e],ar(l[t+e],0,n).length)}return i}(e,t,n,r),i=t.length,o=or(e,t,n,r,a),l=["Tensor"];return s&&(l.push(`  dtype: ${n}`),l.push(`  rank: ${i}`),l.push(`  shape: [${t}]`),l.push("  values:")),l.push(o.map((e=>"    "+e)).join("\n")),l.join("\n")}function ar(e,t,n){let s;return s=Array.isArray(e)?`${parseFloat(e[0].toFixed(7))} + ${parseFloat(e[1].toFixed(7))}j`:E(e)?`'${e}'`:"bool"===n?ir(e):parseFloat(e.toFixed(7)).toString(),g(s,t)}function ir(e){return 0===e?"false":"true"}function or(e,t,n,s,r,a=!0){const i="complex64"===n?2:1,o=t[0],l=t.length;if(0===l){if("complex64"===n){return[ar(lr(e)[0],0,n)]}return"bool"===n?[ir(e[0])]:[e[0].toString()]}if(1===l){if(o>20){const t=3*i;let s=Array.from(e.slice(0,t)),a=Array.from(e.slice((o-3)*i,o*i));return"complex64"===n&&(s=lr(s),a=lr(a)),["["+s.map(((e,t)=>ar(e,r[t],n))).join(", ")+", ..., "+a.map(((e,t)=>ar(e,r[o-3+t],n))).join(", ")+"]"]}return["["+("complex64"===n?lr(e):Array.from(e)).map(((e,t)=>ar(e,r[t],n))).join(", ")+"]"]}const u=t.slice(1),c=s.slice(1),h=s[0]*i,p=[];if(o>20){for(let t=0;t<3;t++){const s=t*h,a=s+h;p.push(...or(e.slice(s,a),u,n,c,r,!1))}p.push("...");for(let t=o-3;t<o;t++){const s=t*h,a=s+h;p.push(...or(e.slice(s,a),u,n,c,r,t===o-1))}}else for(let t=0;t<o;t++){const s=t*h,a=s+h;p.push(...or(e.slice(s,a),u,n,c,r,t===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let e=1;e<p.length-1;e++)p[e]=" "+p[e]+d;let f=",\n";for(let e=2;e<l;e++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(a?"":f),p}function lr(e){const t=[];for(let n=0;n<e.length;n+=2)t.push([e[n],e[n+1]]);return t}class ur{constructor(e,t,n){if(this.dtype=t,this.shape=e.slice(),this.size=p(e),null!=n){const e=n.length;l(e===this.size,(()=>`Length of values '${e}' does not match the size inferred by the shape '${this.size}'.`))}if("complex64"===t)throw new Error("complex64 dtype TensorBuffers are not supported. Please create a TensorBuffer for the real and imaginary parts separately and call tf.complex(real, imag).");this.values=n||v(t,this.size),this.strides=O(e)}set(e,...t){0===t.length&&(t=[0]),l(t.length===this.rank,(()=>`The number of provided coordinates (${t.length}) must match the rank (${this.rank})`));const n=this.locToIndex(t);this.values[n]=e}get(...e){0===e.length&&(e=[0]);let t=0;for(const n of e){if(n<0||n>=this.shape[t]){const t=`Requested out of range element at ${e}.   Buffer shape=${this.shape}`;throw new Error(t)}t++}let n=e[e.length-1];for(let t=0;t<e.length-1;++t)n+=this.strides[t]*e[t];return this.values[n]}locToIndex(e){if(0===this.rank)return 0;if(1===this.rank)return e[0];let t=e[e.length-1];for(let n=0;n<e.length-1;++n)t+=this.strides[n]*e[n];return t}indexToLoc(e){if(0===this.rank)return[];if(1===this.rank)return[e];const t=new Array(this.shape.length);for(let n=0;n<t.length-1;++n)t[n]=Math.floor(e/this.strides[n]),e-=t[n]*this.strides[n];return t[t.length-1]=e,t}get rank(){return this.shape.length}toTensor(){return cr().makeTensor(this.values,this.shape,this.dtype)}}let cr=null,hr=null,pr=null;class dr{constructor(e,t,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=e.slice(),this.dtype=t||"float32",this.size=p(e),this.strides=O(e),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const e=await this.data();return hr.buffer(this.shape,this.dtype,e)}bufferSync(){return hr.buffer(this.shape,this.dtype,this.dataSync())}async array(){const e=await this.data();return L(this.shape,e,"complex64"===this.dtype)}arraySync(){return L(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const e=cr().read(this.dataId);if("string"===this.dtype){const t=await e;try{return t.map((e=>Qs(e)))}catch(e){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return e}dataSync(){this.throwIfDisposed();const e=cr().readSync(this.dataId);if("string"===this.dtype)try{return e.map((e=>Qs(e)))}catch(e){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return e}async bytes(){this.throwIfDisposed();const e=await cr().read(this.dataId);return"string"===this.dtype?e:new Uint8Array(e.buffer)}dispose(){this.isDisposed||(cr().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(e=!1){return hr.print(this,e)}clone(){return this.throwIfDisposed(),hr.clone(this)}toString(e=!1){return rr(this.dataSync(),this.shape,this.dtype,e)}cast(e){return this.throwIfDisposed(),hr.cast(this,e)}variable(e=!0,t,n){return this.throwIfDisposed(),cr().makeVariable(this,e,t,n)}}function fr(){return Z("Tensor",(()=>dr))}Object.defineProperty(dr,Symbol.hasInstance,{value:e=>!!e&&null!=e.data&&null!=e.dataSync&&null!=e.throwIfDisposed}),fr();class mr extends dr{constructor(e,t,n,s){super(e.shape,e.dtype,e.dataId,s),this.trainable=t,this.name=n}assign(e){if(e.dtype!==this.dtype)throw new Error(`dtype of the new value (${e.dtype}) and previous value (${this.dtype}) must match`);if(!d(e.shape,this.shape))throw new Error(`shape of the new value (${e.shape}) and previous value (${this.shape}) must match`);cr().disposeTensor(this),this.dataId=e.dataId,cr().incRef(this,null)}dispose(){cr().disposeVariable(this),this.isDisposedInternal=!0}}var gr,yr,br,xr,wr;Object.defineProperty(mr,Symbol.hasInstance,{value:e=>e instanceof dr&&null!=e.assign&&e.assign instanceof Function}),(gr=e.Rank||(e.Rank={})).R0="R0",gr.R1="R1",gr.R2="R2",gr.R3="R3",gr.R4="R4",gr.R5="R5",gr.R6="R6",function(e){e.float32="float32",e.int32="int32",e.bool="int32",e.complex64="complex64"}(yr||(yr={})),function(e){e.float32="float32",e.int32="int32",e.bool="bool",e.complex64="complex64"}(br||(br={})),function(e){e.float32="float32",e.int32="float32",e.bool="float32",e.complex64="complex64"}(xr||(xr={})),function(e){e.float32="complex64",e.int32="complex64",e.bool="complex64",e.complex64="complex64"}(wr||(wr={}));const kr={float32:xr,int32:yr,bool:br,complex64:wr};function vr(e,t){if("string"===e||"string"===t){if("string"===e&&"string"===t)return"string";throw new Error(`Can not upcast ${e} with ${t}`)}return kr[e][t]}function Nr(e){return vr(e,"int32")}function Ir(e,t){if(e.dtype===t.dtype)return[e,t];const n=vr(e.dtype,t.dtype);return[e.cast(n),t.cast(n)]}function Sr(e,t){l(e.dtype===t.dtype,(()=>`The dtypes of the first(${e.dtype}) and second(${t.dtype}) input must match`))}function $r(e,t){return t.some((t=>t.id===e.id))}function Cr(e){const t=[];return Tr(e,t,new Set),t}function Tr(e,t,n){if(null==e)return;if(e instanceof dr)return void t.push(e);if(s=e,!Array.isArray(s)&&"object"!=typeof s)return;var s;const r=e;for(const e in r){const s=r[e];n.has(s)||(n.add(s),Tr(s,t,n))}}var Er=Object.freeze({__proto__:null,makeTypesMatch:Ir,assertTypesMatch:Sr,isTensorInList:$r,getTensorsInContainer:Cr});function Ar(e){return null!=e.kernelName}class Rr{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((e=>e.name))))}}}dispose(){for(const e in this.registeredVariables)this.registeredVariables[e].dispose()}}class Fr{constructor(e){this.ENV=e,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new Rr}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const e=this.getSortedBackends();for(let t=0;t<e.length;t++){const n=e[t];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:e,asyncInit:t}=this.initializeBackendsAndReturnBest();if(t)throw new Error(`The highest priority backend '${e}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(e)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(e){if(!(e in this.registry)){if(!(e in this.registryFactory))return null;{const{asyncInit:t}=this.initializeBackend(e);if(t)return null}}return this.registry[e]}findBackendFactory(e){return e in this.registryFactory?this.registryFactory[e].factory:null}registerBackend(e,t,n=1){return e in this.registryFactory?(console.warn(`${e} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[e]={factory:t,priority:n},!0)}async setBackend(e){if(null==this.registryFactory[e])throw new Error(`Backend name '${e}' not found in registry`);if(this.backendName=e,null==this.registry[e]){this.backendInstance=null;const{success:t,asyncInit:n}=this.initializeBackend(e);if(!(n?await t:t))return!1}return this.backendInstance=this.registry[e],this.setupRegisteredKernels(),this.profiler=new tr(this.backendInstance),!0}setupRegisteredKernels(){is(this.backendName).forEach((e=>{null!=e.setupFunc&&e.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(e){is(e).forEach((t=>{null!=t.disposeFunc&&t.disposeFunc(this.registry[e])}))}initializeBackend(e){const t=this.registryFactory[e];if(null==t)throw new Error(`Cannot initialize backend ${e}, no registration found.`);try{const s=t.factory();if(!s||s instanceof n||"function"!=typeof s.then)return this.registry[e]=s,{success:!0,asyncInit:!1};{const t=++this.pendingBackendInitId,n=s.then((n=>!(t<this.pendingBackendInitId)&&(this.registry[e]=n,this.pendingBackendInit=null,!0))).catch((n=>(t<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${e} failed`),console.warn(n.stack||n.message)),!1)));return this.pendingBackendInit=n,{success:n,asyncInit:!0}}}catch(t){return console.warn(`Initialization of backend ${e} failed`),console.warn(t.stack||t.message),{success:!1,asyncInit:!1}}}removeBackend(e){if(!(e in this.registryFactory))throw new Error(`${e} backend not found in registry`);this.backendName===e&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,e in this.registry&&(this.disposeRegisteredKernels(e),this.registry[e].dispose(),delete this.registry[e]),delete this.registryFactory[e],this.backendName===e&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((e,t)=>this.registryFactory[t].priority-this.registryFactory[e].priority))}initializeBackendsAndReturnBest(){const e=this.getSortedBackends();for(let t=0;t<e.length;t++){const n=e[t],{success:s,asyncInit:r}=this.initializeBackend(n);if(r||s)return{name:n,asyncInit:r}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(e,t){const n=this.state.tensorInfo.get(t),s=n.backend,r=this.readSync(t),a=s.refCount(t);s.disposeData(t,!0),n.backend=e,e.move(t,r,n.shape,n.dtype,a),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(e,t){let n,s=null;if(null==t){if("function"!=typeof e)throw new Error("Please provide a function to tidy()");t=e}else{if("string"!=typeof e&&!(e instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof t)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=e}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=t(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(e,t,n){e();try{const e=n();return t(),e}catch(e){throw t(),e}}nextTensorId(){return Fr.nextTensorId++}nextVariableId(){return Fr.nextVariableId++}clone(e){const t=Dr.runKernel(ct,{x:e}),n={x:e};return this.addTapeNode(this.state.activeScope.name,n,[t],(e=>({x:()=>{const t={x:e},n={dtype:"float32"};return Dr.runKernel(we,t,n)}})),[],{}),t}runKernel(e,t,n){if(!(null!=rs(e,this.backendName)))throw new Error(`Kernel '${e}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:e,inputs:t,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(e,t,n){const s=this.backend.numDataIds();let r=0;n.forEach((e=>{r+="complex64"===e.dtype?3:1}));const a=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],i=s-t-r-a;if(i>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${i} data ids) after running '${e}'`)}runKernelFunc(e){let t,n=[];const s=this.isTapeOn(),r=this.state.numBytes,a=this.state.numTensors;let i,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const u=Ar(e)?e.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(Ar(e)){const{kernelName:t,inputs:r,attrs:a}=e;null==this.backendName&&this.backend;const u=rs(t,this.backendName);l(null!=u,(()=>`Cannot find registered kernel '${t}' for backend '${this.backendName}'`)),i=()=>{const e=this.backend.numDataIds();o=u.kernelFunc({inputs:r,attrs:a,backend:this.backend});const i=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(t,e,i);const l=i.map((e=>{if(null!=e.rank)return e;const{dataId:t,shape:n,dtype:s}=e;return this.makeTensorFromDataId(t,n,s)}));if(s){const e=this.getTensorsForGradient(t,r,l);n=this.saveTensorsForBackwardMode(e)}return l}}else{const{forwardFunc:t}=e,r=e=>{s&&(n=e.map((e=>this.keep(this.clone(e)))))};i=()=>{const e=this.backend.numDataIds();o=this.tidy((()=>t(this.backend,r)));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(u,e,n),n}}const{inputs:c,attrs:h}=e,p=Ar(e)?null:e.backwardsFunc;let d;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(d=this.profiler.profileKernel(u,c,(()=>i())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(d),t=d.outputs):t=i()})),s&&this.addTapeNode(u,c,t,p,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:u,bytesAdded:this.state.numBytes-r,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-a,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(c).map((e=>null!=c[e]?c[e].shape:null)),outputShapes:t.map((e=>e.shape)),kernelTimeMs:d.timeMs,extraInfo:d.extraInfo}),Array.isArray(o)?t:t[0]}saveTensorsForBackwardMode(e){return e.map((e=>this.keep(this.clone(e))))}getTensorsForGradient(e,t,n){const s=as(e);if(null!=s){const e=s.inputsToSave||[],r=s.outputsToSave||[];let a;s.saveAllInputs?(l(Array.isArray(t),(()=>"saveAllInputs is true, expected inputs to be an array.")),a=Object.keys(t).map((e=>t[e]))):a=e.map((e=>t[e]));const i=n.filter(((e,t)=>r[t]));return a.concat(i)}return[]}makeTensor(e,t,n,s){if(null==e)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let r=e;"string"===n&&E(e[0])&&(r=e.map((e=>Js(e))));const a=s.write(r,t,n),i=new dr(t,n,a,this.nextTensorId());if(this.trackTensor(i,s),"string"===n){const e=this.state.tensorInfo.get(a),t=T(r);this.state.numBytes+=t-e.bytes,e.bytes=t}return i}makeTensorFromDataId(e,t,n,s){const r=new dr(t,n=n||"float32",e,this.nextTensorId());return this.trackTensor(r,s),r}makeVariable(e,t=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==e.dtype&&(e=e.cast(s));const r=new mr(e,t,n,this.nextTensorId());if(null!=this.state.registeredVariables[r.name])throw new Error(`Variable with name ${r.name} was already registered`);return this.state.registeredVariables[r.name]=r,this.incRef(r,this.backend),r}trackTensor(e,t){this.state.numTensors++,"string"===e.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==e.dtype&&"string"!==e.dtype&&(n=e.size*C(e.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(e.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(e.dataId,{backend:t||this.backend,dtype:e.dtype,shape:e.shape,bytes:n})),e instanceof mr||this.track(e)}incRef(e,t){this.trackTensor(e,t),this.backend.incRef(e.dataId)}removeDataId(e,t){this.state.tensorInfo.has(e)&&this.state.tensorInfo.get(e).backend===t&&(this.state.tensorInfo.delete(e),this.state.numDataBuffers--)}disposeTensor(e){if(!this.state.tensorInfo.has(e.dataId))return;const t=this.state.tensorInfo.get(e.dataId);if(this.state.numTensors--,"string"===e.dtype&&(this.state.numStringTensors--,this.state.numBytes-=t.bytes),"complex64"!==e.dtype&&"string"!==e.dtype){const t=e.size*C(e.dtype);this.state.numBytes-=t}t.backend.disposeData(e.dataId)&&this.removeDataId(e.dataId,t.backend)}disposeVariables(){for(const e in this.state.registeredVariables){const t=this.state.registeredVariables[e];this.disposeVariable(t)}}disposeVariable(e){this.disposeTensor(e),null!=this.state.registeredVariables[e.name]&&delete this.state.registeredVariables[e.name]}memory(){const e=this.backend.memory();return e.numTensors=this.state.numTensors,e.numDataBuffers=this.state.numDataBuffers,e.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(e.unreliable=!0,null==e.reasons&&(e.reasons=[]),e.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),e}async profile(e){this.state.profiling=!0;const t=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await e(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((e=>e.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-t,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const e of this.state.activeProfile.kernels)e.kernelTimeMs=await e.kernelTimeMs,e.extraInfo=await e.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(e,t,n,s,r,a){const i={id:this.state.nextTapeNodeId++,kernelName:e,inputs:t,outputs:n,saved:r},o=as(e);null!=o&&(s=o.gradFunc),null!=s&&(i.gradient=e=>(e=e.map(((e,t)=>{if(null==e){const e=n[t],s=B(e.size,e.dtype);return this.makeTensor(s,e.shape,e.dtype)}return e})),s(e.length>1?e:e[0],r,a))),this.state.activeTape.push(i)}keep(e){return e.kept=!0,e}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(e){const t={track:[],name:"unnamed scope",id:this.state.nextScopeId++};e&&(t.name=e),this.state.scopeStack.push(t),this.state.activeScope=t}endScope(e){const t=Cr(e),n=new Set(t.map((e=>e.id)));for(let e=0;e<this.state.activeScope.track.length;e++){const t=this.state.activeScope.track[e];t.kept||n.has(t.id)||t.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],t.forEach((e=>{e.kept||e.scopeId!==s.id||this.track(e)}))}gradients(e,t,n,s=!1){if(l(t.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const r=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",e)));l(r instanceof dr,(()=>"The result y returned by f() must be a tensor."));const a=function(e,t,n){const s={},r={};for(let e=0;e<t.length;e++)s[t[e].id]=!0;for(let n=0;n<e.length;n++){const a=e[n],i=a.inputs;for(const e in i){const n=i[e];let o=!1;for(let e=0;e<t.length;e++)if(s[n.id]){a.outputs.forEach((e=>s[e.id]=!0)),o=!0,r[a.id]=!0;break}if(o)break}}const a={};a[n.id]=!0;const i={};for(let t=e.length-1;t>=0;t--){const n=e[t],s=n.inputs;for(let e=0;e<n.outputs.length;e++)if(a[n.outputs[e].id]){for(const e in s)a[s[e].id]=!0,i[n.id]=!0;break}}const o=[];for(let t=0;t<e.length;t++){const n=e[t];if(r[n.id]&&i[n.id]){const e={};for(const t in n.inputs){const r=n.inputs[t];s[r.id]&&(e[t]=r)}const t=Object.assign({},n);t.inputs=e,t.outputs=n.outputs,o.push(t)}}return o}(this.state.activeTape,t,r);if(!s&&0===a.length&&t.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const e={};e[r.id]=null==n?function(e){const t=z(p(e),"float32");return Dr.makeTensor(t,e,"float32")}(r.shape):n,function(e,t,n,s){for(let r=t.length-1;r>=0;r--){const a=t[r],i=[];if(a.outputs.forEach((t=>{const n=e[t.id];null!=n?i.push(n):i.push(null)})),null==a.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${a.kernelName}.`);const o=a.gradient(i);for(const t in a.inputs){if(!(t in o))throw new Error(`Cannot backprop through input ${t}. Available gradients found: ${Object.keys(o)}.`);const r=n((()=>o[t]()));if("float32"!==r.dtype)throw new Error(`Error in gradient for op ${a.kernelName}. The gradient of input ${t} must have 'float32' dtype, but has '${r.dtype}'`);const i=a.inputs[t];if(!d(r.shape,i.shape))throw new Error(`Error in gradient for op ${a.kernelName}. The gradient of input '${t}' has shape '${r.shape}', which does not match the shape of the input '${i.shape}'`);if(null==e[i.id])e[i.id]=r;else{const t=e[i.id];e[i.id]=s(t,r),t.dispose()}}}}(e,a,(e=>this.tidy(e)),Or);const s=t.map((t=>e[t.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((e=>{for(const t of e.saved)t.dispose()})),this.state.activeTape=null),{value:r,grads:s}}))}customGrad(e){return l(_(e),(()=>"The f passed in customGrad(f) must be a function.")),(...t)=>{let n;l(t.every((e=>e instanceof dr)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const s={};t.forEach(((e,t)=>{s[t]=e}));return this.runKernelFunc({forwardFunc:(s,r)=>(n=e(...t,r),l(n.value instanceof dr,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),l(_(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(e,s)=>{const r=n.gradFunc(e,s),a=Array.isArray(r)?r:[r];l(a.length===t.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),l(a.every((e=>e instanceof dr)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const i={};return a.forEach(((e,t)=>{i[t]=()=>e})),i},inputs:s})}}readSync(e){return this.state.tensorInfo.get(e).backend.readSync(e)}read(e){return this.state.tensorInfo.get(e).backend.read(e)}async time(e){const t=Ys(),n=await this.backend.time(e);return n.wallMs=Ys()-t,n}track(e){return null!=this.state.activeScope&&(e.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(e)),e}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new Rr;for(const e in this.registry)this.disposeRegisteredKernels(e),this.registry[e].dispose(),delete this.registry[e];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}function _r(){const t=Y();if(null==t._tfengine){const e=new j(t);t._tfengine=new Fr(e)}var n;return n=t._tfengine.ENV,e.ENV=n,cr=()=>t._tfengine,t._tfengine}Fr.nextTensorId=0,Fr.nextVariableId=0;const Dr=_r();function Or(e,t){const n={a:e,b:t};return Dr.runKernel(te,n)}function Mr(e){if(e||"undefined"!=typeof navigator&&null!=navigator){if(e||(e=navigator),"ReactNative"===e.product)return!0;const t=e.userAgent||e.vendor||window.opera;return/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i.test(t)||/1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i.test(t.substr(0,4))}return!1}function Lr(){return"undefined"!=typeof window&&null!=window.document||"undefined"!=typeof WorkerGlobalScope}var zr=Object.freeze({__proto__:null,isMobile:Mr,isBrowser:Lr});const Br=K();function Pr(e,t){let n=e;if($(e))return"string"===t?[]:[e.length];if(!Array.isArray(e))return[];const s=[];for(;Array.isArray(n)||$(n)&&"string"!==t;)s.push(n.length),n=n[0];return Array.isArray(e)&&K().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&Wr(e,s,[]),s}function Wr(e,t,n){if(n=n||[],!Array.isArray(e)&&!$(e))return void l(0===t.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${t[0]} elements`));l(t.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${e.length} elements`)),l(e.length===t[0],(()=>`Element arr[${n.join("][")}] should have ${t[0]} elements, but has ${e.length} elements`));const s=t.slice(1);for(let t=0;t<e.length;++t)Wr(e[t],s,n.concat(t))}function Vr(e,t,n,s){if("string_or_numeric"!==e){if(null==e)throw new Error("Expected dtype cannot be null.");if("numeric"!==e&&e!==t||"numeric"===e&&"string"===t)throw new Error(`Argument '${n}' passed to '${s}' must be ${e} tensor, but got ${t} tensor`)}}function Ur(e,t,n,s="numeric"){if(e instanceof dr)return Vr(s,e.dtype,t,n),e;let r=F(e);if("string"!==r&&["bool","int32","float32"].indexOf(s)>=0&&(r=s),Vr(s,r,t,n),null==e||!$(e)&&!Array.isArray(e)&&"number"!=typeof e&&"boolean"!=typeof e&&"string"!=typeof e){const s=null==e?"null":e.constructor.name;throw new Error(`Argument '${t}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const a=Pr(e,r);$(e)||Array.isArray(e)||(e=[e]);const i="string"!==r?Xs(e,r):h(e,[],!0);return Dr.makeTensor(i,a,r)}function Gr(e,t,n,s="numeric"){if(!Array.isArray(e))throw new Error(`Argument ${t} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return e.map(((e,r)=>Ur(e,`${t}[${r}]`,n,s)))}Br.registerFlag("DEBUG",(()=>!1),(e=>{e&&console.warn("Debugging mode is ON. The output of every math call will be downloaded to CPU and checked for NaNs. This significantly impacts performance.")})),Br.registerFlag("IS_BROWSER",(()=>Lr())),Br.registerFlag("IS_NODE",(()=>"undefined"!=typeof process&&void 0!==process.versions&&void 0!==process.versions.node)),Br.registerFlag("IS_CHROME",(()=>"undefined"!=typeof navigator&&null!=navigator&&null!=navigator.userAgent&&/Chrome/.test(navigator.userAgent)&&/Google Inc/.test(navigator.vendor))),Br.registerFlag("PROD",(()=>!1)),Br.registerFlag("TENSORLIKE_CHECK_SHAPE_CONSISTENCY",(()=>Br.getBool("DEBUG"))),Br.registerFlag("DEPRECATION_WARNINGS_ENABLED",(()=>!0)),Br.registerFlag("IS_TEST",(()=>!1)),Br.registerFlag("CHECK_COMPUTATION_FOR_ERRORS",(()=>!0)),Br.registerFlag("WRAP_TO_IMAGEBITMAP",(()=>!1));const Hr="__op";function jr(e){const t=Object.keys(e);if(1!==t.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${t.length} keys.`);let n=t[0];const s=e[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+=Hr;const r=(...e)=>{Dr.startScope(n);try{const t=s(...e);return G(t)&&console.error("Cannot return a Promise inside of tidy."),Dr.endScope(t),t}catch(e){throw Dr.endScope(null),e}};return Object.defineProperty(r,"name",{value:n,configurable:!0}),r}const qr=jr({complex_:function(e,t){const n=Ur(e,"real","complex"),s=Ur(t,"imag","complex");u(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const r={real:n,imag:s};return Dr.runKernel(Ne,r)}});function Kr(e,t,n,s){if(null==s&&(s=F(e)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!$(e)&&!Array.isArray(e)&&"number"!=typeof e&&"boolean"!=typeof e&&"string"!=typeof e)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=t){W(t);const e=p(t),s=p(n);l(e===s,(()=>`Based on the provided shape, [${t}], the tensor should have ${e} values but has ${s}`));for(let e=0;e<n.length;++e){const s=n[e],r=e!==n.length-1||s!==p(t.slice(e));l(n[e]===t[e]||!r,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${t}). `))}}return $(e)||Array.isArray(e)||(e=[e]),t=t||n,e="string"!==s?Xs(e,s):h(e,[],!0),Dr.makeTensor(e,t,s)}function Xr(e,t,n){return Kr(e,t,Pr(e,n),n)}const Yr={float32:4,float16:2,int32:4,uint16:2,uint8:1,bool:1,complex64:8};async function Zr(e,t){const n=[],s=[],r=Array.isArray(e)?e.map((e=>e.name)):Object.keys(e);for(let a=0;a<r.length;++a){const i=r[a],o=Array.isArray(e)?e[a].tensor:e[i];if("float32"!==o.dtype&&"int32"!==o.dtype&&"bool"!==o.dtype&&"string"!==o.dtype&&"complex64"!==o.dtype)throw new Error(`Unsupported dtype in weight '${i}': ${o.dtype}`);const l={name:i,shape:o.shape,dtype:o.dtype};if("string"===o.dtype){const e=new Promise((async e=>{const t=await o.bytes(),n=t.reduce(((e,t)=>e+t.length),0)+4*t.length,s=new Uint8Array(n);let r=0;for(let e=0;e<t.length;e++){const n=t[e],a=new Uint8Array(new Uint32Array([n.length]).buffer);s.set(a,r),r+=4,s.set(n,r),r+=n.length}e(s)}));s.push(e)}else s.push(o.data());null!=t&&(l.group=t),n.push(l)}return{data:Qr(await Promise.all(s)),specs:n}}function Jr(e,t){const n={};let s,r=0;for(const a of t){const t=a.name,i=a.dtype,o=a.shape,l=p(o);let u;if("quantization"in a){const n=a.quantization;if("uint8"===n.dtype||"uint16"===n.dtype){if(!("min"in n)||!("scale"in n))throw new Error(`Weight ${a.name} with quantization ${n.dtype} doesn't have corresponding metadata min and scale.`)}else{if("float16"!==n.dtype)throw new Error(`Weight ${a.name} has unknown quantization dtype ${n.dtype}. Supported quantization dtypes are: 'uint8', 'uint16', and 'float16'.`);if("float32"!==i)throw new Error(`Weight ${a.name} is quantized with ${n.dtype} which only supports weights of type float32 not ${i}.`)}const o=Yr[n.dtype],c=e.slice(r,r+l*o),h="uint8"===n.dtype?new Uint8Array(c):new Uint16Array(c);if("float32"===i)if("uint8"===n.dtype||"uint16"===n.dtype){u=new Float32Array(h.length);for(let e=0;e<h.length;e++){const t=h[e];u[e]=t*n.scale+n.min}}else{if("float16"!==n.dtype)throw new Error(`Unsupported quantization type ${n.dtype} for weight type float32.`);void 0===s&&(s=aa()),u=s(h)}else{if("int32"!==i)throw new Error(`Unsupported dtype in weight '${t}': ${i}`);if("uint8"!==n.dtype&&"uint16"!==n.dtype)throw new Error(`Unsupported quantization type ${n.dtype} for weight type int32.`);u=new Int32Array(h.length);for(let e=0;e<h.length;e++){const t=h[e];u[e]=Math.round(t*n.scale+n.min)}}r+=l*o}else if("string"===i){const t=p(a.shape);u=[];for(let n=0;n<t;n++){const t=new Uint32Array(e.slice(r,r+4))[0];r+=4;const n=new Uint8Array(e.slice(r,r+t));u.push(n),r+=t}}else{const s=Yr[i],a=e.slice(r,r+l*s);if("float32"===i)u=new Float32Array(a);else if("int32"===i)u=new Int32Array(a);else if("bool"===i)u=new Uint8Array(a);else{if("complex64"!==i)throw new Error(`Unsupported dtype in weight '${t}': ${i}`);{u=new Float32Array(a);const e=new Float32Array(u.length/2),s=new Float32Array(u.length/2);for(let t=0;t<e.length;t++)e[t]=u[2*t],s[t]=u[2*t+1];const r=Xr(e,o,"float32"),i=Xr(s,o,"float32");n[t]=qr(r,i),r.dispose(),i.dispose()}}r+=l*s}"complex64"!==i&&(n[t]=Xr(u,o,i))}return n}function Qr(e){if(null===e)throw new Error(`Invalid input value: ${JSON.stringify(e)}`);let t=0;const n=[];e.forEach((e=>{if(t+=e.byteLength,n.push(e.byteLength===e.buffer.byteLength?e:new e.constructor(e)),!(e instanceof Float32Array||e instanceof Int32Array||e instanceof Uint8Array))throw new Error(`Unsupported TypedArray subtype: ${e.constructor.name}`)}));const s=new Uint8Array(t);let r=0;return n.forEach((e=>{s.set(new Uint8Array(e.buffer),r),r+=e.byteLength})),s.buffer}const ea="undefined"!=typeof Buffer&&("undefined"==typeof Blob||"undefined"==typeof atob||"undefined"==typeof btoa);function ta(e){return ea?Buffer.byteLength(e):new Blob([e]).size}function na(e){if(1===e.length)return e[0];let t=0;e.forEach((e=>{t+=e.byteLength}));const n=new Uint8Array(t);let s=0;return e.forEach((e=>{n.set(new Uint8Array(e),s),s+=e.byteLength})),n.buffer}function sa(e){for(e=e.trim();e.endsWith("/");)e=e.slice(0,e.length-1);const t=e.split("/");return t[t.length-1]}function ra(e){if(e.modelTopology instanceof ArrayBuffer)throw new Error("Expected JSON model topology, received ArrayBuffer.");return{dateSaved:new Date,modelTopologyType:"JSON",modelTopologyBytes:null==e.modelTopology?0:ta(JSON.stringify(e.modelTopology)),weightSpecsBytes:null==e.weightSpecs?0:ta(JSON.stringify(e.weightSpecs)),weightDataBytes:null==e.weightData?0:e.weightData.byteLength}}function aa(){const e=function(){const e=e=>{let t=e<<13,n=0;for(;0==(8388608&t);)n-=8388608,t<<=1;return t&=-8388609,n+=947912704,t|n},t=new Uint32Array(2048);t[0]=0;for(let n=1;n<1024;n++)t[n]=e(n);for(let e=1024;e<2048;e++)t[e]=939524096+(e-1024<<13);return t}(),t=function(){const e=new Uint32Array(64);e[0]=0,e[31]=1199570944,e[32]=2147483648,e[63]=3347054592;for(let t=1;t<31;t++)e[t]=t<<23;for(let t=33;t<63;t++)e[t]=2147483648+(t-32<<23);return e}(),n=function(){const e=new Uint32Array(64);for(let t=0;t<64;t++)e[t]=1024;return e[0]=e[32]=0,e}();return s=>{const r=new ArrayBuffer(4*s.length),a=new Uint32Array(r);for(let r=0;r<s.length;r++){const i=s[r],o=e[n[i>>10]+(1023&i)]+t[i>>10];a[r]=o}return new Float32Array(r)}}class ia{constructor(){this.saveRouters=[],this.loadRouters=[]}static getInstance(){return null==ia.instance&&(ia.instance=new ia),ia.instance}static registerSaveRouter(e){ia.getInstance().saveRouters.push(e)}static registerLoadRouter(e){ia.getInstance().loadRouters.push(e)}static getSaveHandlers(e){return ia.getHandlers(e,"save")}static getLoadHandlers(e,t){return ia.getHandlers(e,"load",t)}static getHandlers(e,t,n){const s=[];return("load"===t?ia.getInstance().loadRouters:ia.getInstance().saveRouters).forEach((t=>{const r=t(e,n);null!==r&&s.push(r)})),s}}const oa=e=>ia.getSaveHandlers(e),la=(e,t)=>ia.getLoadHandlers(e,t),ua="tensorflowjs",ca="models_store",ha="model_info_store";function pa(){if(!K().getBool("IS_BROWSER"))throw new Error("Failed to obtain IndexedDB factory because the current environmentis not a web browser.");const e="undefined"==typeof window?self:window,t=e.indexedDB||e.mozIndexedDB||e.webkitIndexedDB||e.msIndexedDB||e.shimIndexedDB;if(null==t)throw new Error("The current browser does not appear to support IndexedDB.");return t}function da(e){const t=e.result;t.createObjectStore(ca,{keyPath:"modelPath"}),t.createObjectStore(ha,{keyPath:"modelPath"})}class fa{constructor(e){if(this.indexedDB=pa(),null==e||!e)throw new Error("For IndexedDB, modelPath must not be null, undefined or empty.");this.modelPath=e}async save(e){if(e.modelTopology instanceof ArrayBuffer)throw new Error("BrowserLocalStorage.save() does not support saving model topology in binary formats yet.");return this.databaseAction(this.modelPath,e)}async load(){return this.databaseAction(this.modelPath)}databaseAction(e,t){return new Promise(((e,n)=>{const s=this.indexedDB.open(ua,1);s.onupgradeneeded=()=>da(s),s.onsuccess=()=>{const r=s.result;if(null==t){const t=r.transaction(ca,"readonly"),s=t.objectStore(ca).get(this.modelPath);s.onsuccess=()=>{if(null==s.result)return r.close(),n(new Error(`Cannot find model with path '${this.modelPath}' in IndexedDB.`));e(s.result.modelArtifacts)},s.onerror=e=>(r.close(),n(s.error)),t.oncomplete=()=>r.close()}else{const s=ra(t),a=r.transaction(ha,"readwrite");let i=a.objectStore(ha);const o=i.put({modelPath:this.modelPath,modelArtifactsInfo:s});let l;o.onsuccess=()=>{l=r.transaction(ca,"readwrite");const o=l.objectStore(ca).put({modelPath:this.modelPath,modelArtifacts:t,modelArtifactsInfo:s});o.onsuccess=()=>e({modelArtifactsInfo:s}),o.onerror=e=>{i=a.objectStore(ha);const t=i.delete(this.modelPath);t.onsuccess=()=>(r.close(),n(o.error)),t.onerror=e=>(r.close(),n(o.error))}},o.onerror=e=>(r.close(),n(o.error)),a.oncomplete=()=>{null==l?r.close():l.oncomplete=()=>r.close()}}},s.onerror=e=>n(s.error)}))}}fa.URL_SCHEME="indexeddb://";const ma=e=>{return K().getBool("IS_BROWSER")&&!Array.isArray(e)&&e.startsWith(fa.URL_SCHEME)?(t=e.slice(fa.URL_SCHEME.length),new fa(t)):null;var t};ia.registerSaveRouter(ma),ia.registerLoadRouter(ma);class ga{constructor(){this.indexedDB=pa()}async listModels(){return new Promise(((e,t)=>{const n=this.indexedDB.open(ua,1);n.onupgradeneeded=()=>da(n),n.onsuccess=()=>{const s=n.result,r=s.transaction(ha,"readonly"),a=r.objectStore(ha).getAll();a.onsuccess=()=>{const t={};for(const e of a.result)t[e.modelPath]=e.modelArtifactsInfo;e(t)},a.onerror=e=>(s.close(),t(a.error)),r.oncomplete=()=>s.close()},n.onerror=e=>t(n.error)}))}async removeModel(e){var t;return e=(t=e).startsWith(fa.URL_SCHEME)?t.slice(fa.URL_SCHEME.length):t,new Promise(((t,n)=>{const s=this.indexedDB.open(ua,1);s.onupgradeneeded=()=>da(s),s.onsuccess=()=>{const r=s.result,a=r.transaction(ha,"readwrite"),i=a.objectStore(ha),o=i.get(e);let l;o.onsuccess=()=>{if(null==o.result)return r.close(),n(new Error(`Cannot find model with path '${e}' in IndexedDB.`));{const s=i.delete(e),a=()=>{l=r.transaction(ca,"readwrite");const s=l.objectStore(ca).delete(e);s.onsuccess=()=>t(o.result.modelArtifactsInfo),s.onerror=e=>n(o.error)};s.onsuccess=a,s.onerror=e=>(a(),r.close(),n(o.error))}},o.onerror=e=>(r.close(),n(o.error)),a.oncomplete=()=>{null==l?r.close():l.oncomplete=()=>r.close()}},s.onerror=e=>n(s.error)}))}}const ya="/",ba="tensorflowjs_models",xa="info",wa="model_topology",ka="weight_specs",va="weight_data",Na="model_metadata";function Ia(e){return{info:[ba,e,xa].join(ya),topology:[ba,e,wa].join(ya),weightSpecs:[ba,e,ka].join(ya),weightData:[ba,e,va].join(ya),modelMetadata:[ba,e,Na].join(ya)}}function Sa(e){const t=e.split(ya);if(t.length<3)throw new Error(`Invalid key format: ${e}`);return t.slice(1,t.length-1).join(ya)}class $a{constructor(e){if(!K().getBool("IS_BROWSER")||"undefined"==typeof window||void 0===window.localStorage)throw new Error("The current environment does not support local storage.");if(this.LS=window.localStorage,null==e||!e)throw new Error("For local storage, modelPath must not be null, undefined or empty.");this.modelPath=e,this.keys=Ia(this.modelPath)}async save(e){if(e.modelTopology instanceof ArrayBuffer)throw new Error("BrowserLocalStorage.save() does not support saving model topology in binary formats yet.");{const t=JSON.stringify(e.modelTopology),n=JSON.stringify(e.weightSpecs),s=ra(e);try{this.LS.setItem(this.keys.info,JSON.stringify(s)),this.LS.setItem(this.keys.topology,t),this.LS.setItem(this.keys.weightSpecs,n),this.LS.setItem(this.keys.weightData,function(e){if(ea)return Buffer.from(e).toString("base64");const t=new Uint8Array(e);let n="";for(let e=0,s=t.length;e<s;e++)n+=String.fromCharCode(t[e]);return btoa(n)}(e.weightData));const r={format:e.format,generatedBy:e.generatedBy,convertedBy:e.convertedBy};return null!=e.signature&&(r.signature=e.signature),null!=e.userDefinedMetadata&&(r.userDefinedMetadata=e.userDefinedMetadata),null!=e.modelInitializer&&(r.modelInitializer=e.modelInitializer),this.LS.setItem(this.keys.modelMetadata,JSON.stringify(r)),{modelArtifactsInfo:s}}catch(e){throw this.LS.removeItem(this.keys.info),this.LS.removeItem(this.keys.topology),this.LS.removeItem(this.keys.weightSpecs),this.LS.removeItem(this.keys.weightData),this.LS.removeItem(this.keys.modelMetadata),new Error(`Failed to save model '${this.modelPath}' to local storage: size quota being exceeded is a possible cause of this failure: modelTopologyBytes=${s.modelTopologyBytes}, weightSpecsBytes=${s.weightSpecsBytes}, weightDataBytes=${s.weightDataBytes}.`)}}}async load(){const e=JSON.parse(this.LS.getItem(this.keys.info));if(null==e)throw new Error(`In local storage, there is no model with name '${this.modelPath}'`);if("JSON"!==e.modelTopologyType)throw new Error("BrowserLocalStorage does not support loading non-JSON model topology yet.");const t={},n=JSON.parse(this.LS.getItem(this.keys.topology));if(null==n)throw new Error(`In local storage, the topology of model '${this.modelPath}' is missing.`);t.modelTopology=n;const s=JSON.parse(this.LS.getItem(this.keys.weightSpecs));if(null==s)throw new Error(`In local storage, the weight specs of model '${this.modelPath}' are missing.`);t.weightSpecs=s;const r=this.LS.getItem(this.keys.modelMetadata);if(null!=r){const e=JSON.parse(r);t.format=e.format,t.generatedBy=e.generatedBy,t.convertedBy=e.convertedBy,null!=e.signature&&(t.signature=e.signature),null!=e.userDefinedMetadata&&(t.userDefinedMetadata=e.userDefinedMetadata),null!=e.modelInitializer&&(t.modelInitializer=e.modelInitializer)}const a=this.LS.getItem(this.keys.weightData);if(null==a)throw new Error(`In local storage, the binary weight values of model '${this.modelPath}' are missing.`);return t.weightData=function(e){if(ea){const t=Buffer.from(e,"base64");return t.buffer.slice(t.byteOffset,t.byteOffset+t.byteLength)}const t=atob(e),n=new Uint8Array(t.length);for(let e=0;e<t.length;++e)n.set([t.charCodeAt(e)],e);return n.buffer}(a),t}}$a.URL_SCHEME="localstorage://";const Ca=e=>{return K().getBool("IS_BROWSER")&&!Array.isArray(e)&&e.startsWith($a.URL_SCHEME)?(t=e.slice($a.URL_SCHEME.length),new $a(t)):null;var t};ia.registerSaveRouter(Ca),ia.registerLoadRouter(Ca);class Ta{constructor(){l(K().getBool("IS_BROWSER"),(()=>"Current environment is not a web browser")),l("undefined"==typeof window||void 0!==window.localStorage,(()=>"Current browser does not appear to support localStorage")),this.LS=window.localStorage}async listModels(){const e={},t=ba+ya,n=ya+xa;for(let s=0;s<this.LS.length;++s){const r=this.LS.key(s);if(r.startsWith(t)&&r.endsWith(n)){e[Sa(r)]=JSON.parse(this.LS.getItem(r))}}return e}async removeModel(e){var t;const n=Ia(e=(t=e).startsWith($a.URL_SCHEME)?t.slice($a.URL_SCHEME.length):t);if(null==this.LS.getItem(n.info))throw new Error(`Cannot find model at path '${e}'`);const s=JSON.parse(this.LS.getItem(n.info));return this.LS.removeItem(n.info),this.LS.removeItem(n.topology),this.LS.removeItem(n.weightSpecs),this.LS.removeItem(n.weightData),s}}const Ea="://";class Aa{constructor(){this.managers={}}static getInstance(){return null==Aa.instance&&(Aa.instance=new Aa),Aa.instance}static registerManager(e,t){l(null!=e,(()=>"scheme must not be undefined or null.")),e.endsWith(Ea)&&(e=e.slice(0,e.indexOf(Ea))),l(e.length>0,(()=>"scheme must not be an empty string."));const n=Aa.getInstance();l(null==n.managers[e],(()=>`A model store manager is already registered for scheme '${e}'.`)),n.managers[e]=t}static getManager(e){const t=this.getInstance().managers[e];if(null==t)throw new Error(`Cannot find model manager for scheme '${e}'`);return t}static getSchemes(){return Object.keys(this.getInstance().managers)}}function Ra(e){if(-1===e.indexOf(Ea))throw new Error(`The url string provided does not contain a scheme. Supported schemes are: ${Aa.getSchemes().join(",")}`);return{scheme:e.split(Ea)[0],path:e.split(Ea)[1]}}async function Fa(e,t,n=!1){l(e!==t,(()=>`Old path and new path are the same: '${e}'`));const s=ia.getLoadHandlers(e);l(s.length>0,(()=>`Copying failed because no load handler is found for source URL ${e}.`)),l(s.length<2,(()=>`Copying failed because more than one (${s.length}) load handlers for source URL ${e}.`));const r=s[0],a=ia.getSaveHandlers(t);l(a.length>0,(()=>`Copying failed because no save handler is found for destination URL ${t}.`)),l(a.length<2,(()=>`Copying failed because more than one (${s.length}) save handlers for destination URL ${t}.`));const i=a[0],o=Ra(e).scheme,u=Ra(e).path,c=o===Ra(e).scheme,h=await r.load();n&&c&&await Aa.getManager(o).removeModel(u);const p=await i.save(h);return n&&!c&&await Aa.getManager(o).removeModel(u),p.modelArtifactsInfo}class _a{fetch(e,t){return fetch(e,t)}now(){return performance.now()}encode(e,t){if("utf-8"!==t&&"utf8"!==t)throw new Error(`Browser's encoder only supports utf-8, but got ${t}`);return null==this.textEncoder&&(this.textEncoder=new TextEncoder),this.textEncoder.encode(e)}decode(e,t){return new TextDecoder(t).decode(e)}}if(K().get("IS_BROWSER")){K().setPlatform("browser",new _a);try{Aa.registerManager($a.URL_SCHEME,new Ta)}catch(e){}try{Aa.registerManager(fa.URL_SCHEME,new ga)}catch(e){}}const Da=()=>require("node-fetch");let Oa;class Ma{constructor(){this.util=require("util"),this.textEncoder=new this.util.TextEncoder}fetch(e,t){return null!=K().global.fetch?K().global.fetch(e,t):(null==Oa&&(Oa=Da()),Oa(e,t))}now(){const e=process.hrtime();return 1e3*e[0]+e[1]/1e6}encode(e,t){if("utf-8"!==t&&"utf8"!==t)throw new Error(`Node built-in encoder only supports utf-8, but got ${t}`);return this.textEncoder.encode(e)}decode(e,t){return 0===e.length?"":new this.util.TextDecoder(t).decode(e)}}function La(e,t="float32",n){return t=t||"float32",W(e),new ur(e,t,n)}K().get("IS_NODE")&&K().setPlatform("node",new Ma);const za=jr({cast_:function(e,t){const n=Ur(e,"x","cast");if(!I(t))throw new Error(`Failed to cast to unknown dtype ${t}`);if("string"===t&&"string"!==n.dtype||"string"!==t&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},r={dtype:t};return Dr.runKernel(we,s,r)}});const Ba=jr({clone_:function(e){const t={x:Ur(e,"x","clone","string_or_numeric")};return Dr.runKernel(ct,t)}});function Pa(e,t=!1){console.log(e.toString(t))}_r();hr={buffer:La,cast:za,clone:Ba,print:Pa};function Wa(e){return new Promise((e=>setTimeout(e))).then(e)}class Va{constructor(e){if(!K().getBool("IS_BROWSER"))throw new Error("browserDownloads() cannot proceed because the current environment is not a browser.");e.startsWith(Va.URL_SCHEME)&&(e=e.slice(Va.URL_SCHEME.length)),null!=e&&0!==e.length||(e="model"),this.modelTopologyFileName=e+".json",this.weightDataFileName=e+".weights.bin"}async save(e){if("undefined"==typeof document)throw new Error("Browser downloads are not supported in this environment since `document` is not present");const t=window.URL.createObjectURL(new Blob([e.weightData],{type:"application/octet-stream"}));if(e.modelTopology instanceof ArrayBuffer)throw new Error("BrowserDownloads.save() does not support saving model topology in binary formats yet.");{const n=[{paths:["./"+this.weightDataFileName],weights:e.weightSpecs}],s={modelTopology:e.modelTopology,format:e.format,generatedBy:e.generatedBy,convertedBy:e.convertedBy,weightsManifest:n};null!=e.signature&&(s.signature=e.signature),null!=e.userDefinedMetadata&&(s.userDefinedMetadata=e.userDefinedMetadata),null!=e.modelInitializer&&(s.modelInitializer=e.modelInitializer);const r=window.URL.createObjectURL(new Blob([JSON.stringify(s)],{type:"application/json"})),a=null==this.jsonAnchor?document.createElement("a"):this.jsonAnchor;if(a.download=this.modelTopologyFileName,a.href=r,await Wa((()=>a.dispatchEvent(new MouseEvent("click")))),null!=e.weightData){const e=null==this.weightDataAnchor?document.createElement("a"):this.weightDataAnchor;e.download=this.weightDataFileName,e.href=t,await Wa((()=>e.dispatchEvent(new MouseEvent("click"))))}return{modelArtifactsInfo:ra(e)}}}}Va.URL_SCHEME="downloads://";class Ua{constructor(e){if(null==e||e.length<1)throw new Error(`When calling browserFiles, at least 1 file is required, but received ${e}`);this.files=e}async load(){const e=this.files[0],t=this.files.slice(1);return new Promise(((n,s)=>{const r=new FileReader;r.onload=r=>{const a=JSON.parse(r.target.result),i=a.modelTopology;if(null==i)return void s(new Error(`modelTopology field is missing from file ${e.name}`));0===t.length&&n({modelTopology:i});const o=a.weightsManifest;if(null==o)return void s(new Error(`weightManifest field is missing from file ${e.name}`));let l;try{l=this.checkManifestAndWeightFiles(o,t)}catch(e){return void s(e)}const u=[],c=[],h=[];o.forEach((e=>{e.paths.forEach((e=>{c.push(e),h.push(null)})),u.push(...e.weights)})),o.forEach((e=>{e.paths.forEach((e=>{const t=new FileReader;t.onload=t=>{const s=t.target.result,r=c.indexOf(e);if(h[r]=s,-1===h.indexOf(null)){const e={modelTopology:i,weightSpecs:u,weightData:na(h),format:a.format,generatedBy:a.generatedBy,convertedBy:a.convertedBy};null!=a.signature&&(e.signature=a.signature),null!=a.userDefinedMetadata&&(e.userDefinedMetadata=a.userDefinedMetadata),null!=a.modelInitializer&&(e.modelInitializer=a.modelInitializer),n(e)}},t.onerror=t=>s(`Failed to weights data from file of path '${e}'.`),t.readAsArrayBuffer(l[e])}))}))},r.onerror=t=>s(`Failed to read model topology and weights manifest JSON from file '${e.name}'. BrowserFiles supports loading Keras-style tf.Model artifacts only.`),r.readAsText(e)}))}checkManifestAndWeightFiles(e,t){const n=[],s=t.map((e=>sa(e.name))),r={};for(const a of e)a.paths.forEach((e=>{const a=sa(e);if(-1!==n.indexOf(a))throw new Error(`Duplicate file basename found in weights manifest: '${a}'`);if(n.push(a),-1===s.indexOf(a))throw new Error(`Weight file with basename '${a}' is not provided.`);r[e]=t[s.indexOf(a)]}));if(n.length!==t.length)throw new Error(`Mismatch in the number of files in weights manifest (${n.length}) and the number of weight files provided (${t.length}).`);return r}}function Ga(e,t,n,s){!function(e){l(null!=e&&Array.isArray(e)&&e.length>0,(()=>"promises must be a none empty array"))}(e),function(e,t){l(e>=0&&e<=1,(()=>`Progress fraction must be in range [0, 1], but got startFraction ${e}`)),l(t>=0&&t<=1,(()=>`Progress fraction must be in range [0, 1], but got endFraction ${t}`)),l(t>=e,(()=>`startFraction must be no more than endFraction, but got startFraction ${e} and endFraction ${t}`))}(n=null==n?0:n,s=null==s?1:s);let r=0;return Promise.all(e.map((a=>(a.then((a=>{const i=n+ ++r/e.length*(s-n);return t(i),a})),a))))}async function Ha(e,t){null==t&&(t={});const n=null==t.fetchFunc?K().platform.fetch:t.fetchFunc,s=e.map((e=>n(e,t.requestInit,{isBinary:!0}))),r=(null==t.onProgress?await Promise.all(s):await Ga(s,t.onProgress,0,.5)).map((e=>e.arrayBuffer()));return null==t.onProgress?await Promise.all(r):await Ga(r,t.onProgress,.5,1)}async function ja(e,t="",n,s){return qa((e=>Ha(e,{requestInit:s})))(e,t,n)}function qa(e){return async(t,n="",s)=>{const r=t.map((()=>!1)),a={},i=null!=s?s.map((()=>!1)):[],o=[];if(t.forEach(((e,t)=>{let n=0;e.weights.forEach((e=>{const l="quantization"in e?e.quantization.dtype:e.dtype,u=Yr[l]*p(e.shape),c=()=>{r[t]=!0,null==a[t]&&(a[t]=[]),a[t].push({manifestEntry:e,groupOffset:n,sizeBytes:u})};null!=s?s.forEach(((t,n)=>{t===e.name&&(c(),i[n]=!0)})):c(),o.push(e.name),n+=u}))})),!i.every((e=>e))){const e=s.filter(((e,t)=>!i[t]));throw new Error(`Could not find weights in manifest with names: ${e.join(", ")}. \nManifest JSON has weights with names: ${o.join(", ")}.`)}const l=r.reduce(((e,t,n)=>(t&&e.push(n),e)),[]),u=[];l.forEach((e=>{t[e].paths.forEach((e=>{const t=n+(n.endsWith("/")?"":"/")+e;u.push(t)}))}));const c=await e(u),h={};let d=0;return l.forEach((e=>{const n=t[e].paths.length;let s=0;for(let e=0;e<n;e++)s+=c[d+e].byteLength;const r=new ArrayBuffer(s),i=new Uint8Array(r);let o=0;for(let e=0;e<n;e++){const t=new Uint8Array(c[d+e]);i.set(t,o),o+=t.byteLength}a[e].forEach((e=>{const t=Jr(r.slice(e.groupOffset,e.groupOffset+e.sizeBytes),[e.manifestEntry]);for(const e in t)h[e]=t[e]})),d+=n})),h}}ia.registerSaveRouter((e=>K().getBool("IS_BROWSER")&&!Array.isArray(e)&&e.startsWith(Va.URL_SCHEME)?function(e="model"){return new Va(e)}(e.slice(Va.URL_SCHEME.length)):null));class Ka{constructor(e,t){if(this.DEFAULT_METHOD="POST",null==t&&(t={}),this.weightPathPrefix=t.weightPathPrefix,this.onProgress=t.onProgress,this.weightUrlConverter=t.weightUrlConverter,null!=t.fetchFunc?(l("function"==typeof t.fetchFunc,(()=>"Must pass a function that matches the signature of `fetch` (see https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)")),this.fetch=t.fetchFunc):this.fetch=K().platform.fetch,l(null!=e&&e.length>0,(()=>"URL path for http must not be null, undefined or empty.")),Array.isArray(e)&&l(2===e.length,(()=>`URL paths for http must have a length of 2, (actual length is ${e.length}).`)),this.path=e,null!=t.requestInit&&null!=t.requestInit.body)throw new Error("requestInit is expected to have no pre-existing body, but has one.");this.requestInit=t.requestInit||{}}async save(e){if(e.modelTopology instanceof ArrayBuffer)throw new Error("BrowserHTTPRequest.save() does not support saving model topology in binary formats yet.");const t=Object.assign({method:this.DEFAULT_METHOD},this.requestInit);t.body=new FormData;const n=[{paths:["./model.weights.bin"],weights:e.weightSpecs}],s={modelTopology:e.modelTopology,format:e.format,generatedBy:e.generatedBy,convertedBy:e.convertedBy,weightsManifest:n};null!=e.signature&&(s.signature=e.signature),null!=e.userDefinedMetadata&&(s.userDefinedMetadata=e.userDefinedMetadata),null!=e.modelInitializer&&(s.modelInitializer=e.modelInitializer),t.body.append("model.json",new Blob([JSON.stringify(s)],{type:"application/json"}),"model.json"),null!=e.weightData&&t.body.append("model.weights.bin",new Blob([e.weightData],{type:"application/octet-stream"}),"model.weights.bin");const r=await this.fetch(this.path,t);if(r.ok)return{modelArtifactsInfo:ra(e),responses:[r]};throw new Error(`BrowserHTTPRequest.save() failed due to HTTP response status ${r.status}.`)}async load(){const e=await this.fetch(this.path,this.requestInit);if(!e.ok)throw new Error(`Request to ${this.path} failed with status code ${e.status}. Please verify this URL points to the model JSON of the model to load.`);let t;try{t=await e.json()}catch(e){let t=`Failed to parse model JSON of response from ${this.path}.`;throw this.path.endsWith(".pb")?t+=" Your path contains a .pb file extension. Support for .pb models have been removed in TensorFlow.js 1.0 in favor of .json models. You can re-convert your Python TensorFlow model using the TensorFlow.js 1.0 conversion scripts or you can convert your.pb models with the 'pb2json'NPM script in the tensorflow/tfjs-converter repository.":t+=" Please make sure the server is serving valid JSON for this request.",new Error(t)}const n=t.modelTopology,s=t.weightsManifest,r=t.generatedBy,a=t.convertedBy,i=t.format,o=t.signature,l=t.userDefinedMetadata;if(null==n&&null==s)throw new Error(`The JSON from HTTP path ${this.path} contains neither model topology or manifest for weights.`);let u,c;if(null!=s){const e=await this.loadWeights(s);[u,c]=e}const h={modelTopology:n,weightSpecs:u,weightData:c,generatedBy:r,convertedBy:a,format:i};null!=o&&(h.signature=o),null!=l&&(h.userDefinedMetadata=l);const p=t.modelInitializer;return p&&(h.modelInitializer=p),h}async loadWeights(e){const t=Array.isArray(this.path)?this.path[1]:this.path,[n,s]=function(e){const t=e.lastIndexOf("/"),n=e.lastIndexOf("?"),s=e.substring(0,t),r=n>t?e.substring(n):"";return[s+"/",r]}(t),r=this.weightPathPrefix||n,a=[];for(const t of e)a.push(...t.weights);const i=[],o=[];for(const t of e)for(const e of t.paths)null!=this.weightUrlConverter?o.push(this.weightUrlConverter(e)):i.push(r+e+s);this.weightUrlConverter&&i.push(...await Promise.all(o));return[a,na(await Ha(i,{requestInit:this.requestInit,fetchFunc:this.fetch,onProgress:this.onProgress}))]}}function Xa(e){return null!=e.match(Ka.URL_SCHEME_REGEX)}Ka.URL_SCHEME_REGEX=/^https?:\/\//;const Ya=(e,t)=>{if("undefined"==typeof fetch&&(null==t||null==t.fetchFunc))return null;{let n=!0;if(n=Array.isArray(e)?e.every((e=>Xa(e))):Xa(e),n)return Za(e,t)}return null};function Za(e,t){return new Ka(e,t)}function Ja(e,t){return Za(e,t)}ia.registerSaveRouter(Ya),ia.registerLoadRouter(Ya);class Qa{constructor(e){this.modelArtifacts=e}async load(){return this.modelArtifacts}}class ei{constructor(e){this.saveHandler=e}async save(e){return this.saveHandler(e)}}var ti=Object.freeze({__proto__:null,browserFiles:function(e){return new Ua(e)},browserHTTPRequest:Ja,concatenateArrayBuffers:na,decodeWeights:Jr,encodeWeights:Zr,fromMemory:function(e,t,n,s){if(1===arguments.length){return null!=e.modelTopology||null!=e.weightSpecs?new Qa(e):(console.warn("Please call tf.io.fromMemory() with only one argument. The argument should be of type ModelArtifacts. The multi-argument signature of tf.io.fromMemory() has been deprecated and will be removed in a future release."),new Qa({modelTopology:e}))}return console.warn("Please call tf.io.fromMemory() with only one argument. The argument should be of type ModelArtifacts. The multi-argument signature of tf.io.fromMemory() has been deprecated and will be removed in a future release."),new Qa({modelTopology:e,weightSpecs:t,weightData:n,trainingConfig:s})},getLoadHandlers:la,getModelArtifactsInfoForJSON:ra,getSaveHandlers:oa,http:Za,isHTTPScheme:Xa,loadWeights:ja,registerLoadRouter:e=>ia.registerLoadRouter(e),registerSaveRouter:e=>ia.registerSaveRouter(e),weightsLoaderFactory:qa,withSaveHandler:function(e){return new ei(e)},copyModel:async function(e,t){return Fa(e,t,!1)},listModels:async function(){const e=Aa.getSchemes(),t={};for(const n of e){const e=await Aa.getManager(n).listModels();for(const s in e){t[n+Ea+s]=e[s]}}return t},moveModel:async function(e,t){return Fa(e,t,!0)},removeModel:async function(e){const t=Ra(e);return Aa.getManager(t.scheme).removeModel(t.path)}});const ni=jr({matMul_:function(e,t,n=!1,s=!1){let r=Ur(e,"a","matMul"),a=Ur(t,"b","matMul");[r,a]=Ir(r,a);const i={a:r,b:a},o={transposeA:n,transposeB:s};return Dr.runKernel(ge,i,o)}});const si=jr({oneHot_:function(e,t,n=1,s=0){if(t<2)throw new Error(`Error in oneHot: depth must be >=2, but it is ${t}`);const r={indices:Ur(e,"indices","oneHot","int32")},a={depth:t,onValue:n,offValue:s};return Dr.runKernel(Kt,r,a)}});const ri=jr({transpose_:function(e,t){const n=Ur(e,"x","transpose");if(null==t&&(t=n.shape.map(((e,t)=>t)).reverse()),l(n.rank===t.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${t}.`)),t.forEach((e=>{l(e>=0&&e<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${t}`))})),n.rank<=1)return n.clone();const s={x:n},r={perm:t};return Dr.runKernel(Hn,s,r)}});const ai=jr({confusionMatrix_:function(e,t,n){const s=Ur(e,"labels","confusionMatrix"),r=Ur(t,"predictions","confusionMatrix");l(null==n||n>0&&Number.isInteger(n),(()=>`If provided, numClasses must be a positive integer, but got ${n}`)),l(1===s.rank,(()=>`Expected the rank of labels to be 1, but got ${s.rank}`)),l(1===r.rank,(()=>`Expected the rank of predictions to be 1, but got ${r.rank}`)),l(s.shape[0]===r.shape[0],(()=>`Mismatch in the number of examples: ${s.shape[0]} vs. ${r.shape[0]}. Labels and predictions should have the same number of elements.`)),l(n>0&&Number.isInteger(n),(()=>`numClasses is required to be a positive integer, but got ${n}`));const a=si(za(s,"int32"),n),i=si(za(r,"int32"),n),o=ri(a),u=ni(o,i);return za(u,"int32")}});var ii=Object.freeze({__proto__:null,confusionMatrix:ai});function oi(e,t,n){if(c(e),null!=t&&3!==t.length)throw new Error("tensor3d() requires shape to have three numbers");const s=Pr(e,n);if(3!==s.length&&1!==s.length)throw new Error("tensor3d() requires values to be number[][][] or flat/TypedArray");if(1===s.length&&null==t)throw new Error("tensor3d() requires shape to be provided when `values` are a flat array");return Kr(e,t,s,n)}let li;function ui(e,t=3){if(t>4)throw new Error("Cannot construct Tensor with more than 4 channels from pixels.");if(null==e)throw new Error("pixels passed to tf.browser.fromPixels() can not be null");let n=!1,s=!1,r=!1,a=!1,i=!1,o=!1;if(e.data instanceof Uint8Array)n=!0;else if("undefined"!=typeof ImageData&&e instanceof ImageData)s=!0;else if("undefined"!=typeof HTMLVideoElement&&e instanceof HTMLVideoElement)r=!0;else if("undefined"!=typeof HTMLImageElement&&e instanceof HTMLImageElement)a=!0;else if(null!=e.getContext)i=!0;else{if(!("undefined"!=typeof ImageBitmap&&e instanceof ImageBitmap))throw new Error(`pixels passed to tf.browser.fromPixels() must be either an HTMLVideoElement, HTMLImageElement, HTMLCanvasElement, ImageData in browser, or OffscreenCanvas, ImageData in webworker or {data: Uint32Array, width: number, height: number}, but was ${e.constructor.name}`);o=!0}if(r){const t=2;if(r&&e.readyState<t)throw new Error("The video element has not loaded data yet. Please wait for `loadeddata` event on the <video> element.")}if(null!=rs(Zn,Dr.backendName)){const n={pixels:e},s={numChannels:t};return Dr.runKernel(Zn,n,s)}const[l,u]=r?[e.videoWidth,e.videoHeight]:[e.width,e.height];let c,h;if(i?c=e.getContext("2d").getImageData(0,0,l,u).data:s||n?c=e.data:(a||r||o)&&(null==li&&(li=document.createElement("canvas").getContext("2d")),li.canvas.width=l,li.canvas.height=u,li.drawImage(e,0,0,l,u),c=li.getImageData(0,0,l,u).data),4===t)h=new Int32Array(c);else{const e=l*u;h=new Int32Array(e*t);for(let n=0;n<e;n++)for(let e=0;e<t;++e)h[n*t+e]=c[4*n+e]}return oi(h,[u,l,t],"int32")}function ci(e){return"undefined"!=typeof window&&"undefined"!=typeof ImageBitmap&&window.hasOwnProperty("createImageBitmap")&&!(e instanceof ImageBitmap)&&function(e){return null!=e&&0!==e.width&&0!==e.height}(e)&&!function(e){return null!=e&&e.data instanceof Uint8Array}(e)}const hi=jr({fromPixels_:ui});var pi=Object.freeze({__proto__:null,fromPixelsAsync:async function(e,t=3){let n=null;if(K().getBool("WRAP_TO_IMAGEBITMAP")&&ci(e)){let t;try{t=await createImageBitmap(e,{premultiplyAlpha:"none"})}catch(e){t=null}n=null!=t&&t.width===e.width&&t.height===e.height?t:e}else n=e;return ui(n,t)},toPixels:async function(e,t){let n=Ur(e,"img","toPixels");if(!(e instanceof dr)){const e=n;n=za(e,"int32"),e.dispose()}if(2!==n.rank&&3!==n.rank)throw new Error(`toPixels only supports rank 2 or 3 tensors, got rank ${n.rank}.`);const[s,r]=n.shape.slice(0,2),a=2===n.rank?1:n.shape[2];if(a>4||2===a)throw new Error(`toPixels only supports depth of size 1, 3 or 4 but got ${a}`);if("float32"!==n.dtype&&"int32"!==n.dtype)throw new Error(`Unsupported type for toPixels: ${n.dtype}. Please use float32 or int32 tensors.`);const i=await n.data(),o="float32"===n.dtype?255:1,l=new Uint8ClampedArray(r*s*4);for(let e=0;e<s*r;++e){const t=[0,0,0,255];for(let s=0;s<a;s++){const r=i[e*a+s];if("float32"===n.dtype){if(r<0||r>1)throw new Error(`Tensor values for a float32 Tensor must be in the range [0 - 1] but encountered ${r}.`)}else if("int32"===n.dtype&&(r<0||r>255))throw new Error(`Tensor values for a int32 Tensor must be in the range [0 - 255] but encountered ${r}.`);1===a?(t[0]=r*o,t[1]=r*o,t[2]=r*o):t[s]=r*o}const s=4*e;l[s+0]=Math.round(t[0]),l[s+1]=Math.round(t[1]),l[s+2]=Math.round(t[2]),l[s+3]=Math.round(t[3])}if(null!=t){t.width=r,t.height=s;const e=t.getContext("2d"),n=new ImageData(l,r,s);e.putImageData(n,0,0)}return n!==e&&n.dispose(),l},fromPixels:hi});function di(e,t){const n=e.shape.length,s=t.shape.length;if(n<1)throw new Error(`tf.gatherND() expects the input to be rank 1 or higher, but the rank was ${n}.`);if(s<1)throw new Error(`tf.gatherND() expects the indices to be rank 1 or higher, but the rank was ${s}.`);if("int32"!==t.dtype)throw new Error(`tf.gatherND() expects the indices to be int32 type, but the dtype was ${t.dtype}.`);if(t.shape[s-1]>n)throw new Error(`index innermost dimension length must be <= tensor rank; saw: ${t.shape[s-1]} vs. ${n}`);if(0===p(e.shape))throw new Error(`Requested more than 0 entries, but input is empty. Input shape: ${e.shape}.`);const r=t.shape,a=r[r.length-1];let i=1;for(let e=0;e<r.length-1;++e)i*=r[e];const o=e.shape,l=r.slice();l.pop();let u=1;for(let e=a;e<n;++e)u*=o[e],l.push(o[e]);const c=[...O(e.shape).map((e=>e/u)),1].slice(0,a);return[l,i,u,c]}var fi=Object.freeze({__proto__:null,prepareAndValidate:di});function mi(e,t,n){const s=t.rank>1?t.shape[t.rank-1]:1,r=t.rank>1?t.rank-1:1,a=`Must have updates.shape = indices.shape[:batchDim] + shape[sliceDim:], got updates.shape: ${n.shape}, indices.shape: ${t.shape}, shape: ${e}, sliceDim: ${s}, and batchDim: ${r}.`;if(n.rank<r)throw new Error(a+` update.rank < ${r}. `);if(e.length<s+(n.rank-r))throw new Error(a+` Output shape length < ${s+(n.rank-r)}`);if(n.rank!==r+e.length-s)throw new Error(a+" update.rank != "+(r+e.length-s));for(let e=0;e<r;++e)if(n.shape[e]!==t.shape[e])throw new Error(a+` updates.shape[${e}] (${n.shape[e]}) != indices.shape[${e}] (${t.shape[e]}).`);for(let t=0;t<n.rank-r;++t)if(n.shape[t+r]!==e[t+s])throw new Error(a+` updates.shape[${t+r}] (${n.shape[t+r]}) != shape[${t+r}] (${e[t+r]})`)}function gi(e,t,n){if(t.rank<1)throw new Error(`tf.scatterND() expects the indices to be rank 1 or higher, but the rank was ${t.rank}.`);if(e.rank<1)throw new Error(`tf.scatterND() expects the updates to be rank 1 or higher, but the rank was ${e.rank}.`);if("int32"!==t.dtype)throw new Error(`The dtype of 'indices' should be int32, but got dtype: ${t.dtype}`);if(n.length<1)throw new Error(`Output rank must be greater or equal to 1, but got shape: ${n}`);if(0===n.length){if(0===t.size)throw new Error(`Indices specified for empty output. indices shape: ${t.shape}`);if(0===e.size)throw new Error(`Updates specified for empty output. updates shape: ${e.shape}`)}mi(n,t,e)}function yi(e,t,n){const s=t.shape.length,r=s>1?t.shape[s-1]:1,a=n.length;let i=1;for(let e=r;e<a;++e)i*=n[e];const o=r<1?1:r;return{sliceRank:r,numUpdates:p(t.shape)/o,sliceSize:i,strides:[...O(n.slice(0,r)),1],outputSize:p(n)}}var bi=Object.freeze({__proto__:null,validateUpdateShape:mi,validateInput:gi,calculateShapes:yi});function xi(e,t,n){const s=e.shape.length;l(s===t.length,(()=>`Error in slice${s}D: Length of begin ${t} must match the rank of the array (${s}).`)),l(s===n.length,(()=>`Error in slice${s}D: Length of size ${n} must match the rank of the array (${s}).`));for(let r=0;r<s;++r)l(t[r]+n[r]<=e.shape[r],(()=>`Error in slice${s}D: begin[${r}] + size[${r}] (${t[r]+n[r]}) would overflow input.shape[${r}] (${e.shape[r]})`))}function wi(e){const t=[];let n=0;for(;e>0;)1&e&&t.push(n),e/=2,n++;return t}function ki(e,t,n){const s=[];for(let r=0;r<e.length;r++)s[r]=Math.ceil((t[r]-e[r])/n[r]);return s}function vi(e,t,n,s){const r=[...e];for(let e=r.length;e<s.length;e++)r.push(1);for(let e=0;e<n;e++)0===e?r[t]=1:(r.splice(t,0,1),r.pop());return r}function Ni(e,t,n){return n<=e?n:n-(t-1)}function Ii(e,t){const n=[];for(let s=0;s<e;s++)n.push(t+s);return n}function Si(e,t,n,s,r,a,i,o,l){const u=e.length;let c=new Array(u),h=new Array(u),p=new Array(u);if(t.length&&n>0){const l=t[0],u=n+1;c=$i(i,l,u,s,e),h=Ci(o,l,u,r,e),p=vi(a,l,u,e)}else for(let t=0;t<u;t++)c[t]=Ei(i,s,a,e,t,l),h[t]=Ai(o,r,a,e,t,l),p[t]=Ti(a,t,l);return{begin:c,end:h,strides:p}}function $i(e,t,n,s,r){const a=[...r],i=Ii(n,t);for(let r=0;r<a.length;r++)if(i.indexOf(r)>-1)a[r]=0;else{const i=Ni(t,n,r);let o=s[i];e&1<<i&&(o=0),a[r]=o}return a}function Ci(e,t,n,s,r){const i=[...r],o=Ii(n,t);for(let r=0;r<i.length;r++)if(o.indexOf(r)>-1)i[r]=Number.MAX_SAFE_INTEGER;else{const a=Ni(t,n,r);let o=s[a];e&1<<a&&(o=Number.MAX_SAFE_INTEGER),i[r]=o}for(let e=0;e<i.length;e++){const t=r[e];i[e]<0&&(i[e]+=t),i[e]=a(0,i[e],r[e])}return i}function Ti(e,t,n){let s=e[t];return(n&1<<t||null==s)&&(s=1),s}function Ei(e,t,n,s,r,i){let o=t[r];const l=n[r]||1;(e&1<<r||i&1<<r||null==o)&&(o=l>0?Number.MIN_SAFE_INTEGER:Number.MAX_SAFE_INTEGER);const u=s[r];return o<0&&(o+=u),o=a(0,o,u-1),o}function Ai(e,t,n,s,r,i){let o=t[r];const l=n[r]||1;(e&1<<r||i&1<<r||null==o)&&(o=l>0?Number.MAX_SAFE_INTEGER:Number.MIN_SAFE_INTEGER);const u=s[r];return o<0&&(o+=u),o=l>0?a(0,o,u):a(-1,o,u-1),o}function Ri(e,t,n){let s=n.length;for(let e=0;e<n.length;e++)if(n[e]>1){s=e;break}for(let r=s+1;r<n.length;r++)if(t[r]>0||n[r]!==e[r])return!1;return!0}function Fi(e,t){let n=e.length>0?e[e.length-1]:1;for(let s=0;s<e.length-1;s++)n+=e[s]*t[s];return n}function _i(e,t,n){let s;const r=e.shape.length;let a;return s="number"==typeof t?[t,...new Array(r-1).fill(0)]:t.length<r?t.concat(new Array(r-t.length).fill(0)):t.slice(),s.forEach((e=>{l(-1!==e,(()=>"slice() does not support negative begin indexing."))})),a=null==n?new Array(r).fill(-1):"number"==typeof n?[n,...new Array(r-1).fill(-1)]:n.length<r?n.concat(new Array(r-n.length).fill(-1)):n,a=a.map(((t,n)=>t>=0?t:(l(-1===t,(()=>`Negative size values should be exactly -1 but got ${t} for the slice() size at index ${n}.`)),e.shape[n]-s[n]))),[s,a]}function Di(e,t,n,s,r,a,i,o,l){let u=t.slice(),c=n.slice(),h=s;null==s&&(h=new Array(u.length));const p=wi(i);if(p.length>1)throw new Error("Multiple ellipses in slice is not allowed.");if(0!==i&&0!==o)throw new Error("Using both ellipsisMask and newAxisMask is not yet supported.");if(0!==i&&0!==l)throw new Error("Using both ellipsisMask and shrinkAxisMask is not yet supported.");const d=e.length-u.length,f=wi(o),m=e.slice();f.forEach((e=>{u[e]=0,c[e]=1,m.splice(e,0,1)}));const{begin:g,end:y,strides:b}=Si(m,p,d,u,c,h,r,a,i);u=g,c=y,h=b;const x=wi(l);x.forEach((e=>{c[e]=u[e]+1,h[e]=1}));const w=ki(u,c,h),k=w.filter(((e,t)=>-1===x.indexOf(t)));return{nonStrided:h.every((e=>1===e)),$begin:u,$end:c,$strides:h,size:w,newShape:m,outShape:k}}var Oi=Object.freeze({__proto__:null,assertParamsValid:xi,maskToAxes:wi,computeOutShape:ki,stridesWithElidedDims:vi,getNormalizedAxes:Si,startIndicesWithElidedDims:$i,stopIndicesWithElidedDims:Ci,stridesForAxis:Ti,startForAxis:Ei,stopForAxis:Ai,isSliceContinous:Ri,computeFlatOffset:Fi,parseSliceParams:_i,sliceInfo:Di});class Mi{getClassName(){return this.constructor.className}static fromConfig(e,t){return new e(t)}}class Li{constructor(){this.classNameMap={}}static getMap(){return null==Li.instance&&(Li.instance=new Li),Li.instance}static register(e){Li.getMap().classNameMap[e.className]=[e,e.fromConfig]}}function zi(e){l(null!=e.className,(()=>"Class being registered does not have the static className property defined.")),l("string"==typeof e.className,(()=>"className is required to be a string, but got type "+typeof e.className)),l(e.className.length>0,(()=>"Class being registered has an empty-string as its className, which is disallowed.")),Li.register(e)}var Bi=Object.freeze({__proto__:null,Serializable:Mi,SerializationMap:Li,registerClass:zi});function Pi(){return 32===Dr.backend.floatPrecision()?.001:.1}function Wi(e,t,n){let s=!0;if(($(e)||$(t))&&(s=!1),$(e)&&$(t)&&(s=!0),s){const n=e.constructor.name,s=t.constructor.name;if(n!==s)throw new Error(`Arrays are of different type. Actual: ${n}. Expected: ${s}`)}if(Array.isArray(e)&&Array.isArray(t)){const n=Pr(e),s=Pr(t);if(!d(n,s))throw new Error(`Arrays have different shapes. Actual: [${n}]. Expected: [${s}]`)}const r=$(e)?e:h(e),a=$(t)?t:h(t);if(r.length!==a.length)throw new Error(`Arrays have different lengths actual: ${r.length} vs expected: ${a.length}.\nActual:   ${r}.\nExpected: ${a}.`);for(let e=0;e<a.length;++e){const t=r[e],s=a[e];if(!n(t,s))throw new Error(`Arrays differ: actual[${e}] = ${t}, expected[${e}] = ${s}.\nActual:   ${r}.\nExpected: ${a}.`)}}function Vi(e,t,n){if(null==n&&(n=Pi()),!Ui(e,t,n))throw new Error(`Numbers differ: actual === ${e}, expected === ${t}`)}function Ui(e,t,n){return!isFinite(e)&&!isFinite(t)||!(isNaN(e)||isNaN(t)||Math.abs(e-t)>n)}var Gi=Object.freeze({__proto__:null,TEST_EPSILON_FLOAT16:.1,expectArraysClose:function(e,t,n){return null==n&&(n=Pi()),Wi(e,t,((e,t)=>Ui(e,t,n)))},testEpsilon:Pi,expectPromiseToFail:function(e,t){e().then((()=>t.fail()),(()=>t()))},expectArraysEqual:function(e,t){const n="string"==typeof t||"number"==typeof t||"boolean"==typeof t?[t]:t;return E(e)||E(e[0])||E(t)||E(t[0])?Wi(e,n,((e,t)=>e==t)):Wi(e,t,((e,t)=>Ui(e,t,0)))},expectNumbersClose:Vi,expectValuesInRange:function(e,t,n){for(let s=0;s<e.length;s++)if(e[s]<t||e[s]>n)throw new Error(`Value out of range:${e[s]} low: ${t}, high: ${n}`)},expectArrayBuffersEqual:function(e,t){expect(new Float32Array(e)).toEqual(new Float32Array(t))},encodeStrings:function e(t){for(let n=0;n<t.length;n++){const s=t[n];Array.isArray(s)?e(s):t[n]=Js(s)}return t}});const Hi="3.7.0";function ji(e){K().getBool("DEPRECATION_WARNINGS_ENABLED")&&console.warn(e+" You can disable deprecation warnings with tf.disableDeprecationWarnings().")}function qi(){return Dr}function Ki(){return Dr.memory()}function Xi(e,t){return Dr.tidy(e,t)}function Yi(e){Cr(e).forEach((e=>e.dispose()))}function Zi(e){return Dr.keep(e)}function Ji(e){return Dr.setBackend(e)}function Qi(e,t,n=1){return Dr.registerBackend(e,t,n)}function eo(){return Dr.backend}pr=ji;const to=jr({add_:function(e,t){let n=Ur(e,"a","add"),s=Ur(t,"b","add");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(te,r)}});const no=jr({floorDiv_:function(e,t){let n=Ur(e,"a","floorDiv"),s=Ur(t,"b","floorDiv");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(rt,r)}});const so=jr({div_:function(e,t){let n=Ur(e,"a","div"),s=Ur(t,"b","div");if([n,s]=Ir(n,s),"int32"===n.dtype&&"int32"===s.dtype)return no(n,s);const r={a:n,b:s};return Dr.runKernel(He,r,{})}});const ro=jr({mul_:function(e,t){let n=Ur(e,"a","mul"),s=Ur(t,"b","mul");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(Wt,r)}});const ao=jr({abs_:function(e){const t=Ur(e,"x","abs");if("complex64"===t.dtype){const e={x:t};return Dr.runKernel(Ie,e)}{const e={x:t};return Dr.runKernel(J,e)}}});const io=jr({acos_:function(e){const t={x:Ur(e,"x","acos")};return Dr.runKernel(Q,t)}});const oo=jr({acosh_:function(e){const t={x:Ur(e,"x","acosh")};return Dr.runKernel(ee,t)}});const lo=jr({addN_:function(e){l(Array.isArray(e),(()=>"The argument passed to tf.addN() must be a list of tensors")),l(e.length>=1,(()=>`Must pass at least one tensor to tf.addN(), but got ${e.length}`));const t=e.map(((e,t)=>Ur(e,`tensors${t}`,"addN"))),n=t[0];t.forEach((e=>{if(e.dtype!==n.dtype)throw new Error("All tensors passed to tf.addN() must have the same dtype")})),t.forEach((e=>{if(!d(e.shape,n.shape))throw new Error("All tensors passed to tf.addN() must have the same shape")}));const s=t;return Dr.runKernel(ne,s)}});const uo=jr({all_:function(e,t=null,n=!1){const s={x:Ur(e,"x","all","bool")},r={axis:t,keepDims:n};return Dr.runKernel(se,s,r)}});const co=jr({any_:function(e,t=null,n=!1){const s={x:Ur(e,"x","any","bool")},r={axis:t,keepDims:n};return Dr.runKernel(re,s,r)}});const ho=jr({argMax_:function(e,t=0){const n={x:Ur(e,"x","argMax")},s={axis:t};return Dr.runKernel(ae,n,s)}});const po=jr({argMin_:function(e,t=0){const n={x:Ur(e,"x","argMin")},s={axis:t};return Dr.runKernel(ie,n,s)}});const fo=jr({asin_:function(e){const t={x:Ur(e,"x","asin")};return Dr.runKernel(oe,t)}});const mo=jr({asinh_:function(e){const t={x:Ur(e,"x","asinh")};return Dr.runKernel(le,t)}});const go=jr({atan_:function(e){const t={x:Ur(e,"x","atan")};return Dr.runKernel(ue,t)}});const yo=jr({atan2_:function(e,t){let n=Ur(e,"a","atan2"),s=Ur(t,"b","atan2");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(he,r)}});const bo=jr({atanh_:function(e){const t={x:Ur(e,"x","atanh")};return Dr.runKernel(ce,t)}});function xo(e,t,n,s,r="NHWC",a){return vo(e,[...t,e[3]],n,a,s,null,null,Ro(r))}function wo(e,t,n,s,r,a,i="channelsLast"){const[o,l]=So(t);let u;if("channelsLast"===i)u=[o,l,e[3],e[3]];else{if("channelsFirst"!==i)throw new Error(`Unknown dataFormat ${i}`);u=[o,l,e[1],e[1]]}return vo(e,u,n,s,r,a,!1,i)}function ko(e,t,n,s,r,a,i="NDHWC"){const[o,l,u]=$o(t);let c,h;if("NDHWC"===i)h="channelsLast",c=[o,l,u,e[4],e[4]];else{if("NCDHW"!==i)throw new Error(`Unknown dataFormat ${i}`);h="channelsFirst",c=[o,l,u,e[1],e[1]]}return No(e,c,n,s,r,!1,h,a)}function vo(e,t,n,s,r,a,i=!1,o="channelsLast"){let[l,u,c,h]=[-1,-1,-1,-1];if("channelsLast"===o)[l,u,c,h]=e;else{if("channelsFirst"!==o)throw new Error(`Unknown dataFormat ${o}`);[l,h,u,c]=e}const[p,d,,f]=t,[m,g]=So(n),[y,b]=So(s),x=Co(p,y),w=Co(d,b),{padInfo:k,outHeight:v,outWidth:N}=function(e,t,n,s,r,a,i,o,l){let u,c,h;if("number"==typeof e){u={top:e,bottom:e,left:e,right:e,type:0===e?"VALID":"NUMBER"};const r=function(e,t,n,s,r){null==s&&(s=Io(e,t,n));const a=e[0],i=e[1],o=To((a-t+2*s)/n+1,r),l=To((i-t+2*s)/n+1,r);return[o,l]}([t,n],a,s,e,o);c=r[0],h=r[1]}else if("same"===e){c=Math.ceil(t/s),h=Math.ceil(n/r);const e=Math.max(0,(c-1)*s+a-t),o=Math.max(0,(h-1)*r+i-n),l=Math.floor(e/2),p=e-l,d=Math.floor(o/2);u={top:l,bottom:p,left:d,right:o-d,type:"SAME"}}else if("valid"===e)u={top:0,bottom:0,left:0,right:0,type:"VALID"},c=Math.ceil((t-a+1)/s),h=Math.ceil((n-i+1)/r);else{if("object"!=typeof e)throw Error(`Unknown padding parameter: ${e}`);{const p="channelsLast"===l?e[1][0]:e[2][0],d="channelsLast"===l?e[1][1]:e[2][1],f="channelsLast"===l?e[2][0]:e[3][0],m="channelsLast"===l?e[2][1]:e[3][1];u={top:p,bottom:d,left:f,right:m,type:0===p&&0===d&&0===f&&0===m?"VALID":"EXPLICIT"},c=To((t-a+p+d)/s+1,o),h=To((n-i+f+m)/r+1,o)}}return{padInfo:u,outHeight:c,outWidth:h}}(r,u,c,m,g,x,w,a,o),I=i?f*h:f;let S;return"channelsFirst"===o?S=[l,I,v,N]:"channelsLast"===o&&(S=[l,v,N,I]),{batchSize:l,dataFormat:o,inHeight:u,inWidth:c,inChannels:h,outHeight:v,outWidth:N,outChannels:I,padInfo:k,strideHeight:m,strideWidth:g,filterHeight:p,filterWidth:d,effectiveFilterHeight:x,effectiveFilterWidth:w,dilationHeight:y,dilationWidth:b,inShape:e,outShape:S,filterShape:t}}function No(e,t,n,s,r,a=!1,i="channelsLast",o){let[l,u,c,h,p]=[-1,-1,-1,-1,-1];if("channelsLast"===i)[l,u,c,h,p]=e;else{if("channelsFirst"!==i)throw new Error(`Unknown dataFormat ${i}`);[l,p,u,c,h]=e}const[d,f,m,,g]=t,[y,b,x]=$o(n),[w,k,v]=$o(s),N=Co(d,w),I=Co(f,k),S=Co(m,v),{padInfo:$,outDepth:C,outHeight:T,outWidth:E}=function(e,t,n,s,r,a,i,o,l,u,c){let h,p,d,f;if("number"==typeof e){h={top:e,bottom:e,left:e,right:e,front:e,back:e,type:0===e?"VALID":"NUMBER"};const a=function(e,t,n,s,r,a){null==r&&(r=Io(e,t,s));const i=e[0],o=e[1],l=e[2],u=To((i-t+2*r)/s+1,a),c=To((o-t+2*r)/s+1,a),h=To((l-t+2*r)/s+1,a);return[u,c,h,n]}([t,n,s,1],o,1,r,e,c);p=a[0],d=a[1],f=a[2]}else if("same"===e){p=Math.ceil(t/r),d=Math.ceil(n/a),f=Math.ceil(s/i);const e=(p-1)*r+o-t,c=(d-1)*a+l-n,m=(f-1)*i+u-s,g=Math.floor(e/2),y=e-g,b=Math.floor(c/2),x=c-b,w=Math.floor(m/2);h={top:b,bottom:x,left:w,right:m-w,front:g,back:y,type:"SAME"}}else{if("valid"!==e)throw Error(`Unknown padding parameter: ${e}`);h={top:0,bottom:0,left:0,right:0,front:0,back:0,type:"VALID"},p=Math.ceil((t-o+1)/r),d=Math.ceil((n-l+1)/a),f=Math.ceil((s-u+1)/i)}return{padInfo:h,outDepth:p,outHeight:d,outWidth:f}}(r,u,c,h,y,b,x,N,I,S,o),A=a?g*p:g;let R;return"channelsFirst"===i?R=[l,A,C,T,E]:"channelsLast"===i&&(R=[l,C,T,E,A]),{batchSize:l,dataFormat:i,inDepth:u,inHeight:c,inWidth:h,inChannels:p,outDepth:C,outHeight:T,outWidth:E,outChannels:A,padInfo:$,strideDepth:y,strideHeight:b,strideWidth:x,filterDepth:d,filterHeight:f,filterWidth:m,effectiveFilterDepth:N,effectiveFilterHeight:I,effectiveFilterWidth:S,dilationDepth:w,dilationHeight:k,dilationWidth:v,inShape:e,outShape:R,filterShape:t}}function Io(e,t,n,s=1){const r=Co(t,s);return Math.floor((e[0]*(n-1)-n+r)/2)}function So(e){return"number"==typeof e?[e,e,e]:2===e.length?[e[0],e[1],1]:e}function $o(e){return"number"==typeof e?[e,e,e]:e}function Co(e,t){return t<=1?e:e+(e-1)*(t-1)}function To(e,t){if(!t)return Math.trunc(e);switch(t){case"round":return Math.round(e);case"ceil":return Math.ceil(e);case"floor":return Math.floor(e);default:throw new Error(`Unknown roundingMode ${t}`)}}function Eo(e){const[t,n,s]=So(e);return 1===t&&1===n&&1===s}function Ao(e,t){return Eo(e)||Eo(t)}function Ro(e){if("NHWC"===e)return"channelsLast";if("NCHW"===e)return"channelsFirst";throw new Error(`Unknown dataFormat ${e}`)}const Fo=jr({reshape_:function(e,t){const n={x:Ur(e,"x","reshape","string_or_numeric")},s={shape:t};return Dr.runKernel(rn,n,s)}});const _o=jr({avgPool_:function(e,t,n,s,r){const a=Ur(e,"x","avgPool","float32");l(Ao(n,1),(()=>`Error in avgPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`));let i=a,o=!1;3===a.rank&&(o=!0,i=Fo(a,[1,a.shape[0],a.shape[1],a.shape[2]])),l(4===i.rank,(()=>`Error in avgPool: x must be rank 4 but got rank ${i.rank}.`)),null!=r&&l(f(s),(()=>`Error in avgPool: pad must be an integer when using, dimRoundingMode ${r} but got pad ${s}.`));const u={x:i},c={filterSize:t,strides:n,pad:s,dimRoundingMode:r};let h=Dr.runKernel(pe,u,c);return h=za(h,a.dtype),o?Fo(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Do=jr({avgPool3d_:function(e,t,n,s,r,a="NDHWC"){const i=Ur(e,"x","avgPool3d","float32");let o=i,u=!1;4===i.rank&&(u=!0,o=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2],i.shape[3]])),l(5===o.rank,(()=>`Error in avgPool3d: x must be rank 5 but got rank ${o.rank}.`)),l("NDHWC"===a,(()=>`Error in avgPool3d: Only NDHWC is currently supported, but got dataFormat of ${a}`)),null!=r&&l(f(s),(()=>`Error in avgPool3d: pad must be an integer when using, dimRoundingMode ${r} but got pad ${s}.`));const c={x:o},h={filterSize:t,strides:n,pad:s,dimRoundingMode:r,dataFormat:a};let p=Dr.runKernel(fe,c,h);return p=za(p,o.dtype),u?Fo(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}});const Oo=jr({concat_:function(e,t=0){l(e.length>=1,(()=>"Pass at least one tensor to concat"));const n=Gr(e,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((e=>{if("complex64"!==e.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${e.dtype}. `)})),1===n.length)return Ba(n[0]);const s=n,r={axis:t};return Dr.runKernel(Se,s,r)}});const Mo=jr({sigmoid_:function(e){const t={x:Ur(e,"x","sigmoid")};return Dr.runKernel(kn,t)}});const Lo=jr({slice_:function(e,t,n){const s=Ur(e,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const r={x:s},a={begin:t,size:n};return Dr.runKernel(yn,r,a)}});const zo=jr({tanh_:function(e){const t={x:Ur(e,"x","tanh")};return Dr.runKernel(Wn,t)}});const Bo=jr({basicLSTMCell_:function(e,t,n,s,r,a){const i=Ur(e,"forgetBias","basicLSTMCell"),o=Ur(t,"lstmKernel","basicLSTMCell"),l=Ur(n,"lstmBias","basicLSTMCell"),u=Ur(s,"data","basicLSTMCell"),c=Ur(r,"c","basicLSTMCell"),h=Ur(a,"h","basicLSTMCell"),p=Oo([u,h],1),d=ni(p,o),f=to(d,l),m=f.shape[0],g=f.shape[1]/4,y=[m,g],b=Lo(f,[0,0],y),x=Lo(f,[0,g],y),w=Lo(f,[0,2*g],y),k=Lo(f,[0,3*g],y),v=to(ro(Mo(b),zo(x)),ro(c,Mo(to(i,w))));return[v,ro(zo(v),Mo(k))]}});const Po=jr({batchToSpaceND_:function(e,t,n){const s=Ur(e,"x","batchToSpaceND"),r=t.reduce(((e,t)=>e*t));l(s.rank>=1+t.length,(()=>`input rank is ${s.rank} but should be > than blockShape.length ${t.length}`)),l(n.length===t.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${t.length}`)),l(s.shape[0]%r==0,(()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${t.join(" * ")} === ${r}`));const a={x:s},i={blockShape:t,crops:n};return Dr.runKernel(ye,a,i)}});const Wo=jr({batchNorm_:function(e,t,n,s,r,a){null==a&&(a=.001);const i=Ur(e,"x","batchNorm"),o=Ur(t,"mean","batchNorm"),u=Ur(n,"variance","batchNorm");let c,h;null!=r&&(c=Ur(r,"scale","batchNorm")),null!=s&&(h=Ur(s,"offset","batchNorm")),l(o.rank===u.rank,(()=>"Batch normalization gradient requires mean and variance to have equal ranks.")),l(null==h||o.rank===h.rank,(()=>"Batch normalization gradient requires mean and offset to have equal ranks.")),l(null==c||o.rank===c.rank,(()=>"Batch normalization gradient requires mean and scale to have equal ranks."));const p={x:function(e){let t;return t=0===e.rank||1===e.rank?Fo(e,[1,1,1,e.size]):2===e.rank?Fo(e,[1,1,e.shape[0],e.shape[1]]):3===e.rank?Fo(e,[1,e.shape[0],e.shape[1],e.shape[2]]):e,t}(i),scale:c,offset:h,mean:o,variance:u},d={varianceEpsilon:a},f=Dr.runKernel(at,p,d);return Fo(f,i.shape)}});const Vo=jr({batchNorm2d_:function(e,t,n,s,r,a){const i=Ur(e,"x","batchNorm"),o=Ur(t,"mean","batchNorm"),u=Ur(n,"variance","batchNorm");let c,h;return null!=r&&(c=Ur(r,"scale","batchNorm")),null!=s&&(h=Ur(s,"offset","batchNorm")),l(2===i.rank,(()=>`Error in batchNorm2D: x must be rank 2 but got rank ${i.rank}.`)),l(2===o.rank||1===o.rank,(()=>`Error in batchNorm2D: mean must be rank 2 or rank 1 but got rank ${o.rank}.`)),l(2===u.rank||1===u.rank,(()=>`Error in batchNorm2D: variance must be rank 2 or rank 1 but got rank ${u.rank}.`)),null!=c&&l(2===c.rank||1===c.rank,(()=>`Error in batchNorm2D: scale must be rank 2 or rank 1 but got rank ${c.rank}.`)),null!=h&&l(2===h.rank||1===h.rank,(()=>`Error in batchNorm2D: offset must be rank 2 or rank 1 but got rank ${h.rank}.`)),Wo(i,o,u,h,c,a)}});const Uo=jr({batchNorm3d_:function(e,t,n,s,r,a){const i=Ur(e,"x","batchNorm"),o=Ur(t,"mean","batchNorm"),u=Ur(n,"variance","batchNorm");let c,h;return null!=r&&(c=Ur(r,"scale","batchNorm")),null!=s&&(h=Ur(s,"offset","batchNorm")),l(3===i.rank,(()=>`Error in batchNorm3D: x must be rank 3 but got rank ${i.rank}.`)),l(3===o.rank||1===o.rank,(()=>`Error in batchNorm3D: mean must be rank 3 or rank 1 but got rank ${o.rank}.`)),l(3===u.rank||1===u.rank,(()=>`Error in batchNorm3D: variance must be rank 3 or rank 1 but got rank ${u.rank}.`)),null!=c&&l(3===c.rank||1===c.rank,(()=>`Error in batchNorm3D: scale must be rank 3 or rank 1 but got rank ${c.rank}.`)),null!=h&&l(3===h.rank||1===h.rank,(()=>`Error in batchNorm3D: offset must be rank 3 or rank 1 but got rank ${h.rank}.`)),Wo(i,o,u,h,c,a)}});const Go=jr({batchNorm4d_:function(e,t,n,s,r,a){const i=Ur(e,"x","batchNorm"),o=Ur(t,"mean","batchNorm"),u=Ur(n,"variance","batchNorm");let c,h;return null!=r&&(c=Ur(r,"scale","batchNorm")),null!=s&&(h=Ur(s,"offset","batchNorm")),l(4===i.rank,(()=>`Error in batchNorm4D: x must be rank 4 but got rank ${i.rank}.`)),l(4===o.rank||1===o.rank,(()=>`Error in batchNorm4D: mean must be rank 4 or rank 1 but got rank ${o.rank}.`)),l(4===u.rank||1===u.rank,(()=>`Error in batchNorm4D: variance must be rank 4 or rank 1 but got rank ${u.rank}.`)),null!=c&&l(4===c.rank||1===c.rank,(()=>`Error in batchNorm4D: scale must be rank 4 or rank 1 but got rank ${c.rank}.`)),null!=h&&l(4===h.rank||1===h.rank,(()=>`Error in batchNorm4D: offset must be rank 4 or rank 1 but got rank ${h.rank}.`)),Wo(i,o,u,h,c,a)}});const Ho=jr({bincount_:function(e,t,n){const s=Ur(e,"x","bincount"),r=Ur(t,"weights","bincount");l("int32"===s.dtype,(()=>`Error in bincount: input dtype must be int32, but got ${s.dtype}`)),l(n>=0,(()=>`size must be non-negative, but got ${n}.`)),l(r.size===s.size||0===r.size,(()=>`Error in bincount: weights must have the same size as input or0-length, but got input shape: ${s.shape}, weights shape: ${r.shape}.`));const a={x:s,weights:r},i={size:n};return Dr.runKernel(be,a,i)}});const jo=jr({broadcastTo_:function(e,t){let n=Ur(e,"broadcastTo","x");const s=n.shape;if(t.some((e=>!(e>0)||e%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${t}].`);if(t.length<n.rank)throw new Error(`broadcastTo(): shape.length=${t.length} < input.rank=${n.rank}.`);if(t.length>n.rank){const e=n.shape.slice();for(;e.length<t.length;)e.unshift(1);n=Fo(n,e)}const r=n.shape,a=Array.from(t);for(let e=t.length-1;e>=0;e--)if(r[e]===t[e])a[e]=1;else if(1!==n.shape[e])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${t}].`);if(0===a.map(((e,t)=>e>1?t:-1)).filter((e=>e>=0)).length)return Ba(n);const i={x:n},o={reps:a};return Dr.runKernel(Vn,i,o)}});const qo=jr({ceil_:function(e){const t={x:Ur(e,"x","ceil")};return Dr.runKernel(ke,t)}});const Ko=jr({clipByValue_:function(e,t,n){const s=Ur(e,"x","clipByValue");l(t<=n,(()=>`Error in clip: min (${t}) must be less than or equal to max (${n}).`));const r={x:s},a={clipValueMin:t,clipValueMax:n};return Dr.runKernel(ve,r,a)}});const Xo=jr({concat1d_:function(e){return Oo(e,0)}});const Yo=jr({concat2d_:function(e,t){return Oo(e,t)}});const Zo=jr({concat3d_:function(e,t){return Oo(e,t)}});const Jo=jr({concat4d_:function(e,t){return Oo(e,t)}});const Qo=jr({conv2d_:function(e,t,n,s,r="NHWC",a=[1,1],i){const o=Ur(e,"x","conv2d"),u=Ur(t,"filter","conv2d");let c=o,h=!1;3===o.rank&&(h=!0,c=Fo(o,[1,o.shape[0],o.shape[1],o.shape[2]])),l(4===c.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${c.rank}.`)),l(4===u.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${u.rank}.`)),null!=i&&l(f(s),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`));const p="NHWC"===r?c.shape[3]:c.shape[1];l(p===u.shape[2],(()=>`Error in conv2d: depth of input (${p}) must match input depth for filter ${u.shape[2]}.`)),l(Ao(n,a),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${a}'`));const d={x:c,filter:u},m={strides:n,pad:s,dataFormat:r,dilations:a,dimRoundingMode:i},g=Dr.runKernel($e,d,m);return h?Fo(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const el=jr({conv1d_:function(e,t,n,s,r="NWC",a=1,i){const o=Ur(e,"x","conv1d"),u=Ur(t,"filter","conv1d");let c=o,h=!1;2===o.rank&&(h=!0,c=Fo(o,[1,o.shape[0],o.shape[1]])),l(3===c.rank,(()=>`Error in conv1d: input must be rank 3, but got rank ${c.rank}.`)),l(3===u.rank,(()=>`Error in conv1d: filter must be rank 3, but got rank ${u.rank}.`)),null!=i&&l(f(s),(()=>`Error in conv1d: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`)),l(c.shape[2]===u.shape[1],(()=>`Error in conv1d: depth of input (${c.shape[2]}) must match input depth for filter ${u.shape[1]}.`)),l(Ao(n,a),(()=>`Error in conv1D: Either stride or dilation must be 1. Got stride ${n} and dilation '${a}'`)),l("NWC"===r,(()=>`Error in conv1d: got dataFormat of ${r} but only NWC is currently supported.`));const p=Fo(u,[1,u.shape[0],u.shape[1],u.shape[2]]),d=Fo(c,[c.shape[0],1,c.shape[1],c.shape[2]]),m=Qo(d,p,[1,n],s,"NHWC",[1,a],i);return Fo(m,h?[m.shape[2],m.shape[3]]:[m.shape[0],m.shape[2],m.shape[3]])}});const tl=jr({conv2DBackpropInput_:function(e,t,n,s,r,a="NHWC",i){l(e.length===t.rank,(()=>`Length of inShape (${e.length}) and rank of dy (${t.rank}) must match`));let o=e,u=t,c=!1;3===t.rank&&(c=!0,u=Fo(t,[1,t.shape[0],t.shape[1],t.shape[2]]),o=[1,e[0],e[1],e[2]]),l(4===o.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${o.length}.`)),l(4===u.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${u.rank}`)),l(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const h="NHWC"===a?o[3]:o[1],p="NHWC"===a?u.shape[3]:u.shape[1];l(h===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${h}) must match input depth for filter ${n.shape[2]}.`)),l(p===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${p}) must match output depth for filter ${n.shape[3]}.`)),null!=i&&l(f(r),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${i} but got pad ${r}.`));const d={dy:u,filter:n},m={strides:s,pad:r,dataFormat:a,dimRoundingMode:i,inputShape:o},g=Dr.runKernel(Te,d,m);return c?Fo(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const nl=jr({conv2dTranspose_:function(e,t,n,s,r,a){const i=Ur(e,"x","conv2dTranspose"),o=Ur(t,"filter","conv2dTranspose");return tl(n,i,o,s,r,"NHWC",a)}});const sl=jr({conv3d_:function(e,t,n,s,r="NDHWC",a=[1,1,1]){const i=Ur(e,"x","conv3d"),o=Ur(t,"filter","conv3d");let u=i,c=!1;4===i.rank&&(c=!0,u=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2],i.shape[3]])),l(5===u.rank,(()=>`Error in conv3d: input must be rank 5, but got rank ${u.rank}.`)),l(5===o.rank,(()=>`Error in conv3d: filter must be rank 5, but got rank ${o.rank}.`)),l(u.shape[4]===o.shape[3],(()=>`Error in conv3d: depth of input (${u.shape[4]}) must match input depth for filter ${o.shape[3]}.`)),l(Ao(n,a),(()=>`Error in conv3D: Either strides or dilations must be 1. Got strides ${n} and dilations '${a}'`)),l("NDHWC"===r,(()=>`Error in conv3d: got dataFormat of ${r} but only NDHWC is currently supported.`));const h={x:u,filter:o},p={strides:n,pad:s,dataFormat:r,dilations:a},d=Dr.runKernel(Ee,h,p);return c?Fo(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}});const rl=jr({conv3DBackpropInput_:function(e,t,n,s,r){l(e.length===t.rank,(()=>`Length of inShape (${e.length}) and rank of dy (${t.rank}) must match`));let a=e,i=t,o=!1;4===t.rank&&(o=!0,i=Fo(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]),a=[1,e[0],e[1],e[2],e[3]]);const u=a[4],c=i.shape[4];l(5===a.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${a.length}.`)),l(5===i.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${i.rank}`)),l(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),l(u===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${u}) must match input depth for filter ${n.shape[3]}.`)),l(c===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${c}) must match output depth for filter ${n.shape[4]}.`));const h={dy:i,filter:n},p={pad:r,strides:s,inputShape:a},d=Dr.runKernel(Re,h,p);return o?Fo(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}});const al=jr({conv3dTranspose_:function(e,t,n,s,r){const a=Ur(e,"x","conv3dTranspose"),i=Ur(t,"filter","conv3dTranspose");return rl(n,a,i,s,r)}});const il=jr({cos_:function(e){const t={x:Ur(e,"x","cos")};return Dr.runKernel(Fe,t)}});const ol=jr({cosh_:function(e){const t={x:Ur(e,"x","cosh")};return Dr.runKernel(_e,t)}});const ll=jr({cumsum_:function(e,t=0,n=!1,s=!1){const r={x:Ur(e,"x","cumsum")},a={axis:t,exclusive:n,reverse:s};return Dr.runKernel(De,r,a)}});const ul=jr({denseBincount_:function(e,t,n,s=!1){const r=Ur(e,"x","denseBincount"),a=Ur(t,"weights","denseBincount");l("int32"===r.dtype,(()=>`Error in denseBincount: input dtype must be int32, but got ${r.dtype}`)),l(r.rank<=2,(()=>`Error in denseBincount: input must be at most rank 2, but got rank ${r.rank}.`)),l(n>=0,(()=>`size must be non-negative, but got ${n}.`)),l(a.size===r.size||0===a.size,(()=>`Error in denseBincount: weights must have the same shape as x or 0-length, but got x shape: ${r.shape}, weights shape: ${a.shape}.`));const i={x:r,weights:a},o={size:n,binaryOutput:s};return Dr.runKernel(Me,i,o)}});const cl=jr({depthToSpace_:function(e,t,n="NHWC"){const s=Ur(e,"x","depthToSpace"),r="NHWC"===n?s.shape[1]:s.shape[2],a="NHWC"===n?s.shape[2]:s.shape[3],i="NHWC"===n?s.shape[3]:s.shape[1];l(r*t>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${r} and ${t}  for depthToSpace with input shape\n    ${s.shape}`)),l(a*t>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${a} and ${t} for depthToSpace with input shape\n        ${s.shape}`)),l(i%(t*t)==0,(()=>`Dimension size must be evenly divisible by ${t*t} but is ${i} for depthToSpace with input shape ${s.shape}`));const o={x:s},u={blockSize:t,dataFormat:n};return Dr.runKernel(Le,o,u)}});const hl=jr({depthwiseConv2d_:function(e,t,n,s,r="NHWC",a=[1,1],i){const o=Ur(e,"x","depthwiseConv2d"),u=Ur(t,"filter","depthwiseConv2d");let c=o,h=!1;3===o.rank&&(h=!0,c=Fo(o,[1,o.shape[0],o.shape[1],o.shape[2]])),l(4===c.rank,(()=>`Error in depthwiseConv2d: input must be rank 4, but got rank ${c.rank}.`)),l(4===u.rank,(()=>`Error in depthwiseConv2d: filter must be rank 4, but got rank ${u.rank}.`)),l(c.shape[3]===u.shape[2],(()=>`Error in depthwiseConv2d: number of input channels (${c.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`)),null!=i&&l(f(s),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`));const p={x:c,filter:u},d={strides:n,pad:s,dataFormat:r,dilations:a,dimRoundingMode:i},m=Dr.runKernel(ze,p,d);return h?Fo(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const pl=jr({diag_:function(e){const t={x:Ur(e,"x","diag")};return Dr.runKernel(We,t)}});const dl=jr({dilation2d_:function(e,t,n,s,r=[1,1],a="NHWC"){const i=Ur(e,"x","dilation2d"),o=Ur(t,"filter","dilation2d");l(3===i.rank||4===i.rank,(()=>`Error in dilation2d: input must be rank 3 or 4, but got rank ${i.rank}.`)),l(3===o.rank,(()=>`Error in dilation2d: filter must be rank 3, but got rank ${o.rank}.`)),l("NHWC"===a,(()=>`Error in dilation2d: Only NHWC is currently supported, but got dataFormat of ${a}`));let u=i,c=!1;3===i.rank&&(u=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2]]),c=!0);const h={x:u,filter:o},p={strides:n,pad:s,dilations:r},d=Dr.runKernel(Ve,h,p);return c?Fo(d,[d.shape[1],d.shape[2],d.shape[3]]):d}});function fl(e,t){const n=e.length,s=[];for(let r=0;r<n;r++){const a=n-1-r,i=e[a]||1;(t[t.length-1-r]||1)>1&&1===i&&s.unshift(a)}return s}function ml(e,t){const n=[];for(let s=0;s<t.length;s++){const r=e[e.length-s-1],a=t.length-s-1,i=t[a];(null==r||1===r&&i>1)&&n.unshift(a)}return n}function gl(e,t){const n=[],s=Math.max(e.length,t.length);for(let r=0;r<s;r++){let s=e[e.length-r-1];null==s&&(s=1);let a=t[t.length-r-1];if(null==a&&(a=1),1===s)n.unshift(a);else if(1===a)n.unshift(s);else{if(s!==a){throw Error(`Operands could not be broadcast together with shapes ${e} and ${t}.`)}n.unshift(s)}}return n}const yl=jr({equal_:function(e,t){let n=Ur(e,"a","equal","string_or_numeric"),s=Ur(t,"b","equal","string_or_numeric");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(Ye,r)}});const bl=jr({where_:function(e,t,n){const s=Ur(t,"a","where"),r=Ur(n,"b","where"),a=Ur(e,"condition","where","bool"),i=gl(gl(a.shape,s.shape),r.shape),o={condition:jo(a,i),t:jo(s,i),e:jo(r,i)};return Dr.runKernel(mn,o)}});const xl=jr({zerosLike_:function(e){const t={x:Ur(e,"x","zerosLike")};return Dr.runKernel(Xn,t)}});const wl=jr({divNoNan_:function(e,t){let n=Ur(e,"a","div"),s=Ur(t,"b","div");[n,s]=Ir(n,s);const r=so(n,s),a=xl(r),i=yl(s,a);return bl(i,a,r)}});const kl=jr({dot_:function(e,t){const n=Ur(e,"t1","dot"),s=Ur(t,"t2","dot");l(!(1!==n.rank&&2!==n.rank||1!==s.rank&&2!==s.rank),(()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${n.rank} and ${s.rank}.`));const r=1===n.rank?n.size:n.shape[1],a=1===s.rank?s.size:s.shape[0];if(l(r===a,(()=>`Error in dot: inner dimensions of inputs must match, but got ${r} and ${a}.`)),1===n.rank&&1===s.rank){const e=Fo(n,[1,-1]),t=Fo(s,[-1,1]),r=ni(e,t);return Fo(r,[])}if(1===n.rank&&2===s.rank){const e=Fo(n,[1,-1]),t=Fo(s,[s.shape[0],s.shape[1]]),r=ni(e,t);return Fo(r,[r.size])}if(2===n.rank&&1===s.rank){const e=Fo(s,[-1,1]),t=ni(n,e);return Fo(t,[t.size])}{const e=Fo(s,[s.shape[0],s.shape[1]]);return ni(n,e)}}});const vl=jr({einsum_:function(e,...t){const n=t.map(((e,t)=>Ur(e,`tensors${t}`,"einsum"))),s={equation:e};return Dr.runKernel(je,n,s)}});const Nl=jr({elu_:function(e){const t={x:Ur(e,"x","elu")};return Dr.runKernel(qe,t)}});const Il=jr({erf_:function(e){let t=Ur(e,"x","erf");l("int32"===t.dtype||"float32"===t.dtype,(()=>"Input dtype must be `int32` or `float32`.")),"int32"===t.dtype&&(t=za(t,"float32"));const n={x:t};return Dr.runKernel(Xe,n)}});const Sl=jr({exp_:function(e){const t={x:Ur(e,"x","exp")};return Dr.runKernel(Ze,t)}});const $l=jr({expandDims_:function(e,t=0){const n=Ur(e,"x","expandDims","string_or_numeric");l(t<=n.rank,(()=>"Axis must be <= rank of the tensor"));const s={input:n},r={dim:t};return Dr.runKernel(Je,s,r)}});const Cl=jr({expm1_:function(e){const t={x:Ur(e,"x","expm1")};return Dr.runKernel(Qe,t)}});const Tl=jr({tile_:function(e,t){const n=Ur(e,"x","tile","string_or_numeric");l(n.rank===t.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${t}.`));const s={x:n},r={reps:t};return Dr.runKernel(Vn,s,r)}});const El=jr({eye_:function(e,t,n,s="float32"){null==t&&(t=e);const r=La([e,t],s),a=e<=t?e:t;for(let e=0;e<a;++e)r.set(1,e,e);const i=Fo(r.toTensor(),[e,t]);if(null==n)return i;if(1===n.length)return Tl($l(i,0),[n[0],1,1]);if(2===n.length)return Tl($l($l(i,0),0),[n[0],n[1],1,1]);if(3===n.length)return Tl($l($l($l(i,0),0),0),[n[0],n[1],n[2],1,1]);throw new Error(`eye() currently supports only 1D and 2D batchShapes, but received ${n.length}D.`)}});function Al(e,t,n){const s={shape:e,value:t,dtype:n};return Dr.runKernel(tt,{},s)}const Rl=jr({floor_:function(e){const t={x:Ur(e,"x","floor")};return Dr.runKernel(st,t)}});const Fl=jr({gather_:function(e,t,n=0,s=0){const r={x:Ur(e,"x","gather"),indices:Ur(t,"indices","gather","int32")},a={axis:n,batchDims:s};return Dr.runKernel(it,r,a)}});const _l=jr({greater_:function(e,t){let n=Ur(e,"a","greater","string_or_numeric"),s=Ur(t,"b","greater","string_or_numeric");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(lt,r)}});const Dl=jr({greaterEqual_:function(e,t){let n=Ur(e,"a","greaterEqual","string_or_numeric"),s=Ur(t,"b","greaterEqual","string_or_numeric");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(ut,r)}});const Ol=jr({imag_:function(e){const t={input:Ur(e,"input","imag")};return Dr.runKernel(pt,t)}});const Ml=jr({isFinite_:function(e){const t={x:Ur(e,"x","isFinite")};return Dr.runKernel(dt,t)}});const Ll=jr({isInf_:function(e){const t={x:Ur(e,"x","isInf")};return Dr.runKernel(ft,t)}});const zl=jr({isNaN_:function(e){const t={x:Ur(e,"x","isNaN")};return Dr.runKernel(mt,t)}});const Bl=jr({leakyRelu_:function(e,t=.2){const n={x:Ur(e,"x","leakyRelu")},s={alpha:t};return Dr.runKernel(gt,n,s)}});const Pl=jr({less_:function(e,t){let n=Ur(e,"a","less","string_or_numeric"),s=Ur(t,"b","less","string_or_numeric");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(yt,r)}});const Wl=jr({lessEqual_:function(e,t){let n=Ur(e,"a","lessEqual","string_or_numeric"),s=Ur(t,"b","lessEqual","string_or_numeric");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(bt,r)}});function Vl(e,t,n){if(n<=0)throw new Error("The number of values should be positive.");const s={start:e,stop:t,num:n};return Dr.runKernel(xt,{},s)}const Ul=jr({localResponseNormalization_:function(e,t=5,n=1,s=1,r=.5){const a=Ur(e,"x","localResponseNormalization");l(4===a.rank||3===a.rank,(()=>`Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${a.rank}.`)),l(f(t),(()=>`Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${t}.`));let i=a,o=!1;3===a.rank&&(o=!0,i=Fo(a,[1,a.shape[0],a.shape[1],a.shape[2]]));const u={x:i},c={depthRadius:t,bias:n,alpha:s,beta:r},h=Dr.runKernel($t,u,c);return o?Fo(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Gl=jr({log_:function(e){const t={x:Ur(e,"x","log")};return Dr.runKernel(wt,t)}});const Hl=jr({log1p_:function(e){const t={x:Ur(e,"x","log1p")};return Dr.runKernel(kt,t)}});function jl(e,t){l(_(e),(()=>"The f passed in variableGrads(f) must be a function")),l(null==t||Array.isArray(t)&&t.every((e=>e instanceof mr)),(()=>"The varList passed in variableGrads(f, varList) must be an array of variables"));const n=null!=t;if(!n){t=[];for(const e in Dr.registeredVariables)t.push(Dr.registeredVariables[e])}const s=n?t.filter((e=>!e.trainable)):null,r=t.length;l((t=t.filter((e=>e.trainable))).length>0,(()=>`variableGrads() expects at least one of the input variables to be trainable, but none of the ${r} variables is trainable.`));const{value:a,grads:i}=Dr.gradients(e,t,null,!0);l(i.some((e=>null!=e)),(()=>"Cannot find a connection between any variable and the result of the loss function y=f(x). Please make sure the operations that use variables are inside the function f passed to minimize().")),l(0===a.rank,(()=>`The f passed in variableGrads(f) must return a scalar, but it returned a rank-${a.rank} tensor`));const o={};return t.forEach(((e,t)=>{null!=i[t]&&(o[e.name]=i[t])})),null!=s&&s.forEach((e=>o[e.name]=null)),{value:a,grads:o}}function ql(e){return Dr.customGrad(e)}function Kl(e){if(e.filter((e=>null==e)).length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that\n    the f you passed encloses all operations that lead from x to y.")}const Xl=jr({neg_:function(e){const t={x:Ur(e,"x","neg")};return Dr.runKernel(Vt,t)}});const Yl=jr({softplus_:function(e){const t={x:Ur(e,"x","softplus")};return Dr.runKernel(vn,t)}});const Zl=jr({logSigmoid_:function(e){const t=Ur(e,"x","logSigmoid");return ql((e=>({value:Xl(Yl(Xl(e))),gradFunc:t=>ro(t,Mo(Xl(e)))})))(t)}});const Jl=jr({max_:function(e,t=null,n=!1){const s={x:Ur(e,"x","max")},r={reductionIndices:t,keepDims:n};return Dr.runKernel(Tt,s,r)}});const Ql=jr({sub_:function(e,t){let n=Ur(e,"a","sub"),s=Ur(t,"b","sub");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(Bn,r)}});const eu=jr({sum_:function(e,t=null,n=!1){let s=Ur(e,"x","sum");"bool"===s.dtype&&(s=za(s,"int32"));const r={x:s},a={axis:t,keepDims:n};return Dr.runKernel(In,r,a)}});const tu=jr({logSoftmax_:function(e,t=-1){const n=Ur(e,"logits","logSoftmax");if(-1===t&&(t=n.rank-1),t!==n.rank-1)throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and axis was ${t}`);return ql(((e,n)=>{const s=Jl(e,t,!0),r=Ql(e,s),a=Ql(za(r,"float32"),Gl(eu(Sl(r),t,!0)));n([a]);return{value:a,gradFunc:(e,n)=>{const[s]=n,r=Sl(s);return Ql(e,ro(eu(e,t,!0),r))}}}))(n)}});function nu(e,t){for(let n=0;n<e.length;++n)if(e[e.length-n-1]!==t-1-n)return!1;return!0}function su(e,t,n){const s=e.length+t.length,r=[];let a=0,i=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?r.push(e[a++]):r.push(t[i++]);return r}function ru(e,t){const n=[],s=e.length;for(let r=0;r<s;r++)-1===t.indexOf(r)&&n.push(e[r]);return[n,t.map((t=>e[t]))]}function au(e,t){return su(e,t.map((e=>1)),t)}function iu(e,t,n){l(nu(t,n),(()=>`${e} supports only inner-most axes for now. Got axes ${t} and rank-${n} input.`))}function ou(e,t){if(nu(e,t))return null;const n=[];for(let s=0;s<t;++s)-1===e.indexOf(s)&&n.push(s);return e.forEach((e=>n.push(e))),n}function lu(e){return e.map(((e,t)=>[t,e])).sort(((e,t)=>e[1]-t[1])).map((e=>e[0]))}function uu(e,t){const n=[];for(let s=t-e;s<t;++s)n.push(s);return n}const cu=jr({logSumExp_:function(e,t=null,n=!1){const s=Ur(e,"x","logSumExp"),r=x(t,s.shape),a=Jl(s,r,!0),i=Ql(s,a),o=Sl(i),l=eu(o,r),u=Gl(l),c=to(Fo(a,u.shape),u);if(n){const e=au(c.shape,r);return Fo(c,e)}return c}});const hu=jr({logicalAnd_:function(e,t){const n=Ur(e,"a","logicalAnd","bool"),s=Ur(t,"b","logicalAnd","bool");gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(vt,r)}});const pu=jr({logicalNot_:function(e){const t={x:Ur(e,"x","logicalNot","bool")};return Dr.runKernel(Nt,t)}});const du=jr({logicalOr_:function(e,t){const n=Ur(e,"a","logicalOr","bool"),s=Ur(t,"b","logicalOr","bool");gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(It,r)}});const fu=jr({logicalXor_:function(e,t){const n=Ur(e,"a","logicalXor","bool"),s=Ur(t,"b","logicalXor","bool");return gl(n.shape,s.shape),hu(du(e,t),pu(hu(e,t)))}});const mu=jr({maxPool_:function(e,t,n,s,r){const a=Ur(e,"x","maxPool");let i=a,o=!1;3===a.rank&&(o=!0,i=Fo(a,[1,a.shape[0],a.shape[1],a.shape[2]])),l(4===i.rank,(()=>`Error in maxPool: input must be rank 4 but got rank ${i.rank}.`)),l(Ao(n,1),(()=>`Error in maxPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`)),null!=r&&l(f(s),(()=>`Error in maxPool: pad must be an integer when using, dimRoundingMode ${r} but got pad ${s}.`));const u={x:i},c={filterSize:t,strides:n,pad:s,dimRoundingMode:r},h=Dr.runKernel(At,u,c);return o?Fo(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const gu=jr({maxPool3d_:function(e,t=[1,1,1],n,s,r,a="NDHWC"){const i=Ur(e,"x","maxPool3d");let o=i,u=!1;4===i.rank&&(u=!0,o=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2],i.shape[3]])),l(5===o.rank,(()=>`Error in maxPool3d: x must be rank 5 but got rank ${o.rank}.`)),l("NDHWC"===a,(()=>`Error in maxPool3d: Only NDHWC is currently supported, but got dataFormat of ${a}`)),null!=r&&l(f(s),(()=>`Error in maxPool3d: pad must be an integer when using, dimRoundingMode ${r} but got pad ${s}.`));const c={x:o},h={filterSize:t,strides:n,pad:s,dimRoundingMode:r,dataFormat:a},p=Dr.runKernel(Ft,c,h);return u?Fo(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}});const yu=jr({maxPoolWithArgmax_:function(e,t,n,s,r=!1){const a={x:Ur(e,"x","maxPoolWithArgmax")},i={filterSize:t,strides:n,pad:s,includeBatchInIndex:r},o=Dr.runKernel(Dt,a,i);return{result:o[0],indexes:o[1]}}});const bu=jr({maximum_:function(e,t){let n=Ur(e,"a","maximum"),s=Ur(t,"b","maximum");[n,s]=Ir(n,s),"bool"===n.dtype&&(n=za(n,"int32"),s=za(s,"int32")),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(Et,r)}});const xu=jr({mean_:function(e,t=null,n=!1){const s={x:Ur(e,"x","mean")},r={axis:t,keepDims:n};return Dr.runKernel(Ot,s,r)}});function wu(e,t="float32"){if("complex64"===t){const t=wu(e,"float32"),n=wu(e,"float32");return qr(t,n)}const n=B(p(e),t);return Dr.makeTensor(n,e,t)}function ku(e,t="float32"){if("complex64"===t){const t=ku(e,"float32"),n=wu(e,"float32");return qr(t,n)}const n=z(p(e),t);return Dr.makeTensor(n,e,t)}const vu=jr({min_:function(e,t=null,n=!1){const s={x:Ur(e,"x","min")},r={axis:t,keepDims:n};return Dr.runKernel(Mt,s,r)}});const Nu=jr({minimum_:function(e,t){let n=Ur(e,"a","minimum"),s=Ur(t,"b","minimum");[n,s]=Ir(n,s),"bool"===n.dtype&&(n=za(n,"int32"),s=za(s,"int32")),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(Lt,r)}});const Iu=jr({mirrorPad_:function(e,t,n){l("reflect"===n||"symmetric"===n,(()=>`Invalid mode. Mode must be either reflect or symmetric. Got ${n}.`));const s=Ur(e,"x","mirrorPad");if(0===s.rank)throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");l(t.length===s.rank,(()=>`Padding doesn't match input. Must be ${s.rank}. Got ${t.length}.`));const r="reflect"===n?1:0;for(let e=0;e<s.rank;e++)l(2===t[e].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),l(t[e][0]>=0&&t[e][0]<=s.shape[e]-r&&t[e][1]>=0&&t[e][1]<=s.shape[e]-r,(()=>`Padding in dimension ${e} cannot be greater than or equal to ${s.shape[e]-r} or less than 0 for input of shape ${s.shape}`));const a={paddings:t,mode:n},i={x:s};return Dr.runKernel(zt,i,a)}});const Su=jr({mod_:function(e,t){let n=Ur(e,"a","mod"),s=Ur(t,"b","mod");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(Bt,r)}});const $u=jr({square_:function(e){const t=Ur(e,"x","square");return Dr.runKernel("Square",{x:t},{})}});const Cu=jr({moments_:function(e,t=null,n=!1){const s=x(t,(e=Ur(e,"x","moments")).shape),r=xu(e,s,n);let a=r.shape;n||(a=au(r.shape,s));const i=$u(Ql(za(e,"float32"),Fo(r,a)));return{mean:r,variance:xu(i,s,n)}}});const Tu=jr({multiRNNCell_:function(e,t,n,s){const r=Ur(t,"data","multiRNNCell"),a=Gr(n,"c","multiRNNCell"),i=Gr(s,"h","multiRNNCell");let o=r;const l=[];for(let t=0;t<e.length;t++){const n=e[t](o,a[t],i[t]);l.push(n[0]),l.push(n[1]),o=n[1]}const u=[],c=[];for(let e=0;e<l.length;e+=2)u.push(l[e]),c.push(l[e+1]);return[u,c]}});const Eu=jr({multinomial_:function(e,t,n,s=!1){const r=Ur(e,"logits","multinomial"),a=r.size,i=r.rank;if(a<2)throw new Error(`Error in multinomial: you need at least 2 outcomes, but got ${a}.`);if(i>2)throw new Error(`Rank of probabilities must be 1 or 2, but is ${i}`);n=n||Math.random();const o={logits:1===i?Fo(r,[1,-1]):r},l={numSamples:t,seed:n,normalized:s},u=Dr.runKernel(Pt,o,l);return 1===i?Fo(u,[u.size]):u}});const Au=jr({notEqual_:function(e,t){let n=Ur(e,"a","notEqual","string_or_numeric"),s=Ur(t,"b","notEqual","string_or_numeric");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(Ut,r)}});const Ru=jr({onesLike_:function(e){const t={x:Ur(e,"x","onesLike")};return Dr.runKernel(qt,t)}});const Fu=jr({outerProduct_:function(e,t){const n=Ur(e,"v1","outerProduct"),s=Ur(t,"v2","outerProduct");l(1===n.rank&&1===s.rank,(()=>`Error in outerProduct: inputs must be rank 1, but got ranks ${n.rank} and ${s.rank}.`));const r=Fo(n,[-1,1]),a=Fo(s,[1,-1]);return ni(r,a)}});const _u=jr({pad_:function(e,t,n=0){const s=Ur(e,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const r={paddings:t,constantValue:n},a={x:s};return Dr.runKernel(Yt,a,r)}});const Du=jr({pad1d_:function(e,t,n=0){return l(2===t.length,(()=>"Invalid number of paddings. Must be length of 2.")),_u(e,[t],n)}});const Ou=jr({pad2d_:function(e,t,n=0){return l(2===t.length&&2===t[0].length&&2===t[1].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),_u(e,t,n)}});const Mu=jr({pad3d_:function(e,t,n=0){return l(3===t.length&&2===t[0].length&&2===t[1].length&&2===t[2].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),_u(e,t,n)}});const Lu=jr({pad4d_:function(e,t,n=0){return l(4===t.length&&2===t[0].length&&2===t[1].length&&2===t[2].length&&2===t[3].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),_u(e,t,n)}});const zu=jr({spaceToBatchND_:function(e,t,n){const s=Ur(e,"x","spaceToBatchND");l(s.rank>=1+t.length,(()=>`input rank ${s.rank} should be > than [blockShape] ${t.length}`)),l(n.length===t.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${t.length}`)),l(s.shape.reduce(((e,s,r)=>r>0&&r<=t.length?e&&(s+n[r-1][0]+n[r-1][1])%t[r-1]==0:e),!0),(()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${t.toString()}`));const r={x:s},a={blockShape:t,paddings:n};return Dr.runKernel(Sn,r,a)}});const Bu=jr({pool_:function(e,t,n,s,r,a){null==r&&(r=[1,1]),null==a&&(a=1),0===s&&(s="valid");const i=Ur(e,"x","maxPool");let o=i,u=!1;3===i.rank&&(u=!0,o=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2]])),l(Ao(a,r),(()=>`Error in pool: Either strides or dilations must be 1. Got strides ${a} and dilations '${r}'`));const c=wo(o.shape,t,a,r,s),h=[c.dilationHeight,c.dilationWidth];let p;p="same"===s?function(e,t){const n=e.map(((e,n)=>e+(e-1)*(t[n]-1))).map((e=>e-1)),s=n.map((e=>Math.floor(e/2))),r=n.map(((e,t)=>e-s[t]));return n.map(((e,t)=>[s[t],r[t]]))}([c.filterHeight,c.filterWidth],h):[[0,0],[0,0]];const d=1===h[0]&&1===h[1],[f,m]=function(e,t,n){const s=n.map((e=>e[0])),r=n.map((e=>e[1])),a=e.concat(s,r),i=t.map(((e,t)=>(e-a[t]%e)%e)),o=r.map(((e,t)=>e+i[t])),l=t.map(((e,t)=>[s[t],o[t]])),u=t.map(((e,t)=>[0,i[t]]));return[l,u]}([c.inHeight,c.inWidth],h,p),g=d?s:"valid",y=d?o:zu(o,h,f),b=("avg"===n?()=>_o(y,t,a,g):()=>mu(y,t,a,g))(),x=d?b:Po(b,h,m);return u?Fo(x,[x.shape[1],x.shape[2],x.shape[3]]):x}});const Pu=jr({pow_:function(e,t){let n=Ur(e,"base","pow"),s=Ur(t,"exp","pow");[n,s]=Ir(n,s);const r={a:n,b:s};return Dr.runKernel(Zt,r)}});const Wu=jr({prelu_:function(e,t){const n={x:Ur(e,"x","prelu"),alpha:Ur(t,"alpha","prelu")};return Dr.runKernel(Jt,n)}});const Vu=jr({prod_:function(e,t=null,n=!1){let s=Ur(e,"x","prod");"bool"===s.dtype&&(s=za(s,"int32"));const r={x:s},a={axis:t,keepDims:n};return Dr.runKernel(Qt,r,a)}});const Uu=jr({rand_:function(e,t,n){const s=p(e);let r=null;if(null==n||"float32"===n)r=new Float32Array(s);else if("int32"===n)r=new Int32Array(s);else{if("bool"!==n)throw new Error(`Unknown data type ${n}`);r=new Uint8Array(s)}for(let e=0;e<s;e++)r[e]=t();return Dr.makeTensor(r,e,n)}});"undefined"!=typeof globalThis?globalThis:"undefined"!=typeof window?window:"undefined"!=typeof global?global:"undefined"!=typeof self&&self;function Gu(e,t){return e(t={exports:{}},t.exports),t.exports}var Hu=Gu((function(e){!function(e,t,n){function s(e){var t,n=this,s=(t=4022871197,function(e){e=e.toString();for(var n=0;n<e.length;n++){var s=.02519603282416938*(t+=e.charCodeAt(n));s-=t=s>>>0,t=(s*=t)>>>0,t+=4294967296*(s-=t)}return 2.3283064365386963e-10*(t>>>0)});n.next=function(){var e=2091639*n.s0+2.3283064365386963e-10*n.c;return n.s0=n.s1,n.s1=n.s2,n.s2=e-(n.c=0|e)},n.c=1,n.s0=s(" "),n.s1=s(" "),n.s2=s(" "),n.s0-=s(e),n.s0<0&&(n.s0+=1),n.s1-=s(e),n.s1<0&&(n.s1+=1),n.s2-=s(e),n.s2<0&&(n.s2+=1),s=null}function r(e,t){return t.c=e.c,t.s0=e.s0,t.s1=e.s1,t.s2=e.s2,t}function a(e,t){var n=new s(e),a=t&&t.state,i=n.next;return i.int32=function(){return 4294967296*n.next()|0},i.double=function(){return i()+11102230246251565e-32*(2097152*i()|0)},i.quick=i,a&&("object"==typeof a&&r(a,n),i.state=function(){return r(n,{})}),i}t&&t.exports?t.exports=a:n&&n.amd?n((function(){return a})):this.alea=a}(0,e,!1)})),ju=Gu((function(e){!function(e,t,n){function s(e){var t=this,n="";t.x=0,t.y=0,t.z=0,t.w=0,t.next=function(){var e=t.x^t.x<<11;return t.x=t.y,t.y=t.z,t.z=t.w,t.w^=t.w>>>19^e^e>>>8},e===(0|e)?t.x=e:n+=e;for(var s=0;s<n.length+64;s++)t.x^=0|n.charCodeAt(s),t.next()}function r(e,t){return t.x=e.x,t.y=e.y,t.z=e.z,t.w=e.w,t}function a(e,t){var n=new s(e),a=t&&t.state,i=function(){return(n.next()>>>0)/4294967296};return i.double=function(){do{var e=((n.next()>>>11)+(n.next()>>>0)/4294967296)/(1<<21)}while(0===e);return e},i.int32=n.next,i.quick=i,a&&("object"==typeof a&&r(a,n),i.state=function(){return r(n,{})}),i}t&&t.exports?t.exports=a:n&&n.amd?n((function(){return a})):this.xor128=a}(0,e,!1)})),qu=Gu((function(e){!function(e,t,n){function s(e){var t=this,n="";t.next=function(){var e=t.x^t.x>>>2;return t.x=t.y,t.y=t.z,t.z=t.w,t.w=t.v,(t.d=t.d+362437|0)+(t.v=t.v^t.v<<4^e^e<<1)|0},t.x=0,t.y=0,t.z=0,t.w=0,t.v=0,e===(0|e)?t.x=e:n+=e;for(var s=0;s<n.length+64;s++)t.x^=0|n.charCodeAt(s),s==n.length&&(t.d=t.x<<10^t.x>>>4),t.next()}function r(e,t){return t.x=e.x,t.y=e.y,t.z=e.z,t.w=e.w,t.v=e.v,t.d=e.d,t}function a(e,t){var n=new s(e),a=t&&t.state,i=function(){return(n.next()>>>0)/4294967296};return i.double=function(){do{var e=((n.next()>>>11)+(n.next()>>>0)/4294967296)/(1<<21)}while(0===e);return e},i.int32=n.next,i.quick=i,a&&("object"==typeof a&&r(a,n),i.state=function(){return r(n,{})}),i}t&&t.exports?t.exports=a:n&&n.amd?n((function(){return a})):this.xorwow=a}(0,e,!1)})),Ku=Gu((function(e){!function(e,t,n){function s(e){var t=this;t.next=function(){var e,n,s=t.x,r=t.i;return e=s[r],n=(e^=e>>>7)^e<<24,n^=(e=s[r+1&7])^e>>>10,n^=(e=s[r+3&7])^e>>>3,n^=(e=s[r+4&7])^e<<7,e=s[r+7&7],n^=(e^=e<<13)^e<<9,s[r]=n,t.i=r+1&7,n},function(e,t){var n,s=[];if(t===(0|t))s[0]=t;else for(t=""+t,n=0;n<t.length;++n)s[7&n]=s[7&n]<<15^t.charCodeAt(n)+s[n+1&7]<<13;for(;s.length<8;)s.push(0);for(n=0;n<8&&0===s[n];++n);for(8==n?s[7]=-1:s[n],e.x=s,e.i=0,n=256;n>0;--n)e.next()}(t,e)}function r(e,t){return t.x=e.x.slice(),t.i=e.i,t}function a(e,t){null==e&&(e=+new Date);var n=new s(e),a=t&&t.state,i=function(){return(n.next()>>>0)/4294967296};return i.double=function(){do{var e=((n.next()>>>11)+(n.next()>>>0)/4294967296)/(1<<21)}while(0===e);return e},i.int32=n.next,i.quick=i,a&&(a.x&&r(a,n),i.state=function(){return r(n,{})}),i}t&&t.exports?t.exports=a:n&&n.amd?n((function(){return a})):this.xorshift7=a}(0,e,!1)})),Xu=Gu((function(e){!function(e,t,n){function s(e){var t=this;t.next=function(){var e,n,s=t.w,r=t.X,a=t.i;return t.w=s=s+1640531527|0,n=r[a+34&127],e=r[a=a+1&127],n^=n<<13,e^=e<<17,n^=n>>>15,e^=e>>>12,n=r[a]=n^e,t.i=a,n+(s^s>>>16)|0},function(e,t){var n,s,r,a,i,o=[],l=128;for(t===(0|t)?(s=t,t=null):(t+="\0",s=0,l=Math.max(l,t.length)),r=0,a=-32;a<l;++a)t&&(s^=t.charCodeAt((a+32)%t.length)),0===a&&(i=s),s^=s<<10,s^=s>>>15,s^=s<<4,s^=s>>>13,a>=0&&(i=i+1640531527|0,r=0==(n=o[127&a]^=s+i)?r+1:0);for(r>=128&&(o[127&(t&&t.length||0)]=-1),r=127,a=512;a>0;--a)s=o[r+34&127],n=o[r=r+1&127],s^=s<<13,n^=n<<17,s^=s>>>15,n^=n>>>12,o[r]=s^n;e.w=i,e.X=o,e.i=r}(t,e)}function r(e,t){return t.i=e.i,t.w=e.w,t.X=e.X.slice(),t}function a(e,t){null==e&&(e=+new Date);var n=new s(e),a=t&&t.state,i=function(){return(n.next()>>>0)/4294967296};return i.double=function(){do{var e=((n.next()>>>11)+(n.next()>>>0)/4294967296)/(1<<21)}while(0===e);return e},i.int32=n.next,i.quick=i,a&&(a.X&&r(a,n),i.state=function(){return r(n,{})}),i}t&&t.exports?t.exports=a:n&&n.amd?n((function(){return a})):this.xor4096=a}(0,e,!1)})),Yu=Gu((function(e){!function(e,t,n){function s(e){var t=this,n="";t.next=function(){var e=t.b,n=t.c,s=t.d,r=t.a;return e=e<<25^e>>>7^n,n=n-s|0,s=s<<24^s>>>8^r,r=r-e|0,t.b=e=e<<20^e>>>12^n,t.c=n=n-s|0,t.d=s<<16^n>>>16^r,t.a=r-e|0},t.a=0,t.b=0,t.c=-1640531527,t.d=1367130551,e===Math.floor(e)?(t.a=e/4294967296|0,t.b=0|e):n+=e;for(var s=0;s<n.length+20;s++)t.b^=0|n.charCodeAt(s),t.next()}function r(e,t){return t.a=e.a,t.b=e.b,t.c=e.c,t.d=e.d,t}function a(e,t){var n=new s(e),a=t&&t.state,i=function(){return(n.next()>>>0)/4294967296};return i.double=function(){do{var e=((n.next()>>>11)+(n.next()>>>0)/4294967296)/(1<<21)}while(0===e);return e},i.int32=n.next,i.quick=i,a&&("object"==typeof a&&r(a,n),i.state=function(){return r(n,{})}),i}t&&t.exports?t.exports=a:n&&n.amd?n((function(){return a})):this.tychei=a}(0,e,!1)})),Zu=Gu((function(e){!function(t,n){var s,r=this,a=256,i=n.pow(a,6),o=n.pow(2,52),l=2*o,u=255;function c(e,u,c){var g=[],y=f(d((u=1==u?{entropy:!0}:u||{}).entropy?[e,m(t)]:null==e?function(){try{var e;return s&&(e=s.randomBytes)?e=e(a):(e=new Uint8Array(a),(r.crypto||r.msCrypto).getRandomValues(e)),m(e)}catch(e){var n=r.navigator,i=n&&n.plugins;return[+new Date,r,i,r.screen,m(t)]}}():e,3),g),b=new h(g),x=function(){for(var e=b.g(6),t=i,n=0;e<o;)e=(e+n)*a,t*=a,n=b.g(1);for(;e>=l;)e/=2,t/=2,n>>>=1;return(e+n)/t};return x.int32=function(){return 0|b.g(4)},x.quick=function(){return b.g(4)/4294967296},x.double=x,f(m(b.S),t),(u.pass||c||function(e,t,s,r){return r&&(r.S&&p(r,b),e.state=function(){return p(b,{})}),s?(n.random=e,t):e})(x,y,"global"in u?u.global:this==n,u.state)}function h(e){var t,n=e.length,s=this,r=0,i=s.i=s.j=0,o=s.S=[];for(n||(e=[n++]);r<a;)o[r]=r++;for(r=0;r<a;r++)o[r]=o[i=u&i+e[r%n]+(t=o[r])],o[i]=t;(s.g=function(e){for(var t,n=0,r=s.i,i=s.j,o=s.S;e--;)t=o[r=u&r+1],n=n*a+o[u&(o[r]=o[i=u&i+t])+(o[i]=t)];return s.i=r,s.j=i,n})(a)}function p(e,t){return t.i=e.i,t.j=e.j,t.S=e.S.slice(),t}function d(e,t){var n,s=[],r=typeof e;if(t&&"object"==r)for(n in e)try{s.push(d(e[n],t-1))}catch(e){}return s.length?s:"string"==r?e:e+"\0"}function f(e,t){for(var n,s=e+"",r=0;r<s.length;)t[u&r]=u&(n^=19*t[u&r])+s.charCodeAt(r++);return m(t)}function m(e){return String.fromCharCode.apply(0,e)}if(n.seedrandom=c,f(n.random(),t),e.exports){e.exports=c;try{s=require("crypto")}catch(e){}}else 0}([],Math)}));Zu.alea=Hu,Zu.xor128=ju,Zu.xorwow=qu,Zu.xorshift7=Ku,Zu.xor4096=Xu,Zu.tychei=Yu;var Ju=Zu.alea;class Qu{constructor(e,t,n,s,r){this.mean=e,this.stdDev=t,this.dtype=n,this.nextVal=NaN,this.truncated=s,this.truncated&&(this.upper=this.mean+2*this.stdDev,this.lower=this.mean-2*this.stdDev);const a=r||Math.random();this.random=Ju(a.toString())}nextValue(){if(!isNaN(this.nextVal)){const e=this.nextVal;return this.nextVal=NaN,e}let e,t,n=!1;for(;!n;){let s,r,a;do{s=2*this.random()-1,r=2*this.random()-1,a=s*s+r*r}while(a>=1||0===a);const i=Math.sqrt(-2*Math.log(a)/a);e=this.mean+this.stdDev*s*i,t=this.mean+this.stdDev*r*i,this.truncated&&!this.isValidTruncated(e)||(n=!0)}return this.truncated&&!this.isValidTruncated(t)||(this.nextVal=this.convertValue(t)),this.convertValue(e)}convertValue(e){return null==this.dtype||"float32"===this.dtype?e:Math.round(e)}isValidTruncated(e){return e<=this.upper&&e>=this.lower}}class ec{constructor(e,t,n,s){this.alpha=e,this.beta=1/t,this.dtype=n;const r=s||Math.random();this.randu=Ju(r.toString()),this.randn=new Qu(0,1,n,!1,this.randu()),this.d=e<1?e+2/3:e-1/3,this.c=1/Math.sqrt(9*this.d)}nextValue(){let e,t,n,s,r,a;for(;;){do{s=this.randn.nextValue(),a=1+this.c*s}while(a<=0);if(a*=a*a,e=s*s,t=1-.331*e*e,n=.5*e+this.d*(1-a+Math.log(a)),r=this.randu(),r<t||Math.log(r)<n)break}return a=1/this.beta*this.d*a,this.alpha<1&&(a*=Math.pow(this.randu(),1/this.alpha)),this.convertValue(a)}convertValue(e){return"float32"===this.dtype?e:Math.round(e)}}class tc{constructor(e=0,t=1,n,s){if(this.canReturnFloat=()=>null==this.dtype||"float32"===this.dtype,this.min=e,this.range=t-e,this.dtype=n,null==s&&(s=Math.random()),"number"==typeof s&&(s=s.toString()),!this.canReturnFloat()&&this.range<=1)throw new Error(`The difference between ${e} - ${t} <= 1 and dtype is not float`);this.random=Ju(s)}convertValue(e){return this.canReturnFloat()?e:Math.round(e)}nextValue(){return this.convertValue(this.min+this.range*this.random())}}const nc=jr({randomGamma_:function(e,t,n=1,s="float32",r){if(null==n&&(n=1),null==s&&(s="float32"),"float32"!==s&&"int32"!==s)throw new Error(`Unsupported data type ${s}`);const a=new ec(t,n,s,r),i=La(e,s);for(let e=0;e<i.values.length;e++)i.values[e]=a.nextValue();return i.toTensor()}});const sc=jr({randomNormal_:function(e,t=0,n=1,s,r){if(null!=s&&"bool"===s)throw new Error(`Unsupported data type ${s}`);const a=new Qu(t,n,s,!1,r),i=La(e,s);for(let e=0;e<i.values.length;e++)i.values[e]=a.nextValue();return i.toTensor()}});const rc=jr({randomUniform_:function(e,t=0,n=1,s="float32",r){const a=La(e,s),i=new tc(t,n,null,r);for(let e=0;e<a.values.length;e++)a.values[e]=i.nextValue();return a.toTensor()}});function ac(e,t,n=1,s="float32"){if(0===n)throw new Error("Cannot have a step of zero");const r={start:e,stop:t,step:n,dtype:s};return Dr.runKernel(en,{},r)}const ic=jr({real_:function(e){const t={input:Ur(e,"input","real")};return Dr.runKernel(tn,t)}});const oc=jr({reciprocal_:function(e){const t={x:Ur(e,"x","reciprocal")};return Dr.runKernel(nn,t)}});const lc=jr({relu_:function(e){const t={x:Ur(e,"x","relu")};return Dr.runKernel(sn,t)}});const uc=jr({relu6_:function(e){const t={x:Ur(e,"x","relu6")};return Dr.runKernel(cn,t)}});const cc=jr({reverse_:function(e,t){const n={x:Ur(e,"x","reverse")},s={dims:t};return Dr.runKernel(hn,n,s)}});const hc=jr({reverse1d_:function(e){const t=Ur(e,"x","reverse");return l(1===t.rank,(()=>`Error in reverse1D: x must be rank 1 but got rank ${t.rank}.`)),cc(t,0)}});const pc=jr({reverse2d_:function(e,t){const n=Ur(e,"x","reverse");return l(2===n.rank,(()=>`Error in reverse2D: x must be rank 2 but got rank ${n.rank}.`)),cc(n,t)}});const dc=jr({reverse3d_:function(e,t){const n=Ur(e,"x","reverse");return l(3===n.rank,(()=>`Error in reverse3D: x must be rank 3 but got rank ${n.rank}.`)),cc(n,t)}});const fc=jr({reverse4d_:function(e,t){const n=Ur(e,"x","reverse");return l(4===n.rank,(()=>`Error in reverse4D: x must be rank 4 but got rank ${n.rank}.`)),cc(n,t)}});const mc=jr({round_:function(e){const t={x:Ur(e,"x","round")};return Dr.runKernel(pn,t)}});const gc=jr({rsqrt_:function(e){const t={x:Ur(e,"x","rsqrt")};return Dr.runKernel(dn,t)}});function yc(e,t){if(($(e)&&"string"!==t||Array.isArray(e))&&"complex64"!==t)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===t&&$(e)&&!(e instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return Kr(e,[],[],t)}const bc=jr({selu_:function(e){const t={x:Ur(e,"x","selu")};return Dr.runKernel(gn,t)}});const xc=jr({separableConv2d_:function(e,t,n,s,r,a=[1,1],i="NHWC"){const o=Ur(e,"x","separableConv2d"),u=Ur(t,"depthwiseFilter","separableConv2d"),c=Ur(n,"pointwiseFilter","separableConv2d");let h=o,p=!1;if(3===o.rank&&(p=!0,h=Fo(o,[1,o.shape[0],o.shape[1],o.shape[2]])),"NCHW"===i)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");l(4===h.rank,(()=>`Error in separableConv2d: input must be rank 4, but got rank ${h.rank}.`)),l(4===u.rank,(()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${u.rank}.`)),l(4===c.rank,(()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${u.rank}.`)),l(1===c.shape[0],(()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${c.shape[0]}.`)),l(1===c.shape[1],(()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${c.shape[1]}.`));const d=u.shape[2],f=u.shape[3];l(c.shape[2]===d*f,(()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${d*f}, but got ${c.shape[2]}.`));const m=hl(h,u,s,r,i,a),g=Qo(m,c,1,"valid",i);return p?Fo(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const wc=async function(e,t){const n=Ur(e,"x","setdiff1d"),s=Ur(t,"y","setdiff1d");l(n.dtype===s.dtype,(()=>`x and y should have the same dtype, but got x (${n.dtype}) and y (${s.dtype}).`)),l(1===n.rank,(()=>`x should be 1D tensor, but got x (${n.shape}).`)),l(1===s.rank,(()=>`y should be 1D tensor, but got y (${s.shape}).`));const r=await n.data(),a=await s.data(),i=new Set(a);let o=0;for(let e=0;e<r.length;e++)i.has(r[e])||o++;const u=new ur([o],n.dtype),c=new ur([o],"int32");for(let e=0,t=0;e<r.length;e++)i.has(r[e])||(u.values[t]=r[e],c.values[t]=e,t++);return[u.toTensor(),c.toTensor()]};const kc=jr({sign_:function(e){const t={x:Ur(e,"x","sign")};return Dr.runKernel(wn,t)}});const vc=jr({sin_:function(e){const t={x:Ur(e,"x","sin")};return Dr.runKernel(bn,t)}});const Nc=jr({sinh_:function(e){const t={x:Ur(e,"x","sinh")};return Dr.runKernel(xn,t)}});const Ic=jr({slice1d_:function(e,t,n){const s=Ur(e,"x","slice1d");return l(1===s.rank,(()=>`slice1d expects a rank-1 tensor, but got a rank-${s.rank} tensor`)),Lo(s,[t],[n])}});const Sc=jr({slice2d_:function(e,t,n){const s=Ur(e,"x","slice2d");return l(2===s.rank,(()=>`slice2d expects a rank-2 tensor, but got a rank-${s.rank} tensor`)),Lo(s,t,n)}});const $c=jr({slice3d_:function(e,t,n){const s=Ur(e,"x","slice3d");return l(3===s.rank,(()=>`slice3d expects a rank-3 tensor, but got a rank-${s.rank} tensor`)),Lo(s,t,n)}});const Cc=jr({slice4d_:function(e,t,n){const s=Ur(e,"x","slice4d");return l(4===s.rank,(()=>`slice4d expects a rank-4 tensor, but got a rank-${s.rank} tensor`)),Lo(s,t,n)}});const Tc=jr({softmax_:function(e,t=-1){const n=Ur(e,"logits","softmax","float32");if(-1===t&&(t=n.rank-1),t!==n.rank-1)throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and dim was ${t}`);const s={logits:n},r={dim:t};return Dr.runKernel(Cn,s,r)}});const Ec=jr({fft_:function(e){l("complex64"===e.dtype,(()=>`The dtype for tf.spectral.fft() must be complex64 but got ${e.dtype}.`));const t={input:e};return Dr.runKernel(et,t)}});const Ac=jr({ifft_:function(e){l("complex64"===e.dtype,(()=>`The dtype for tf.spectral.ifft() must be complex64 but got ${e.dtype}.`));const t={input:e};return Dr.runKernel(ht,t)}});const Rc=jr({irfft_:function(e){const t=e.shape[e.shape.length-1],n=e.size/t;let s;if(t<=2){const r=Fo(e,[n,t]);s=Ac(r)}else{const r=[n,2*(t-1)],a=Fo(ic(e),[n,t]),i=Fo(Ol(e),[n,t]),o=cc(Lo(a,[0,1],[n,t-2]),1),l=ro(cc(Lo(i,[0,1],[n,t-2]),1),yc(-1)),u=Oo([a,o],1),c=Oo([i,l],1),h=Fo(qr(u,c),[r[0],r[1]]);s=Ac(h)}if(s=ic(s),3===e.rank&&0!==e.shape[0]){const t=s,n=e.shape[0];s=Fo(s,[n,s.shape[0]/n,s.shape[1]]),t.dispose()}return s}});const Fc=jr({split_:function(e,t,n=0){const s={x:Ur(e,"x","split")},r={numOrSizeSplits:t,axis:n};return Dr.runKernel($n,s,r)}});const _c=jr({rfft_:function(e,t){l("float32"===e.dtype,(()=>`The dtype for rfft() must be real value but got ${e.dtype}`));let n=e.shape[e.shape.length-1];const s=e.size/n;let r;if(null!=t&&t<n){const s=e.shape.map((e=>0)),a=e.shape.map((e=>e));a[e.shape.length-1]=t,r=Lo(e,s,a),n=t}else if(null!=t&&t>n){const s=e.shape.map((e=>e));s[e.shape.length-1]=t-n,r=Oo([e,wu(s)],e.shape.length-1),n=t}else r=e;const a=xl(r),i=Fo(qr(r,a),[s,n]),o=Ec(i),u=Math.floor(n/2)+1,c=ic(o),h=Ol(o),p=Fc(c,[u,n-u],c.shape.length-1),d=Fc(h,[u,n-u],h.shape.length-1),f=r.shape.slice();return f[r.shape.length-1]=u,Fo(qr(p[0],d[0]),f)}});const Dc=jr({sqrt_:function(e){const t={x:Ur(e,"x","sqrt")};return Dr.runKernel(Nn,t)}});const Oc=jr({squaredDifference_:function(e,t){let n=Ur(e,"a","squaredDifference"),s=Ur(t,"b","squaredDifference");[n,s]=Ir(n,s),gl(n.shape,s.shape);const r={a:n,b:s};return Dr.runKernel(_n,r,{})}});const Mc=jr({squeeze_:function(e,t){const n=Ur(e,"x","squeeze");return Fo(n,w(n.shape,t).newShape)}});const Lc=jr({stack_:function(e,t=0){const n=Gr(e,"tensors","stack","string_or_numeric");l(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&l(t<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const s=n,r={axis:t};return Dr.runKernel(Xt,s,r)}});const zc=jr({step_:function(e,t=0){const n={x:Ur(e,"x","step")},s={alpha:t};return Dr.runKernel(Yn,n,s)}});const Bc=jr({stridedSlice_:function(e,t,n,s,r=0,a=0,i=0,o=0,l=0){const u={x:Ur(e,"x","stridedSlice","string_or_numeric")},c={begin:t,end:n,strides:s,beginMask:r,endMask:a,ellipsisMask:i,newAxisMask:o,shrinkAxisMask:l};return Dr.runKernel(On,u,c)}});const Pc=jr({tan_:function(e){const t={x:Ur(e,"x","tan")};return Dr.runKernel(Pn,t)}});function Wc(e,t){c(e);const n=Pr(e,t);if(1!==n.length)throw new Error("tensor1d() requires values to be a flat/TypedArray");return Kr(e,null,n,t)}function Vc(e,t,n){if(c(e),null!=t&&2!==t.length)throw new Error("tensor2d() requires shape to have two numbers");const s=Pr(e,n);if(2!==s.length&&1!==s.length)throw new Error("tensor2d() requires values to be number[][] or flat/TypedArray");if(1===s.length&&null==t)throw new Error("tensor2d() requires shape to be provided when `values` are a flat/TypedArray");return Kr(e,t,s,n)}const Uc=jr({topk_:function(e,t=1,n=!0){const s=Ur(e,"x","topk");if(0===s.rank)throw new Error("topk() expects the input to be of rank 1 or higher");const r=s.shape[s.shape.length-1];if(t>r)throw new Error(`'k' passed to topk() must be <= the last dimension (${r}) but got ${t}`);const a={x:s},i={k:t,sorted:n},[o,l]=Dr.runKernel(Un,a,i);return{values:o,indices:l}}});const Gc=jr({truncatedNormal_:function(e,t=0,n=1,s,r){if(null!=s&&"bool"===s)throw new Error("Unsupported data type $ { dtype }");const a=new Qu(t,n,s,!0,r),i=La(e,s);for(let e=0;e<i.values.length;e++)i.values[e]=a.nextValue();return i.toTensor()}});const Hc=jr({unique_:function(e,t=0){const n=Ur(e,"x","unique","string_or_numeric");l(n.rank>0,(()=>"The input tensor must be at least 1D"));const s={x:n},r={axis:t},[a,i]=Dr.runKernel(jn,s,r);return{values:a,indices:i}}});const jc=jr({unsortedSegmentSum_:function(e,t,n){const s=Ur(e,"x","unsortedSegmentSum"),r=Ur(t,"segmentIds","unsortedSegmentSum","int32");l(f(n),(()=>"numSegments must be of dtype int"));const a={x:s,segmentIds:r},i={numSegments:n};return Dr.runKernel(Kn,a,i)}});const qc=jr({unstack_:function(e,t=0){const n=Ur(e,"x","unstack","string_or_numeric");l(t>=-n.shape.length&&t<n.shape.length,(()=>`Axis = ${t} is not in [-${n.shape.length}, ${n.shape.length})`));const s={value:n},r={axis:t};return Dr.runKernel(qn,s,r)}});function Kc(e,t=!0,n,s){return Dr.makeVariable(e,t,n,s)}function Xc(e,t){const n=[];for(let e=0;e<t.length;e++)t[e]&&n.push(e);const s=La(e,"int32"),r=La([n.length,e.length],"int32");for(let t=0;t<n.length;t++){const a=s.indexToLoc(n[t]),i=t*e.length;r.values.set(a,i)}return r.toTensor()}const Yc=async function(e){const t=Ur(e,"condition","whereAsync","bool"),n=await t.data(),s=Xc(t.shape,n);return e!==t&&t.dispose(),s};const Zc=async function(e,t,n){const s=Ur(e,"tensor","boolMask"),r=Ur(t,"mask","boolMask","bool"),a=null==n?0:n,i=r.rank,o=s.shape;l(i>0,(()=>"mask cannot be scalar")),u(o.slice(a,a+i),r.shape,"mask's shape must match the first K dimensions of tensor's shape,");let c=1;for(let e=a;e<a+i;e++)c*=o[e];const h=o.slice(0,a).concat([c],o.slice(a+i)),p=Fo(s,h),d=Fo(r,[-1]),f=await Yc(d),m=Mc(f,[1]),g=Fl(p,m,a);return e!==s&&s.dispose(),t!==r&&r.dispose(),m.dispose(),p.dispose(),d.dispose(),f.dispose(),g};function Jc(e,t,n=null){if(0===e.rank)return ao(e);if(1!==e.rank&&null===n)return Jc(Fo(e,[-1]),t,n);if(1===e.rank||"number"==typeof n||Array.isArray(n)&&1===n.length){if(1===t)return eu(ao(e),n);if(t===1/0)return Jl(ao(e),n);if(t===-1/0)return vu(ao(e),n);if("euclidean"===t||2===t)return Dc(eu(Pu(ao(e),yc(2,"int32")),n));throw new Error(`Error in norm: invalid ord value: ${t}`)}if(Array.isArray(n)&&2===n.length){if(1===t)return Jl(eu(ao(e),n[0]),n[1]-1);if(t===1/0)return Jl(eu(ao(e),n[1]),n[0]);if(t===-1/0)return vu(eu(ao(e),n[1]),n[0]);if("fro"===t||"euclidean"===t)return Dc(eu($u(e),n));throw new Error(`Error in norm: invalid ord value: ${t}`)}throw new Error(`Error in norm: invalid axis: ${n}`)}const Qc=jr({norm_:function(e,t="euclidean",n=null,s=!1){const r=Jc(e=Ur(e,"x","norm"),t,n);let a=r.shape;if(s){const t=x(n,e.shape);a=au(r.shape,t)}return Fo(r,a)}});const eh=jr({movingAverage_:function(e,t,n,s,r=!0){const a=Ur(e,"v","movingAverage"),i=Ur(t,"x","movingAverage"),o=Ur(n,"decay","movingAverage");Sr(a,i),l(d(a.shape,i.shape),(()=>"Shape mismatch in v and x"));const u=yc(1),c=Ql(u,o);let h=ro(Ql(i,a),c);if(r){l(null!=s,(()=>"When using zeroDebias: true, step is required."));const e=Ur(s,"step","movingAverage");h=so(h,Ql(u,Pu(o,e)))}return to(a,h)}});const th=jr({scatterND_:function(e,t,n){const s=Ur(e,"indices","scatterND","int32"),r=Ur(t,"updates","scatterND");gi(r,s,n);const a={indices:s,updates:r},i={shape:n};return Dr.runKernel(fn,a,i)}});const nh=jr({sparseToDense_:function(e,t,n,s=0){const r=Ur(e,"sparseIndices","sparseToDense","int32"),a=Ur(t,"sparseValues","sparseToDense"),i=Ur(s,"defaultValue","sparseToDense",a.dtype);!function(e,t,n,s){if("int32"!==e.dtype)throw new Error(`tf.sparseToDense() expects the indices to be int32 type, but the dtype was ${e.dtype}.`);if(e.rank>2)throw new Error(`sparseIndices should be a scalar, vector, or matrix, but got shape ${e.shape}.`);const r=e.rank>0?e.shape[0]:1,a=e.rank>1?e.shape[1]:1;if(n.length!==a)throw new Error(`outputShape has incorrect number of elements:, ${n.length}, should be: ${a}.`);const i=t.size;if(0!==t.rank&&(1!==t.rank||i!==r))throw new Error(`sparseValues has incorrect shape ${t.shape}, should be [] or [${r}]`);if(t.dtype!==s.dtype)throw new Error("sparseValues.dtype must match defaultValues.dtype")}(r,a,n,i);const o={sparseIndices:r,sparseValues:a,defaultValue:i},l={outputShape:n};return Dr.runKernel(Fn,o,l)}});const sh=jr({gatherND_:function(e,t){const n=Ur(t,"indices","gatherND","int32"),s={params:Ur(e,"x","gatherND","string_or_numeric"),indices:n};return Dr.runKernel(ot,s)}});const rh=jr({dropout_:function(e,t,n,s){const r=Ur(e,"x","dropout");if(l("float32"===r.dtype,(()=>`x has to be a floating point tensor since it's going to be scaled, but got a ${r.dtype} tensor instead.`)),l(t>=0&&t<1,(()=>`rate must be a float in the range [0, 1), but got ${t}.`)),0===t)return e instanceof dr?r.clone():r;const a=function(e,t){if(null==t)return e.shape.slice();if(d(e.shape,t))return t;if(e.shape.length===t.length){const n=[];for(let s=0;s<e.shape.length;s++)null==t[s]&&null!=e.shape[s]?n.push(e.shape[s]):n.push(t[s]);return n}return t}(r,n),i=1-t,o=so(Rl(to(rc(a,0,1,"float32",s),i)),i);return ro(r,o)}});function ah(e){return Math.floor(Math.pow(2,Math.ceil(Math.log(e)/Math.log(2))))}function ih(e,t,n){const s=1-e%2,r=new Float32Array(e);for(let a=0;a<e;++a){const i=2*Math.PI*a/(e+s-1);r[a]=t-n*Math.cos(i)}return Wc(r,"float32")}const oh=async function(e,t,n=1){const s=Ur(e,"predictions","inTopK"),r=Ur(t,"targets","inTopK");l(s.rank>1,(()=>`inTopK() expects the predictions to be of rank 2 or higher, but got ${s.rank}`)),l(s.rank-1===r.rank,(()=>`predictions rank should be 1 larger than targets rank, but got predictions rank ${s.rank} and targets rank ${r.rank}`)),u(s.shape.slice(0,s.shape.length-1),r.shape,"predictions's shape should be align with the targets' shape, except the last dimension.");const a=s.shape[s.shape.length-1];l(n>0&&n<=a,(()=>`'k' passed to inTopK() must be > 0 && <= the predictions last dimension (${a}), but got ${n}`));const i=await s.data(),o=await r.data(),[c,h]=[i.length/a,a],p=k("bool",c);for(let e=0;e<c;e++){const t=e*h,s=i.subarray(t,t+h),r=[];for(let e=0;e<s.length;e++)r.push({value:s[e],index:e});r.sort(((e,t)=>t.value-e.value)),p[e]=0;for(let t=0;t<n;t++)if(r[t].index===o[e]){p[e]=1;break}}return e!==s&&s.dispose(),t!==r&&r.dispose(),Xr(p,r.shape,"bool")};const lh=jr({conv2DBackpropFilter_:function(e,t,n,s,r,a="NHWC",i){let o=e;3===e.rank&&(o=Fo(e,[1,e.shape[0],e.shape[1],e.shape[2]]));let u=t;3===u.rank&&(u=Fo(t,[1,t.shape[0],t.shape[1],t.shape[2]])),l(4===o.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${o.shape}.`)),l(4===u.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${u.shape}.`)),l(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const c="NHWC"===a?o.shape[3]:o.shape[1],h="NHWC"===a?u.shape[3]:u.shape[1];l(c===n[2],(()=>`Error in conv2dDerFilter: depth of input ${c}) must match input depth in filter (${n[2]}.`)),l(h===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`)),null!=i&&l(f(r),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${i} but got pad ${r}.`));const p={x:o,dy:u},d={strides:s,pad:r,dataFormat:a,dimRoundingMode:i,filterShape:n};return Dr.runKernel(Ce,p,d)}});function uh(e,t,n){if(null==n||"linear"===n)return e;if("relu"===n)return ro(e,zc(t));throw new Error(`Cannot compute gradient for fused activation ${n}.`)}function ch(e,t){let n=t;const s=ml(e.shape,t.shape);return s.length>0&&(n=eu(n,s)),Fo(n,e.shape)}function hh(e,t,n,s){if("linear"===t)return e;if("relu"===t)return lc(e);if("elu"===t)return Nl(e);if("relu6"===t)return uc(e);if("prelu"===t)return Wu(e,n);if("leakyrelu"===t)return Bl(e,s);if("sigmoid"===t)return Mo(e);throw new Error(`Unknown fused activation ${t}.`)}const ph=(e,t)=>!(e>0)||"linear"===t;const dh=jr({fusedConv2d_:function({x:e,filter:t,strides:n,pad:s,dataFormat:r="NHWC",dilations:a=[1,1],dimRoundingMode:i,bias:o,activation:u="linear",preluActivationWeights:c,leakyreluAlpha:h}){if(u=u||"linear",!1===ph(Dr.state.gradientDepth,u)){let l=Qo(e,t,n,s,r,a,i);return null!=o&&(l=to(l,o)),hh(l,u,c,h)}const p=Ur(e,"x","conv2d"),d=Ur(t,"filter","conv2d");let m=p,g=!1;3===p.rank&&(g=!0,m=Fo(p,[1,p.shape[0],p.shape[1],p.shape[2]])),l(4===m.rank,(()=>`Error in fused conv2d: input must be rank 4, but got rank ${m.rank}.`)),l(4===d.rank,(()=>`Error in fused conv2d: filter must be rank 4, but got rank ${d.rank}.`)),null!=i&&l(f(s),(()=>`Error in fused conv2d: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`)),l(m.shape[3]===d.shape[2],(()=>`Error in conv2d: depth of input (${m.shape[3]}) must match input depth for filter ${d.shape[2]}.`)),l(Ao(n,a),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${a}'`)),l("NHWC"===r,(()=>`Error in conv2d: got dataFormat of ${r} but only NHWC is currently supported.`));const y=vo(m.shape,d.shape,n,a,s,i);let b,x;null!=o&&(b=Ur(o,"bias","fused conv2d"),[b]=Ir(b,p),gl(y.outShape,b.shape)),null!=c&&(x=Ur(c,"prelu weights","fused conv2d"));const w=(e,t)=>{const[r,i,o,c]=t,h=uh(e,o,u);l(Eo(a),(()=>`Error in gradient of fused conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${a}'`));const p=[tl(i.shape,h,r,n,s),lh(i,h,r.shape,n,s)];if(null!=c){const e=ch(c,h);p.push(e)}return p},k={x:m,filter:d,bias:b,preluActivationWeights:x},v={strides:n,pad:s,dataFormat:r,dilations:a,dimRoundingMode:i,activation:u,leakyreluAlpha:h};if(null==o){return ql(((e,t,n)=>{let s=Dr.runKernel(es,k,v);return n([t,e,s]),g&&(s=Fo(s,[s.shape[1],s.shape[2],s.shape[3]])),{value:s,gradFunc:w}}))(m,d)}return ql(((e,t,n,s)=>{let r=Dr.runKernel(es,k,v);return s([t,e,r,n]),g&&(r=Fo(r,[r.shape[1],r.shape[2],r.shape[3]])),{value:r,gradFunc:w}}))(m,d,b)}});const fh=jr({depthwiseConv2dNativeBackpropFilter_:function(e,t,n,s,r,a=[1,1],i){let o=e;3===e.rank&&(o=Fo(e,[1,e.shape[0],e.shape[1],e.shape[2]]));let l=t;3===l.rank&&(l=Fo(t,[1,t.shape[0],t.shape[1],t.shape[2]]));const u={x:o,dy:l},c={strides:s,pad:r,dimRoundingMode:i,dilations:a,filterShape:n};return Dr.runKernel(Be,u,c)}});const mh=jr({depthwiseConv2dNativeBackpropInput_:function(e,t,n,s,r,a=[1,1],i){let o=t,l=!1;3===t.rank&&(l=!0,o=Fo(t,[1,t.shape[0],t.shape[1],t.shape[2]]));const u={dy:o,filter:n},c={strides:s,pad:r,dimRoundingMode:i,dilations:a,inputShape:e},h=Dr.runKernel(Pe,u,c);return l?Fo(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const gh=jr({fusedDepthwiseConv2d_:function({x:e,filter:t,strides:n,pad:s,dataFormat:r="NHWC",dilations:a=[1,1],dimRoundingMode:i,bias:o,activation:u="linear",preluActivationWeights:c,leakyreluAlpha:h}){if(!1===ph(Dr.state.gradientDepth,u)){let l=hl(e,t,n,s,r,a,i);return null!=o&&(l=to(l,o)),hh(l,u,c,h)}const p=Ur(e,"x","depthwiseConv2d"),d=Ur(t,"filter","depthwiseConv2d");let m=p,g=!1;3===p.rank&&(g=!0,m=Fo(p,[1,p.shape[0],p.shape[1],p.shape[2]])),l(4===m.rank,(()=>`Error in fused depthwiseConv2d: input must be rank 4, but got rank ${m.rank}.`)),l(4===d.rank,(()=>`Error in fused depthwiseConv2d: filter must be rank 4, but got rank ${d.rank}.`)),l(m.shape[3]===d.shape[2],(()=>`Error in fused depthwiseConv2d: number of input channels (${m.shape[3]}) must match the inChannels dimension in filter ${d.shape[2]}.`)),null==a&&(a=[1,1]),l(Ao(n,a),(()=>`Error in fused depthwiseConv2d: Either strides or dilations must be 1. Got strides ${n} and dilations '${a}'`)),null!=i&&l(f(s),(()=>`Error in fused depthwiseConv2d: pad must be an integer when using dimRoundingMode ${i} but got pad ${s}.`));const y=vo(m.shape,d.shape,n,a,s,i,!0);let b,x;null!=o&&(b=Ur(o,"bias","fused conv2d"),[b]=Ir(b,p),gl(y.outShape,b.shape)),null!=c&&(x=Ur(c,"prelu weights","fused depthwiseConv2d"));const w=(e,t)=>{l(Eo(a),(()=>`Error in gradient of fused depthwiseConv2d: dilation rates greater than 1 are not yet supported. Got dilations '${a}'`));const[r,o,c,h]=t,p=uh(e,c,u),d=mh(o.shape,p,r,n,s,a,i),f=fh(o,p,r.shape,n,s,a,i);if(null!=h){return[d,f,ch(b,p)]}return[d,f]},k={x:m,filter:d,bias:b,preluActivationWeights:x},v={strides:n,pad:s,dataFormat:r,dilations:a,dimRoundingMode:i,activation:u,leakyreluAlpha:h};if(null==o){return ql(((e,t,n)=>{let s=Dr.runKernel(ts,k,v);return n([t,e,s]),g&&(s=Fo(s,[s.shape[1],s.shape[2],s.shape[3]])),{value:s,gradFunc:w}}))(m,d)}return ql(((e,t,n,s)=>{let r=Dr.runKernel(ts,k,v);return s([t,e,r,n]),g&&(r=Fo(r,[r.shape[1],r.shape[2],r.shape[3]])),{value:r,gradFunc:w}}))(m,d,b)}});const yh=jr({fusedMatMul_:function({a:e,b:t,transposeA:n=!1,transposeB:s=!1,bias:r,activation:a="linear",preluActivationWeights:i,leakyreluAlpha:o}){if(!1===ph(Dr.state.gradientDepth,a)){let l=ni(e,t,n,s);return null!=r&&(l=to(l,r)),hh(l,a,i,o)}let u=Ur(e,"a","fused matMul"),c=Ur(t,"b","fused matMul");[u,c]=Ir(u,c);const h=n?u.shape[u.rank-2]:u.shape[u.rank-1],f=s?c.shape[c.rank-1]:c.shape[c.rank-2],m=n?u.shape[u.rank-1]:u.shape[u.rank-2],g=s?c.shape[c.rank-2]:c.shape[c.rank-1],y=u.shape.slice(0,-2),b=c.shape.slice(0,-2),x=p(y),w=p(b);l(u.rank>=2&&c.rank>=2&&u.rank===c.rank,(()=>`Error in fused matMul: inputs must have the same rank of at least 2, got ranks ${u.rank} and ${c.rank}.`)),l(d(y,b),(()=>`Error in fused matMul: outer dimensions (${y}) and (${b}) of Tensors with shapes ${u.shape} and ${c.shape} must match.`)),l(h===f,(()=>`Error in fused matMul: inner shapes (${h}) and (${f}) of Tensors with shapes ${u.shape} and ${c.shape} and transposeA=${n} and transposeB=${s} must match.`));const k=u.shape.slice(0,-2).concat([m,g]),v=Fo(u,n?[x,h,m]:[x,m,h]),N=Fo(c,s?[w,g,f]:[w,f,g]);let I,S;null!=r&&(I=Ur(r,"bias","fused matMul"),[I]=Ir(I,u),gl(k,I.shape)),null!=i&&(S=Ur(i,"prelu weights","fused matMul"));const $=(e,t)=>{const[i,o,l,u]=t,c=uh(Fo(e,l.shape),l,a);let h,p;if(n||s?!n&&s?(h=ni(c,o,!1,!1),p=ni(c,i,!0,!1)):n&&!s?(h=ni(o,c,!1,!0),p=ni(i,c,!1,!1)):(h=ni(o,c,!0,!0),p=ni(c,i,!0,!0)):(h=ni(c,o,!1,!0),p=ni(i,c,!0,!1)),null!=r){return[h,p,ch(u,c)]}return[h,p]},C={a:v,b:N,bias:I,preluActivationWeights:S},T={transposeA:n,transposeB:s,activation:a,leakyreluAlpha:o};if(null==r){return ql(((e,t,n)=>{const s=Dr.runKernel(Qn,C,T);return n([e,t,s]),{value:Fo(s,k),gradFunc:$}}))(v,N)}return ql(((e,t,n,s)=>{const r=Dr.runKernel(Qn,C,T);return s([e,t,r,n]),{value:Fo(r,k),gradFunc:$}}))(v,N,I)}});var bh=Object.freeze({__proto__:null,conv2d:dh,depthwiseConv2d:gh,matMul:yh});const xh=jr({hammingWindow_:function(e){return ih(e,.54,.46)}});const wh=jr({hannWindow_:function(e){return ih(e,.5,.5)}});const kh=jr({frame_:function(e,t,n,s=!1,r=0){let a=0;const i=[];for(;a+t<=e.size;)i.push(Lo(e,a,t)),a+=n;if(s)for(;a<e.size;){const s=a+t-e.size,o=Oo([Lo(e,a,t-s),Al([s],r)]);i.push(o),a+=n}return 0===i.length?Vc([],[0,t]):Fo(Oo(i),[i.length,t])}});const vh=jr({stft_:function(e,t,n,s,r=wh){null==s&&(s=ah(t));const a=kh(e,t,n),i=ro(a,r(t));return _c(i,s)}});const Nh=jr({cropAndResize_:function(e,t,n,s,r="bilinear",a=0){const i=Ur(e,"image","cropAndResize"),o=Ur(t,"boxes","cropAndResize","float32"),u=Ur(n,"boxInd","cropAndResize","int32"),c=o.shape[0];l(4===i.rank,(()=>`Error in cropAndResize: image must be rank 4,but got rank ${i.rank}.`)),l(2===o.rank&&4===o.shape[1],(()=>`Error in cropAndResize: boxes must be have size [${c},4] but had shape ${o.shape}.`)),l(1===u.rank&&u.shape[0]===c,(()=>`Error in cropAndResize: boxInd must be have size [${c}] but had shape ${o.shape}.`)),l(2===s.length,(()=>`Error in cropAndResize: cropSize must be of length 2, but got length ${s.length}.`)),l(s[0]>=1&&s[1]>=1,(()=>`cropSize must be atleast [1,1], but was ${s}`)),l("bilinear"===r||"nearest"===r,(()=>`method must be bilinear or nearest, but was ${r}`));const h={image:i,boxes:o,boxInd:u},p={method:r,extrapolationValue:a,cropSize:s};return Dr.runKernel(Oe,h,p)}});const Ih=jr({flipLeftRight_:function(e){const t=Ur(e,"image","flipLeftRight","float32");l(4===t.rank,(()=>`Error in flipLeftRight: image must be rank 4,but got rank ${t.rank}.`));const n={image:t};return Dr.runKernel(nt,n,{})}});const Sh=jr({rotateWithOffset_:function(e,t,n=0,s=.5){const r=Ur(e,"image","rotateWithOffset","float32");l(4===r.rank,(()=>`Error in rotateWithOffset: image must be rank 4,but got rank ${r.rank}.`));const a={image:r},i={radians:t,fillValue:n,center:s};return Dr.runKernel(Jn,a,i)}});function $h(e,t,n,s,r,a){null==s&&(s=.5),null==r&&(r=Number.NEGATIVE_INFINITY),null==a&&(a=0);const i=e.shape[0];return n=Math.min(n,i),l(0<=s&&s<=1,(()=>`iouThreshold must be in [0, 1], but was '${s}'`)),l(2===e.rank,(()=>`boxes must be a 2D tensor, but was of rank '${e.rank}'`)),l(4===e.shape[1],(()=>`boxes must have 4 columns, but 2nd dimension was ${e.shape[1]}`)),l(1===t.rank,(()=>"scores must be a 1D tensor")),l(t.shape[0]===i,(()=>`scores has incompatible shape with boxes. Expected ${i}, but was ${t.shape[0]}`)),l(0<=a&&a<=1,(()=>`softNmsSigma must be in [0, 1], but was '${a}'`)),{maxOutputSize:n,iouThreshold:s,scoreThreshold:r,softNmsSigma:a}}const Ch=jr({nonMaxSuppression_:function(e,t,n,s=.5,r=Number.NEGATIVE_INFINITY){const a=Ur(e,"boxes","nonMaxSuppression"),i=Ur(t,"scores","nonMaxSuppression"),o=$h(a,i,n,s,r),l={maxOutputSize:n=o.maxOutputSize,iouThreshold:s=o.iouThreshold,scoreThreshold:r=o.scoreThreshold};return Dr.runKernel(Gt,{boxes:a,scores:i},l)}});function Th(e,t,n){const s=function(e,t,n){return function(e,t,n){let s=0,r=e.length,a=0,i=!1;for(;s<r;){a=s+(r-s>>>1);const o=n(t,e[a]);o>0?s=a+1:(r=a,i=!o)}return i?s:-s-1}(e,t,n||Eh)}(e,t,n),r=s<0?-(s+1):s;e.splice(r,0,t)}function Eh(e,t){return e>t?1:e<t?-1:0}function Ah(e,t,n,s,r){return _h(e,t,n,s,r,0)}function Rh(e,t,n,s,r,a){return _h(e,t,n,s,r,0,!1,a,!0)}function Fh(e,t,n,s,r,a){return _h(e,t,n,s,r,a,!0)}function _h(e,t,n,s,r,a,i=!1,o=!1,l=!1){const u=[];for(let e=0;e<t.length;e++)t[e]>r&&u.push({score:t[e],boxIndex:e,suppressBeginIndex:0});u.sort(Mh);const c=a>0?-.5/a:0,h=[],p=[];for(;h.length<n&&u.length>0;){const t=u.pop(),{score:n,boxIndex:a,suppressBeginIndex:i}=t;if(n<r)break;let o=!1;for(let n=h.length-1;n>=i;--n){const i=Dh(e,a,h[n]);if(i>=s){o=!0;break}if(t.score=t.score*Oh(s,c,i),t.score<=r)break}t.suppressBeginIndex=h.length,o||(t.score===n?(h.push(a),p.push(t.score)):t.score>r&&Th(u,t,Mh))}const d=h.length,f=n-d;o&&f>0&&(h.push(...new Array(f).fill(0)),p.push(...new Array(f).fill(0)));const m={selectedIndices:h};return i&&(m.selectedScores=p),l&&(m.validOutputs=d),m}function Dh(e,t,n){const s=e.subarray(4*t,4*t+4),r=e.subarray(4*n,4*n+4),a=Math.min(s[0],s[2]),i=Math.min(s[1],s[3]),o=Math.max(s[0],s[2]),l=Math.max(s[1],s[3]),u=Math.min(r[0],r[2]),c=Math.min(r[1],r[3]),h=Math.max(r[0],r[2]),p=Math.max(r[1],r[3]),d=(o-a)*(l-i),f=(h-u)*(p-c);if(d<=0||f<=0)return 0;const m=Math.max(a,u),g=Math.max(i,c),y=Math.min(o,h),b=Math.min(l,p),x=Math.max(y-m,0)*Math.max(b-g,0);return x/(d+f-x)}function Oh(e,t,n){const s=Math.exp(t*n*n);return n<=e?s:0}function Mh(e,t){return e.score-t.score||e.score===t.score&&t.boxIndex-e.boxIndex}const Lh=async function(e,t,n,s=.5,r=Number.NEGATIVE_INFINITY){const a=Ur(e,"boxes","nonMaxSuppressionAsync"),i=Ur(t,"scores","nonMaxSuppressionAsync"),o=$h(a,i,n,s,r);n=o.maxOutputSize,s=o.iouThreshold,r=o.scoreThreshold;const l=await Promise.all([a.data(),i.data()]),u=l[0],c=l[1],{selectedIndices:h}=Ah(u,c,n,s,r);return a!==e&&a.dispose(),i!==t&&i.dispose(),Wc(h,"int32")};const zh=jr({nonMaxSuppressionWithScore_:function(e,t,n,s=.5,r=Number.NEGATIVE_INFINITY,a=0){const i=Ur(e,"boxes","nonMaxSuppression"),o=Ur(t,"scores","nonMaxSuppression"),l=$h(i,o,n,s,r,a),u={boxes:i,scores:o},c={maxOutputSize:n=l.maxOutputSize,iouThreshold:s=l.iouThreshold,scoreThreshold:r=l.scoreThreshold,softNmsSigma:a=l.softNmsSigma},h=Dr.runKernel(jt,u,c);return{selectedIndices:h[0],selectedScores:h[1]}}});const Bh=async function(e,t,n,s=.5,r=Number.NEGATIVE_INFINITY,a=0){const i=Ur(e,"boxes","nonMaxSuppressionAsync"),o=Ur(t,"scores","nonMaxSuppressionAsync"),l=$h(i,o,n,s,r,a);n=l.maxOutputSize,s=l.iouThreshold,r=l.scoreThreshold,a=l.softNmsSigma;const u=await Promise.all([i.data(),o.data()]),c=u[0],h=u[1],{selectedIndices:p,selectedScores:d}=Fh(c,h,n,s,r,a);return i!==e&&i.dispose(),o!==t&&o.dispose(),{selectedIndices:Wc(p,"int32"),selectedScores:Wc(d)}};const Ph=jr({nonMaxSuppressionPadded_:function(e,t,n,s=.5,r=Number.NEGATIVE_INFINITY,a=!1){const i=Ur(e,"boxes","nonMaxSuppression"),o=Ur(t,"scores","nonMaxSuppression"),l=$h(i,o,n,s,r,null),u={boxes:i,scores:o},c={maxOutputSize:l.maxOutputSize,iouThreshold:l.iouThreshold,scoreThreshold:l.scoreThreshold,padToMaxOutputSize:a},h=Dr.runKernel(Ht,u,c);return{selectedIndices:h[0],validOutputs:h[1]}}});const Wh=async function(e,t,n,s=.5,r=Number.NEGATIVE_INFINITY,a=!1){const i=Ur(e,"boxes","nonMaxSuppressionAsync"),o=Ur(t,"scores","nonMaxSuppressionAsync"),l=$h(i,o,n,s,r,null),u=l.maxOutputSize,c=l.iouThreshold,h=l.scoreThreshold,[p,d]=await Promise.all([i.data(),o.data()]),{selectedIndices:f,validOutputs:m}=Rh(p,d,u,c,h,a);return i!==e&&i.dispose(),o!==t&&o.dispose(),{selectedIndices:Wc(f,"int32"),validOutputs:yc(m,"int32")}};const Vh=jr({resizeBilinear_:function(e,t,n=!1,s=!1){const r=Ur(e,"images","resizeBilinear");l(3===r.rank||4===r.rank,(()=>`Error in resizeBilinear: x must be rank 3 or 4, but got rank ${r.rank}.`)),l(2===t.length,(()=>`Error in resizeBilinear: new shape must 2D, but got shape ${t}.`)),l(!1===s||!1===n,(()=>"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false."));let a=r,i=!1;3===r.rank&&(i=!0,a=Fo(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const[]=t,o={images:a},u={alignCorners:n,halfPixelCenters:s,size:t},c=Dr.runKernel(ln,o,u);return i?Fo(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Uh=jr({resizeNearestNeighbor_:function(e,t,n=!1,s=!1){const r=Ur(e,"images","resizeNearestNeighbor");l(3===r.rank||4===r.rank,(()=>`Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${r.rank}.`)),l(2===t.length,(()=>`Error in resizeNearestNeighbor: new shape must 2D, but got shape ${t}.`)),l("float32"===r.dtype||"int32"===r.dtype,(()=>"`images` must have `int32` or `float32` as dtype")),l(!1===s||!1===n,(()=>"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false."));let a=r,i=!1;3===r.rank&&(i=!0,a=Fo(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const[]=t,o={images:a},u={alignCorners:n,halfPixelCenters:s,size:t},c=Dr.runKernel(an,o,u);return i?Fo(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Gh=jr({threshold_:function(e,t="binary",n=!1,s=.5){const r=Ur(e,"image","threshold"),a=r.shape[0]*r.shape[1];let i,o,u,c,h=ro(Wc([s]),255);if(l(3===r.rank,(()=>`Error in threshold: image must be rank 3,but got rank ${r.rank}.`)),l(3===r.shape[2]||1===r.shape[2],(()=>`Error in threshold: image color channel must be equal to 3 or 1but got ${r.shape[2]}.`)),l("int32"===r.dtype||"float32"===r.dtype,(()=>`Error in dtype: image dtype must be int32 or float32,but got dtype ${r.dtype}.`)),l("otsu"===t||"binary"===t,(()=>`Method must be binary or otsu, but was ${t}`)),3===r.shape[2]){[i,o,u]=Fc(r,[1,1,1],-1);const e=ro(i,.2989),t=ro(o,.587),n=ro(u,.114);c=to(to(e,t),n)}else c=e;if("otsu"===t){h=function(e,t){let n,s,r,a,i,o,l=Wc([-1]),u=Wc([0]),c=Wc([0]);for(let h=0;h<e.size-1;h++){n=Lo(e,0,h+1),s=Lo(e,h+1),i=so(eu(n),t),o=so(eu(s),t);const p=eu(ro(n,ac(0,n.size)));r=so(p,eu(n));const d=Al(s.shape,n.size),f=to(ac(0,s.size),d),m=ro(s,f);a=so(eu(m),eu(s));const g=Ql(r,a),y=Ql(r,a),b=ro(i,o);c=ro(ro(b,g),y);const x=_l(c,u);u=bl(x,c,u),l=bl(x,Wc([h]),l)}return l}(Ho(za(mc(c),"int32"),Xr([]),256),a)}const p=n?Wl(c,h):_l(c,h);return za(ro(p,255),"int32")}});const Hh=jr({transform_:function(e,t,n="nearest",s="constant",r=0,a){const i=Ur(e,"image","transform","float32"),o=Ur(t,"transforms","transform","float32");l(4===i.rank,(()=>`Error in transform: image must be rank 4,but got rank ${i.rank}.`)),l(2===o.rank&&(o.shape[0]===i.shape[0]||1===o.shape[0])&&8===o.shape[1],(()=>"Error in transform: Input transform should be batch x 8 or 1 x 8")),l(null==a||2===a.length,(()=>`Error in transform: outputShape must be [height, width] or null, but got ${a}.`));const u={image:i,transforms:o},c={interpolation:n,fillMode:s,fillValue:r,outputShape:a};return Dr.runKernel(Gn,u,c)}});const jh=jr({bandPart_:function(e,t,n){l(t%1==0,(()=>`bandPart(): numLower must be an integer, got ${t}.`)),l(n%1==0,(()=>`bandPart(): numUpper must be an integer, got ${n}.`));const s=Ur(e,"a","bandPart");l(s.rank>=2,(()=>`bandPart(): Rank must be at least 2, got ${s.rank}.`));const r=s.shape,[a,i]=s.shape.slice(-2);if(!(t<=a))throw new Error(`bandPart(): numLower (${t}) must not be greater than the number of rows (${a}).`);if(!(n<=i))throw new Error(`bandPart(): numUpper (${n}) must not be greater than the number of columns (${i}).`);t<0&&(t=a),n<0&&(n=i);const o=Fo(ac(0,a,1,"int32"),[-1,1]),u=ac(0,i,1,"int32"),c=Ql(o,u),h=hu(Wl(c,yc(+t,"int32")),Dl(c,yc(-n,"int32"))),p=wu([a,i],s.dtype);return Fo(Lc(qc(Fo(s,[-1,a,i])).map((e=>bl(h,e,p)))),r)}});const qh=jr({gramSchmidt_:function(e){let t;if(Array.isArray(e)){t=!1,l(null!=e&&e.length>0,(()=>"Gram-Schmidt process: input must not be null, undefined, or empty"));const n=e[0].shape[0];for(let t=1;t<e.length;++t)l(e[t].shape[0]===n,(()=>`Gram-Schmidt: Non-unique lengths found in the input vectors: (${e[t].shape[0]} vs. ${n})`))}else t=!0,e=Fc(e,e.shape[0],0).map((e=>Mc(e,[0])));l(e.length<=e[0].shape[0],(()=>`Gram-Schmidt: Number of vectors (${e.length}) exceeds number of dimensions (${e[0].shape[0]}).`));const n=[],s=e;for(let t=0;t<e.length;++t)n.push(Dr.tidy((()=>{let e=s[t];if(t>0)for(let s=0;s<t;++s){const t=ro(eu(ro(n[s],e)),n[s]);e=Ql(e,t)}return so(e,Qc(e,"euclidean"))})));return t?Lc(n,0):n}});function Kh(e,t=!1){return Dr.tidy((()=>{l(2===e.shape.length,(()=>`qr2d() requires a 2D Tensor, but got a ${e.shape.length}D Tensor.`));const n=e.shape[0],s=e.shape[1];let r=El(n),a=Ba(e);const i=Vc([[1]],[1,1]);let o=Ba(i);const u=n>=s?s:n;for(let e=0;e<u;++e){const t=a,l=o,u=r;[o,a,r]=Dr.tidy((()=>{const t=Lo(a,[e,e],[n-e,1]),l=Qc(t),u=Lo(a,[e,e],[1,1]),c=bl(_l(u,0),Vc([[-1]]),Vc([[1]])),h=Ql(u,ro(c,l)),p=so(t,h);o=1===p.shape[0]?Ba(i):Oo([i,Lo(p,[1,0],[p.shape[0]-1,p.shape[1]])],0);const d=Xl(so(ni(c,h),l)),f=Lo(a,[e,0],[n-e,s]),m=ro(d,o),g=ri(o);if(0===e)a=Ql(f,ni(m,ni(g,f)));else{const t=Ql(f,ni(m,ni(g,f)));a=Oo([Lo(a,[0,0],[e,s]),t],0)}const y=ri(m),b=Lo(r,[0,e],[n,r.shape[1]-e]);if(0===e)r=Ql(b,ni(ni(b,o),y));else{const t=Ql(b,ni(ni(b,o),y));r=Oo([Lo(r,[0,0],[n,e]),t],1)}return[o,a,r]})),Yi([t,l,u])}return!t&&n>s&&(r=Lo(r,[0,0],[n,s]),a=Lo(a,[0,0],[s,s])),[r,a]}))}const Xh=jr({qr_:function(e,t=!1){if(l(e.rank>=2,(()=>`qr() requires input tensor to have a rank >= 2, but got rank ${e.rank}`)),2===e.rank)return Kh(e,t);{const n=e.shape.slice(0,e.shape.length-2).reduce(((e,t)=>e*t)),s=qc(Fo(e,[n,e.shape[e.shape.length-2],e.shape[e.shape.length-1]]),0),r=[],a=[];s.forEach((e=>{const[n,s]=Kh(e,t);r.push(n),a.push(s)}));return[Fo(Lc(r,0),e.shape),Fo(Lc(a,0),e.shape)]}}});var Yh;(Yh=e.Reduction||(e.Reduction={}))[Yh.NONE=0]="NONE",Yh[Yh.MEAN=1]="MEAN",Yh[Yh.SUM=2]="SUM",Yh[Yh.SUM_BY_NONZERO_WEIGHTS=3]="SUM_BY_NONZERO_WEIGHTS";const Zh=jr({computeWeightedLoss_:function(t,n,s=e.Reduction.SUM_BY_NONZERO_WEIGHTS){const r=Ur(t,"losses","computeWeightedLoss");let a=null;null!=n&&(a=Ur(n,"weights","computeWeightedLoss"));const i=null==a?r:ro(r,a);if(s===e.Reduction.NONE)return i;if(s===e.Reduction.SUM)return eu(i);if(s===e.Reduction.MEAN){if(null==a)return xu(i);{const e=r.size/a.size,t=so(eu(i),eu(a));return e>1?so(t,yc(e)):t}}if(s===e.Reduction.SUM_BY_NONZERO_WEIGHTS){if(null==a)return so(eu(i),yc(r.size));{const e=ro(a,ku(r.shape)),t=za(eu(Au(e,yc(0))),"float32");return so(eu(i),t)}}throw Error(`Unknown reduction: ${s}`)}});const Jh=jr({absoluteDifference_:function(t,n,s,r=e.Reduction.SUM_BY_NONZERO_WEIGHTS){const a=Ur(t,"labels","absoluteDifference"),i=Ur(n,"predictions","absoluteDifference");let o=null;null!=s&&(o=Ur(s,"weights","absoluteDifference")),u(a.shape,i.shape,"Error in absoluteDifference: ");const l=ao(Ql(a,i));return Zh(l,o,r)}});const Qh=jr({cosineDistance_:function(t,n,s,r,a=e.Reduction.SUM_BY_NONZERO_WEIGHTS){const i=Ur(t,"labels","cosineDistance"),o=Ur(n,"predictions","cosineDistance");let l=null;null!=r&&(l=Ur(r,"weights","cosineDistance")),u(i.shape,o.shape,"Error in cosineDistance: ");const c=yc(1),h=Ql(c,eu(ro(i,o),s,!0));return Zh(h,l,a)}});const ep=jr({hingeLoss_:function(t,n,s,r=e.Reduction.SUM_BY_NONZERO_WEIGHTS){let a=Ur(t,"labels","hingeLoss");const i=Ur(n,"predictions","hingeLoss");let o=null;null!=s&&(o=Ur(s,"weights","hingeLoss")),u(a.shape,i.shape,"Error in hingeLoss: ");const l=yc(1);a=Ql(ro(yc(2),a),l);const c=lc(Ql(l,ro(a,i)));return Zh(c,o,r)}});const tp=jr({huberLoss_:function(t,n,s,r=1,a=e.Reduction.SUM_BY_NONZERO_WEIGHTS){const i=Ur(t,"labels","huberLoss"),o=Ur(n,"predictions","huberLoss");let l=null;null!=s&&(l=Ur(s,"weights","huberLoss")),u(i.shape,o.shape,"Error in huberLoss: ");const c=yc(r),h=ao(Ql(o,i)),p=Nu(h,c),d=Ql(h,p),f=to(ro(yc(.5),$u(p)),ro(c,d));return Zh(f,l,a)}});const np=jr({logLoss_:function(t,n,s,r=1e-7,a=e.Reduction.SUM_BY_NONZERO_WEIGHTS){const i=Ur(t,"labels","logLoss"),o=Ur(n,"predictions","logLoss");let l=null;null!=s&&(l=Ur(s,"weights","logLoss")),u(i.shape,o.shape,"Error in logLoss: ");const c=yc(1),h=yc(r),p=Xl(ro(i,Gl(to(o,h)))),d=ro(Ql(c,i),Gl(to(Ql(c,o),h))),f=Ql(p,d);return Zh(f,l,a)}});const sp=jr({meanSquaredError_:function(t,n,s,r=e.Reduction.SUM_BY_NONZERO_WEIGHTS){const a=Ur(t,"labels","meanSquaredError"),i=Ur(n,"predictions","meanSquaredError");let o=null;null!=s&&(o=Ur(s,"weights","meanSquaredError")),u(a.shape,i.shape,"Error in meanSquaredError: ");const l=Oc(a,i);return Zh(l,o,r)}});const rp=jr({sigmoidCrossEntropy_:function(t,n,s,r=0,a=e.Reduction.SUM_BY_NONZERO_WEIGHTS){let i=Ur(t,"multiClassLabels","sigmoidCrossEntropy");const o=Ur(n,"logits","sigmoidCrossEntropy");let l=null;if(null!=s&&(l=Ur(s,"weights","sigmoidCrossEntropy")),u(i.shape,o.shape,"Error in sigmoidCrossEntropy: "),r>0){const e=yc(r),t=yc(1),n=yc(.5);i=to(ro(i,Ql(t,e)),ro(n,e))}const c=function(e,t){const n=Ur(e,"labels","sigmoidCrossEntropyWithLogits"),s=Ur(t,"logits","sigmoidCrossEntropyWithLogits");u(n.shape,s.shape,"Error in sigmoidCrossEntropyWithLogits: ");const r=lc(s),a=ro(s,n),i=Hl(Sl(Xl(ao(s))));return to(Ql(r,a),i)}(i,o);return Zh(c,l,a)}});const ap=jr({softmaxCrossEntropy_:function(t,n,s,r=0,a=e.Reduction.SUM_BY_NONZERO_WEIGHTS){let i=Ur(t,"onehotLabels","softmaxCrossEntropy");const o=Ur(n,"logits","softmaxCrossEntropy");let l=null;if(null!=s&&(l=Ur(s,"weights","softmaxCrossEntropy")),u(i.shape,o.shape,"Error in softmaxCrossEntropy: "),r>0){const e=yc(r),t=yc(1),n=yc(i.shape[1]);i=to(ro(i,Ql(t,e)),so(e,n))}const c=function(e,t,n=-1){if(-1===n&&(n=t.rank-1),n!==t.rank-1)throw Error(`Softmax cross entropy along a non-last dimension is not yet supported. Labels / logits was rank ${t.rank} and dim was ${n}`);return ql(((e,t,s)=>{const r=cu(t,[n],!0),a=Ql(za(t,"float32"),r);s([e,a]);const i=Xl(ro(a,e));return{value:eu(i,[n]),gradFunc:(e,t)=>{const[s,r]=t,a=au(e.shape,[n]);return[ro(Fo(e,a),Ql(za(s,"float32"),Sl(r))),ro(Fo(e,a),Ql(Sl(r),za(s,"float32")))]}}}))(e,t)}(i,o);return Zh(c,l,a)}});const ip=jr({sparseFillEmptyRows_:function(e,t,n,s){const r=Ur(e,"indices","sparseFillEmptyRows"),a=Ur(t,"values","sparseFillEmptyRows"),i=Ur(n,"denseShape","sparseFillEmptyRows"),o=Ur(s,"defaultValue","sparseFillEmptyRows",a.dtype);if(2!==r.rank)throw new Error(`Indices should be Tensor2D but received shape\n        ${r.shape}`);if(1!==a.rank)throw new Error(`Values should be Tensor1D but received shape ${a.shape}`);if(1!==i.rank)throw new Error(`Dense shape should be Tensor1D but received shape ${i.shape}`);if(0!==o.rank)throw new Error(`Default value should be a scalar but received shape ${o.shape}`);const l={indices:r,values:a,denseShape:i,defaultValue:o},u=Dr.runKernel(Tn,l);return{outputIndices:u[0],outputValues:u[1],emptyRowIndicator:u[2],reverseIndexMap:u[3]}}});const op=jr({sparseReshape_:function(e,t,n){const s=Ur(e,"inputIndices","sparseReshape"),r=Ur(t,"inputShape","sparseReshape"),a=Ur(n,"newShape","sparseReshape");if(2!==s.rank)throw new Error(`Input indices should be Tensor2D but received shape\n        ${s.shape}`);if(1!==r.rank)throw new Error(`Input shape should be Tensor1D but received shape ${r.shape}`);if(1!==a.rank)throw new Error(`New shape should be Tensor1D but received shape ${a.shape}`);const i={inputIndices:s,inputShape:r,newShape:a},o=Dr.runKernel(En,i);return{outputIndices:o[0],outputShape:o[1]}}});const lp=jr({sparseSegmentMean_:function(e,t,n){const s=Ur(e,"data","sparseSegmentMean"),r=Ur(t,"indices","sparseSegmentMean"),a=Ur(n,"segmentIds","sparseSegmentMean");if(s.rank<1)throw new Error("Data should be at least 1 dimensional but received scalar");if(1!==r.rank)throw new Error(`Indices should be Tensor1D but received shape\n          ${r.shape}`);if(1!==a.rank)throw new Error(`Segment ids should be Tensor1D but received shape\n          ${a.shape}`);const i={data:s,indices:r,segmentIds:a};return Dr.runKernel(An,i)}});const up=jr({sparseSegmentSum_:function(e,t,n){const s=Ur(e,"data","sparseSegmentSum"),r=Ur(t,"indices","sparseSegmentSum"),a=Ur(n,"segmentIds","sparseSegmentSum");if(s.rank<1)throw new Error("Data should be at least 1 dimensional but received scalar");if(1!==r.rank)throw new Error(`Indices should be Tensor1D but received shape\n         ${r.shape}`);if(1!==a.rank)throw new Error(`Segment ids should be Tensor1D but received shape\n         ${a.shape}`);const i={data:s,indices:r,segmentIds:a};return Dr.runKernel(Rn,i)}});const cp=jr({stringNGrams_:function(e,t,n,s,r,a,i,o){const l=Ur(e,"data","stringNGrams","string");if("string"!==l.dtype)throw new Error("Data must be of datatype string");if(1!==l.shape.length)throw new Error(`Data must be a vector, saw: ${l.shape}`);const u=Ur(t,"dataSplits","stringNGrams");if("int32"!==u.dtype)throw new Error("Data splits must be of datatype int32");const c={separator:n,nGramWidths:s,leftPad:r,rightPad:a,padWidth:i,preserveShortSequences:o},h={data:l,dataSplits:u},p=Dr.runKernel(Mn,h,c);return{nGrams:p[0],nGramsSplits:p[1]}}});const hp=jr({stringSplit_:function(e,t,n=!0){const s=Ur(e,"input","stringSplit","string"),r=Ur(t,"delimiter","stringSplit","string");if(1!==s.rank)throw new Error(`Input should be Tensor1D but received shape ${s.shape}`);if(0!==r.rank)throw new Error(`Delimiter should be a scalar but received shape ${r.shape}`);const a={skipEmpty:n},i={input:s,delimiter:r},o=Dr.runKernel(Ln,i,a);return{indices:o[0],values:o[1],shape:o[2]}}});const pp=jr({stringToHashBucketFast_:function(e,t){const n=Ur(e,"input","stringToHashBucketFast","string"),s={numBuckets:t};if(t<=0)throw new Error("Number of buckets must be at least 1");const r={input:n};return Dr.runKernel(zn,r,s)}}),dp={fft:Ec,ifft:Ac,rfft:_c,irfft:Rc},fp={hammingWindow:xh,hannWindow:wh,frame:kh,stft:vh},mp={flipLeftRight:Ih,resizeNearestNeighbor:Uh,resizeBilinear:Vh,rotateWithOffset:Sh,cropAndResize:Nh,nonMaxSuppression:Ch,nonMaxSuppressionAsync:Lh,nonMaxSuppressionWithScore:zh,nonMaxSuppressionWithScoreAsync:Bh,nonMaxSuppressionPadded:Ph,nonMaxSuppressionPaddedAsync:Wh,threshold:Gh,transform:Hh},gp={bandPart:jh,gramSchmidt:qh,qr:Xh},yp={absoluteDifference:Jh,computeWeightedLoss:Zh,cosineDistance:Qh,hingeLoss:ep,huberLoss:tp,logLoss:np,meanSquaredError:sp,sigmoidCrossEntropy:rp,softmaxCrossEntropy:ap},bp={sparseFillEmptyRows:ip,sparseReshape:op,sparseSegmentMean:lp,sparseSegmentSum:up},xp={stringNGrams:cp,stringSplit:hp,stringToHashBucketFast:pp};class wp extends Mi{minimize(e,t=!1,n){const{value:s,grads:r}=this.computeGradients(e,n);if(null!=n){const e=n.map((e=>({name:e.name,tensor:r[e.name]})));this.applyGradients(e)}else this.applyGradients(r);return Yi(r),t?s:(s.dispose(),null)}get iterations(){return null==this.iterations_&&(this.iterations_=0),this.iterations_}incrementIterations(){this.iterations_=this.iterations+1}computeGradients(e,t){return jl(e,t)}dispose(){null!=this.iterations_&&Yi(this.iterations_)}async saveIterations(){return null==this.iterations_&&(this.iterations_=0),{name:"iter",tensor:yc(this.iterations_,"int32")}}async getWeights(){throw new Error("getWeights() is not implemented for this optimizer yet.")}async setWeights(e){throw new Error(`setWeights() is not implemented for this optimizer class ${this.getClassName()}`)}async extractIterations(e){return this.iterations_=(await e[0].tensor.data())[0],e.slice(1)}}Object.defineProperty(wp,Symbol.hasInstance,{value:e=>null!=e.minimize&&null!=e.computeGradients&&null!=e.applyGradients});class kp extends wp{constructor(e,t,n=null){super(),this.learningRate=e,this.rho=t,this.epsilon=n,this.accumulatedGrads=[],this.accumulatedUpdates=[],null==n&&(this.epsilon=Dr.backend.epsilon())}applyGradients(e){(Array.isArray(e)?e.map((e=>e.name)):Object.keys(e)).forEach(((t,n)=>{const s=Dr.registeredVariables[t];null==this.accumulatedGrads[n]&&(this.accumulatedGrads[n]={originalName:`${t}/accum_grad`,variable:Xi((()=>xl(s).variable(false)))}),null==this.accumulatedUpdates[n]&&(this.accumulatedUpdates[n]={originalName:`${t}/accum_var`,variable:Xi((()=>xl(s).variable(false)))});const r=Array.isArray(e)?e[n].tensor:e[t];if(null==r)return;const a=this.accumulatedGrads[n].variable,i=this.accumulatedUpdates[n].variable;Xi((()=>{const e=to(ro(a,this.rho),ro($u(r),1-this.rho)),t=ro(so(Dc(to(i,this.epsilon)),Dc(to(a,this.epsilon))),r),n=to(ro(i,this.rho),ro($u(t),1-this.rho));a.assign(e),i.assign(n);const o=to(ro(t,-this.learningRate),s);s.assign(o)}))})),this.incrementIterations()}dispose(){null!=this.accumulatedUpdates&&(Yi(this.accumulatedGrads.map((e=>e.variable))),Yi(this.accumulatedUpdates.map((e=>e.variable))))}async getWeights(){const e=[...this.accumulatedGrads,...this.accumulatedUpdates];return[await this.saveIterations()].concat(e.map((e=>({name:e.originalName,tensor:e.variable}))))}async setWeights(e){const t=(e=await this.extractIterations(e)).length/2;this.accumulatedGrads=e.slice(0,t).map((e=>({originalName:e.name,variable:e.tensor.variable(false)}))),this.accumulatedUpdates=e.slice(t,2*t).map((e=>({originalName:e.name,variable:e.tensor.variable(false)})))}getConfig(){return{learningRate:this.learningRate,rho:this.rho,epsilon:this.epsilon}}static fromConfig(e,t){return new e(t.learningRate,t.rho,t.epsilon)}}kp.className="Adadelta",zi(kp);class vp extends wp{constructor(e,t=.1){super(),this.learningRate=e,this.initialAccumulatorValue=t,this.accumulatedGrads=[]}applyGradients(e){(Array.isArray(e)?e.map((e=>e.name)):Object.keys(e)).forEach(((t,n)=>{const s=Dr.registeredVariables[t];if(null==this.accumulatedGrads[n]){const e=!1;this.accumulatedGrads[n]={originalName:`${t}/accumulator`,variable:Xi((()=>Al(s.shape,this.initialAccumulatorValue).variable(e)))}}const r=Array.isArray(e)?e[n].tensor:e[t];if(null==r)return;const a=this.accumulatedGrads[n].variable;Xi((()=>{const e=to(a,$u(r));a.assign(e);const t=to(ro(so(r,Dc(to(e,Dr.backend.epsilon()))),-this.learningRate),s);s.assign(t)}))})),this.incrementIterations()}dispose(){null!=this.accumulatedGrads&&Yi(this.accumulatedGrads.map((e=>e.variable)))}async getWeights(){return[await this.saveIterations()].concat(this.accumulatedGrads.map((e=>({name:e.originalName,tensor:e.variable}))))}async setWeights(e){e=await this.extractIterations(e);this.accumulatedGrads=e.map((e=>({originalName:e.name,variable:e.tensor.variable(false)})))}getConfig(){return{learningRate:this.learningRate,initialAccumulatorValue:this.initialAccumulatorValue}}static fromConfig(e,t){return new e(t.learningRate,t.initialAccumulatorValue)}}vp.className="Adagrad",zi(vp);class Np extends wp{constructor(e,t,n,s=null){super(),this.learningRate=e,this.beta1=t,this.beta2=n,this.epsilon=s,this.accumulatedFirstMoment=[],this.accumulatedSecondMoment=[],Xi((()=>{this.accBeta1=yc(t).variable(),this.accBeta2=yc(n).variable()})),null==s&&(this.epsilon=Dr.backend.epsilon())}applyGradients(e){const t=Array.isArray(e)?e.map((e=>e.name)):Object.keys(e);Xi((()=>{const n=Ql(1,this.accBeta1),s=Ql(1,this.accBeta2);t.forEach(((t,r)=>{const a=Dr.registeredVariables[t];null==this.accumulatedFirstMoment[r]&&(this.accumulatedFirstMoment[r]={originalName:`${t}/m`,variable:Xi((()=>xl(a).variable(false)))}),null==this.accumulatedSecondMoment[r]&&(this.accumulatedSecondMoment[r]={originalName:`${t}/v`,variable:Xi((()=>xl(a).variable(false)))});const i=Array.isArray(e)?e[r].tensor:e[t];if(null==i)return;const o=this.accumulatedFirstMoment[r].variable,l=this.accumulatedSecondMoment[r].variable,u=to(ro(o,this.beta1),ro(i,1-this.beta1)),c=to(ro(l,this.beta2),ro($u(i),1-this.beta2)),h=so(u,n),p=so(c,s);o.assign(u),l.assign(c);const d=to(ro(so(h,to(Dc(p),this.epsilon)),-this.learningRate),a);a.assign(d)})),this.accBeta1.assign(ro(this.accBeta1,this.beta1)),this.accBeta2.assign(ro(this.accBeta2,this.beta2))})),this.incrementIterations()}dispose(){this.accBeta1.dispose(),this.accBeta2.dispose(),null!=this.accumulatedFirstMoment&&Yi(this.accumulatedFirstMoment.map((e=>e.variable))),null!=this.accumulatedSecondMoment&&Yi(this.accumulatedSecondMoment.map((e=>e.variable)))}async getWeights(){const e=[...this.accumulatedFirstMoment,...this.accumulatedSecondMoment];return[await this.saveIterations()].concat(e.map((e=>({name:e.originalName,tensor:e.variable}))))}async setWeights(e){e=await this.extractIterations(e),Xi((()=>{this.accBeta1.assign(Pu(this.beta1,this.iterations_+1)),this.accBeta2.assign(Pu(this.beta2,this.iterations_+1))}));const t=e.length/2;this.accumulatedFirstMoment=e.slice(0,t).map((e=>({originalName:e.name,variable:e.tensor.variable(false)}))),this.accumulatedSecondMoment=e.slice(t,2*t).map((e=>({originalName:e.name,variable:e.tensor.variable(false)})))}getConfig(){return{learningRate:this.learningRate,beta1:this.beta1,beta2:this.beta2,epsilon:this.epsilon}}static fromConfig(e,t){return new e(t.learningRate,t.beta1,t.beta2,t.epsilon)}}Np.className="Adam",zi(Np);class Ip extends wp{constructor(e,t,n,s=null,r=0){super(),this.learningRate=e,this.beta1=t,this.beta2=n,this.epsilon=s,this.decay=r,this.accumulatedFirstMoment=[],this.accumulatedWeightedInfNorm=[],Xi((()=>{this.iteration=yc(0).variable(),this.accBeta1=yc(t).variable()})),null==s&&(this.epsilon=Dr.backend.epsilon())}applyGradients(e){const t=Array.isArray(e)?e.map((e=>e.name)):Object.keys(e);Xi((()=>{const n=Ql(1,this.accBeta1),s=so(-this.learningRate,to(ro(this.iteration,this.decay),1));t.forEach(((t,r)=>{const a=Dr.registeredVariables[t];null==this.accumulatedFirstMoment[r]&&(this.accumulatedFirstMoment[r]={originalName:`${t}/m`,variable:xl(a).variable(false)}),null==this.accumulatedWeightedInfNorm[r]&&(this.accumulatedWeightedInfNorm[r]={originalName:`${t}/v`,variable:xl(a).variable(false)});const i=Array.isArray(e)?e[r].tensor:e[t];if(null==i)return;const o=this.accumulatedFirstMoment[r].variable,l=this.accumulatedWeightedInfNorm[r].variable,u=to(ro(o,this.beta1),ro(i,1-this.beta1)),c=ro(l,this.beta2),h=ao(i),p=bu(c,h);o.assign(u),l.assign(p);const d=to(ro(so(s,n),so(u,to(p,this.epsilon))),a);a.assign(d)})),this.iteration.assign(to(this.iteration,1)),this.accBeta1.assign(ro(this.accBeta1,this.beta1))})),this.incrementIterations()}dispose(){this.accBeta1.dispose(),this.iteration.dispose(),null!=this.accumulatedFirstMoment&&Yi(this.accumulatedFirstMoment.map((e=>e.variable))),null!=this.accumulatedWeightedInfNorm&&Yi(this.accumulatedWeightedInfNorm.map((e=>e.variable)))}async getWeights(){throw new Error("getWeights() is not implemented for Adamax yet.")}async setWeights(e){throw new Error("setWeights() is not implemented for Adamax yet.")}getConfig(){return{learningRate:this.learningRate,beta1:this.beta1,beta2:this.beta2,epsilon:this.epsilon,decay:this.decay}}static fromConfig(e,t){return new e(t.learningRate,t.beta1,t.beta2,t.epsilon,t.decay)}}Ip.className="Adamax",zi(Ip);class Sp extends wp{constructor(e){super(),this.learningRate=e,this.setLearningRate(e)}applyGradients(e){(Array.isArray(e)?e.map((e=>e.name)):Object.keys(e)).forEach(((t,n)=>{const s=Array.isArray(e)?e[n].tensor:e[t];if(null==s)return;const r=Dr.registeredVariables[t];Xi((()=>{const e=to(ro(this.c,s),r);r.assign(e)}))})),this.incrementIterations()}setLearningRate(e){this.learningRate=e,null!=this.c&&this.c.dispose(),this.c=Zi(yc(-e))}dispose(){this.c.dispose()}async getWeights(){return[await this.saveIterations()]}async setWeights(e){if(0!==(e=await this.extractIterations(e)).length)throw new Error("SGD optimizer does not have settable weights.")}getConfig(){return{learningRate:this.learningRate}}static fromConfig(e,t){return new e(t.learningRate)}}Sp.className="SGD",zi(Sp);class $p extends Sp{constructor(e,t,n=!1){super(e),this.learningRate=e,this.momentum=t,this.useNesterov=n,this.accumulations=[],this.m=yc(this.momentum)}applyGradients(e){(Array.isArray(e)?e.map((e=>e.name)):Object.keys(e)).forEach(((t,n)=>{const s=Dr.registeredVariables[t];if(null==this.accumulations[n]){const e=!1;this.accumulations[n]={originalName:`${t}/momentum`,variable:Xi((()=>xl(s).variable(e)))}}const r=this.accumulations[n].variable,a=Array.isArray(e)?e[n].tensor:e[t];null!=a&&Xi((()=>{let e;const t=to(ro(this.m,r),a);e=this.useNesterov?to(ro(this.c,to(a,ro(t,this.m))),s):to(ro(this.c,t),s),r.assign(t),s.assign(e)}))})),this.incrementIterations()}dispose(){this.m.dispose(),null!=this.accumulations&&Yi(this.accumulations.map((e=>e.variable)))}setMomentum(e){this.momentum=e}async getWeights(){return[await this.saveIterations()].concat(this.accumulations.map((e=>({name:e.originalName,tensor:e.variable}))))}async setWeights(e){e=await this.extractIterations(e);this.accumulations=e.map((e=>({originalName:e.name,variable:e.tensor.variable(false)})))}getConfig(){return{learningRate:this.learningRate,momentum:this.momentum,useNesterov:this.useNesterov}}static fromConfig(e,t){return new e(t.learningRate,t.momentum,t.useNesterov)}}$p.className="Momentum",zi($p);class Cp extends wp{constructor(e,t=.9,n=0,s=null,r=!1){if(super(),this.learningRate=e,this.decay=t,this.momentum=n,this.epsilon=s,this.accumulatedMeanSquares=[],this.accumulatedMoments=[],this.accumulatedMeanGrads=[],this.centered=r,null==s&&(this.epsilon=Dr.backend.epsilon()),null==e)throw new Error("learningRate for RMSPropOptimizer must be defined.")}applyGradients(e){(Array.isArray(e)?e.map((e=>e.name)):Object.keys(e)).forEach(((t,n)=>{const s=Dr.registeredVariables[t],r=!1;null==this.accumulatedMeanSquares[n]&&(this.accumulatedMeanSquares[n]={originalName:`${t}/rms`,variable:Xi((()=>xl(s).variable(r)))}),null==this.accumulatedMoments[n]&&(this.accumulatedMoments[n]={originalName:`${t}/momentum`,variable:Xi((()=>xl(s).variable(r)))}),null==this.accumulatedMeanGrads[n]&&this.centered&&(this.accumulatedMeanGrads[n]={originalName:`${t}/mg`,variable:Xi((()=>xl(s).variable(r)))});const a=Array.isArray(e)?e[n].tensor:e[t];if(null==a)return;const i=this.accumulatedMeanSquares[n].variable,o=this.accumulatedMoments[n].variable;Xi((()=>{const e=to(ro(i,this.decay),ro($u(a),1-this.decay));if(this.centered){const t=this.accumulatedMeanGrads[n].variable,r=to(ro(t,this.decay),ro(a,1-this.decay)),l=so(ro(a,this.learningRate),Dc(Ql(e,to($u(r),this.epsilon)))),u=to(ro(o,this.momentum),l);i.assign(e),t.assign(r),o.assign(u);const c=Ql(s,u);s.assign(c)}else{const e=to(ro(i,this.decay),ro($u(a),1-this.decay)),t=to(ro(o,this.momentum),so(ro(a,this.learningRate),Dc(to(e,this.epsilon))));i.assign(e),o.assign(t);const n=Ql(s,t);s.assign(n)}}))})),this.incrementIterations()}dispose(){null!=this.accumulatedMeanSquares&&Yi(this.accumulatedMeanSquares.map((e=>e.variable))),null!=this.accumulatedMeanGrads&&this.centered&&Yi(this.accumulatedMeanGrads.map((e=>e.variable))),null!=this.accumulatedMoments&&Yi(this.accumulatedMoments.map((e=>e.variable)))}async getWeights(){const e=[...this.accumulatedMeanSquares,...this.accumulatedMoments];return this.centered&&e.push(...this.accumulatedMeanGrads),[await this.saveIterations()].concat(e.map((e=>({name:e.originalName,tensor:e.variable}))))}async setWeights(e){e=await this.extractIterations(e);const t=this.centered?e.length/3:e.length/2,n=!1;this.accumulatedMeanSquares=e.slice(0,t).map((e=>({originalName:e.name,variable:e.tensor.variable(n)}))),this.accumulatedMoments=e.slice(t,2*t).map((e=>({originalName:e.name,variable:e.tensor.variable(n)}))),this.centered&&(this.accumulatedMeanGrads=e.slice(2*t,3*t).map((e=>({originalName:e.name,variable:e.tensor.variable(n)}))))}getConfig(){return{learningRate:this.learningRate,decay:this.decay,momentum:this.momentum,epsilon:this.epsilon,centered:this.centered}}static fromConfig(e,t){return new e(t.learningRate,t.decay,t.momentum,t.epsilon,t.centered)}}Cp.className="RMSProp",zi(Cp);class Tp{static sgd(e){return new Sp(e)}static momentum(e,t,n=!1){return new $p(e,t,n)}static rmsprop(e,t=.9,n=0,s=null,r=!1){return new Cp(e,t,n,s,r)}static adam(e=.001,t=.9,n=.999,s=null){return new Np(e,t,n,s)}static adadelta(e=.001,t=.95,n=null){return new kp(e,t,n)}static adamax(e=.002,t=.9,n=.999,s=null,r=0){return new Ip(e,t,n,s,r)}static adagrad(e,t=.1){return new vp(e,t)}}const Ep={sgd:Tp.sgd,momentum:Tp.momentum,adadelta:Tp.adadelta,adagrad:Tp.adagrad,rmsprop:Tp.rmsprop,adamax:Tp.adamax,adam:Tp.adam},Ap="undefined"!=typeof requestAnimationFrame?requestAnimationFrame:"undefined"!=typeof setImmediate?setImmediate:e=>e();function Rp(){return new Promise((e=>Ap((()=>e()))))}function Fp(e,t){const n=e[0].length;e.forEach(((e,t)=>{l(e.length===n,(()=>`Error in concat${n}D: rank of tensors[${t}] must be the same as the rank of the rest (${n})`))})),l(t>=0&&t<n,(()=>`Error in concat${n}D: axis must be between 0 and ${n-1}.`));const s=e[0];e.forEach(((e,r)=>{for(let a=0;a<n;a++)l(a===t||e[a]===s[a],(()=>`Error in concat${n}D: Shape of tensors[${r}] (${e}) does not match the shape of the rest (${s}) along the non-concatenated axis ${r}.`))}))}function _p(e,t){const n=e[0].slice();for(let s=1;s<e.length;s++)n[t]+=e[s][t];return n}function Dp(e){return e<=30?e:D(e,Math.floor(Math.sqrt(e)))}function Op(e,t,n){return[n*("number"==typeof e?e:e[0]),t*("number"==typeof e?e:e[1])]}function Mp(e,t,n,s=!0){let r=[];if(s)r=r.concat(t.slice(0)),r.push(e[0]/n),r=r.concat(e.slice(1));else{r=r.concat(e[0]);const n=t.length;for(let s=0;s<n;++s)r=r.concat([e[s+1]/t[s],t[s]]);r=r.concat(e.slice(n+1))}return r}function Lp(e,t,n=!0){const s=[];if(n){s.push(t);for(let n=t+1;n<e;++n)n<=2*t?(s.push(n),s.push(n-(t+1))):s.push(n)}else{const n=[],r=[];for(let s=1;s<e;++s)s>=2*t+1||s%2==1?r.push(s):n.push(s);s.push(...n),s.push(0),s.push(...r)}return s}function zp(e,t,n,s=!0){const r=[];s?r.push(e[0]/n):r.push(e[0]*n);for(let n=1;n<e.length;++n)n<=t.length?s?r.push(t[n-1]*e[n]):r.push(e[n]/t[n-1]):r.push(e[n]);return r}function Bp(e,t){const n=[0];for(let s=0;s<t;++s)n.push(e[s][0]);return n}function Pp(e,t,n){const s=e.slice(0,1);for(let r=0;r<n;++r)s.push(e[r+1]-t[r][0]-t[r][1]);return s}const Wp=1.7580993408473768,Vp=1.0507009873554805,Up=.3275911,Gp=.254829592,Hp=-.284496736,jp=1.421413741,qp=-1.453152027,Kp=1.061405429;function Xp(...e){K().getBool("IS_TEST")||console.warn(...e)}function Yp(e,t){if(e.length!==t.length)throw new Error(`Cannot merge real and imag arrays of different lengths. real:${e.length}, imag: ${t.length}.`);const n=new Float32Array(2*e.length);for(let s=0;s<n.length;s+=2)n[s]=e[s/2],n[s+1]=t[s/2];return n}function Zp(e){const t=new Float32Array(e.length/2),n=new Float32Array(e.length/2);for(let s=0;s<e.length;s+=2)t[s/2]=e[s],n[s/2]=e[s+1];return{real:t,imag:n}}function Jp(e){const t=Math.ceil(e.length/4),n=new Float32Array(t),s=new Float32Array(t);for(let t=0;t<e.length;t+=4)n[Math.floor(t/4)]=e[t],s[Math.floor(t/4)]=e[t+1];return{real:n,imag:s}}function Qp(e){const t=Math.floor(e.length/4),n=new Float32Array(t),s=new Float32Array(t);for(let t=2;t<e.length;t+=4)n[Math.floor(t/4)]=e[t],s[Math.floor(t/4)]=e[t+1];return{real:n,imag:s}}function ed(e,t){return{real:e[2*t],imag:e[2*t+1]}}function td(e,t,n,s){e[2*s]=t,e[2*s+1]=n}function nd(e,t){const n=new Float32Array(e/2),s=new Float32Array(e/2);for(let r=0;r<Math.ceil(e/2);r++){const a=(t?2:-2)*Math.PI*(r/e);n[r]=Math.cos(a),s[r]=Math.sin(a)}return{real:n,imag:s}}function sd(e,t,n){const s=(n?2:-2)*Math.PI*(e/t);return{real:Math.cos(s),imag:Math.sin(s)}}const rd="->",ad=/->/g;function id(e,t){const n=((e=e.replace(/\s/g,"")).length-e.replace(ad,"").length)/rd.length;if(n<1)throw new Error("Equations without an arrow are not supported.");if(n>1)throw new Error('Equation must contain exactly one arrow ("->").');const[s,r]=e.split(rd);l(-1===s.indexOf("..."),(()=>'The ellipsis notation ("...") is not supported yet.'));const a=s.split(","),i=a.length;if(t!==i)throw new Error(`Expected ${i} input tensors, received ${t}`);if(i>2)throw new Error("Support for more than 2 input tensors is not implemented yet.");const o=[];for(let e=0;e<r.length;++e){const t=r[e];if(!a.some((e=>-1!==e.indexOf(t))))throw new Error(`Output subscripts contain the label ${t} not present in the input subscripts.`);-1===o.indexOf(t)&&o.push(t)}for(let e=0;e<s.length;++e){const t=s[e];-1===o.indexOf(t)&&","!==t&&o.push(t)}const u=new Array(a.length);for(let e=0;e<i;++e){if(new Set(a[e].split("")).size!==a[e].length)throw new Error(`Found duplicate axes in input component ${a[e]}. Support for duplicate axes in input is not implemented yet.`);u[e]=[];for(let t=0;t<a[e].length;++t)u[e].push(o.indexOf(a[e][t]))}const c=o.length,h=[];for(let e=r.length;e<c;++e)h.push(e);return{allDims:o,summedDims:h,idDims:u}}function od(e,t){let n=new Array(e);n.fill(-1);for(let e=0;e<t.length;++e)n[t[e]]=e;const s=[];for(let t=0;t<e;++t)-1===n[t]&&s.push(t);return n=n.filter((e=>-1!==e)),{permutationIndices:n,expandDims:s}}function ld(e,t,n){const s=new Array(e);for(let e=0;e<n.length;++e){const r=n[e].shape;for(let n=0;n<t[e].length;++n)void 0===s[t[e][n]]?s[t[e][n]]=r[n]:l(s[t[e][n]]===r[n],(()=>`Expected dimension ${s[t[e][n]]} at axis ${n} of input shaped ${JSON.stringify(r)}, but got dimension ${r[n]}`))}}function ud(e,t){const n=e,s=[];let r=0;0===e.length&&n.push(-1),r=e.length+1;for(let e=0;e<r;++e)s.push([]);const a=[];for(let e=0;e<n.length;++e){const r=hd(t,n[e]);for(const t of r)-1===a.indexOf(t)&&(s[e].push(t),a.push(t))}return{path:n,steps:s}}function cd(e){return e.every(((e,t)=>e===t))}function hd(e,t){const n=[];for(let s=0;s<e.length;++s)0!==e[s].length&&-1===e[s].indexOf(t)&&-1!==t||n.push(s);return n}function pd(e,t,n=0){let s=[];if("number"==typeof t)l(e.shape[n]%t==0,(()=>"Number of splits must evenly divide the axis.")),s=new Array(t).fill(e.shape[n]/t);else{l(t.reduce(((e,t)=>(-1===t&&(e+=1),e)),0)<=1,(()=>"There should be only one negative value in split array."));const r=t.indexOf(-1);if(-1!==r){const s=t.reduce(((e,t)=>t>0?e+t:e));t[r]=e.shape[n]-s}l(e.shape[n]===t.reduce(((e,t)=>e+t)),(()=>"The sum of sizes must match the size of the axis dimension.")),s=t}return s}function dd(e,t){let n,s=!1;for(e<=30?(n=e,s=!0):n=D(e,Math.floor(Math.sqrt(e)));!s;)n>t||n===e?s=!0:n=D(e,n+1);return n}function fd(e,t,n){const s=[],r=e.length;for(let a=0;a<r;a++)a!==t?s.push(e[a]):s.push(n);return s}function md(e,t,n,s){const r=t.shape.length,a=e.shape.length;if(0!==s&&(s<-r||s>r))throw new Error(`Expect batchDims in the range of [-${r}, ${r}], but got ${s}`);if(s<0&&(s+=r),s>a)throw new Error(`batchDims (${s}) must be less than rank(x) (\n    ${a}).`);if(n<s)throw new Error(`batchDims (${s}) must be less than or equal to axis (${n}).`);for(let n=0;n<s;++n)if(e.shape[n]!==t.shape[n])throw new Error(`x.shape[${n}]: ${e.shape[n]} should be equal to indices.shape[${n}]: ${t.shape[n]}.`);const i=e.shape[n],o=[];let l=1,u=1,c=1;for(let t=0;t<s;++t)o.push(e.shape[t]),l*=e.shape[t];for(let t=s;t<n;t++)o.push(e.shape[t]),u*=e.shape[t];for(let e=s;e<r;e++)o.push(t.shape[e]);for(let t=n+1;t<a;t++)o.push(e.shape[t]),c*=e.shape[t];return{batchSize:l,sliceSize:c,outerSize:u,dimSize:i,outputShape:o}}var gd=Object.freeze({__proto__:null,segOpComputeOptimalWindowSize:dd,computeOutShape:fd,collectGatherOpShapeInfo:md});function yd(e){try{return e.map((e=>Qs(e)))}catch(e){throw new Error(`Failed to decode encoded string bytes into utf-8, error: ${e}`)}}function bd(e){return e.map((e=>Js(e)))}var xd=Object.freeze({__proto__:null,slice_util:Oi,segment_util:gd,fromUint8ToStringArray:yd,fromStringArrayToUint8:bd,upcastType:vr,axesAreInnerMostDims:nu,combineLocations:su,computeOutAndReduceShapes:ru,expandShapeToKeepDim:au,assertAxesAreInnerMostDims:iu,getAxesPermutation:ou,getUndoAxesPermutation:lu,getInnerMostAxes:uu,getBroadcastDims:fl,getReductionAxes:ml,assertAndGetBroadcastShape:gl,assertParamsConsistent:Fp,computeOutShape:_p,computeDilation2DInfo:xo,computePool2DInfo:wo,computePool3DInfo:ko,computeConv2DInfo:vo,computeConv3DInfo:No,computeDefaultPad:Io,tupleValuesAreOne:Eo,eitherStridesOrDilationsAreOne:Ao,convertConv2DDataFormat:Ro,getFusedDyActivation:uh,getFusedBiasGradient:ch,applyActivation:hh,shouldFuse:ph,PARALLELIZE_THRESHOLD:30,computeOptimalWindowSize:Dp,getImageCenter:Op,getReshaped:Mp,getPermuted:Lp,getReshapedPermuted:zp,getSliceBeginCoords:Bp,getSliceSize:Pp,prepareAndValidate:di,validateUpdateShape:mi,validateInput:gi,calculateShapes:yi,SELU_SCALEALPHA:Wp,SELU_SCALE:Vp,ERF_P:Up,ERF_A1:Gp,ERF_A2:Hp,ERF_A3:jp,ERF_A4:qp,ERF_A5:Kp,warn:Xp,log:function(...e){K().getBool("IS_TEST")||console.log(...e)},mergeRealAndImagArrays:Yp,splitRealAndImagArrays:Zp,complexWithEvenIndex:Jp,complexWithOddIndex:Qp,getComplexWithIndex:ed,assignToTypedArray:td,exponents:nd,exponent:sd,decodeEinsumEquation:id,getEinsumPermutation:od,checkEinsumDimSizes:ld,getEinsumComputePath:ud,isIdentityPermutation:cd,prepareSplitSize:pd}),wd=Object.freeze({__proto__:null,nonMaxSuppressionV3Impl:Ah,nonMaxSuppressionV4Impl:Rh,nonMaxSuppressionV5Impl:Fh,whereImpl:Xc});const kd={kernelName:J,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,zc(za(n,"float32"),-1))}}},vd={kernelName:Q,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>{const t=$u(za(n,"float32")),s=Dc(Ql(yc(1),t));return Xl(so(e,s))}}}},Nd={kernelName:ee,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>{const t=Dc(Ql($u(za(n,"float32")),1));return so(e,t)}}}},Id={kernelName:te,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{let t=e;const s=ml(n.shape,r);return s.length>0&&(t=eu(t,s)),Fo(t,n.shape)},b:()=>{let t=e;const n=ml(s.shape,r);return n.length>0&&(t=eu(t,n)),Fo(t,s.shape)}}}},Sd={kernelName:ne,saveAllInputs:!0,gradFunc:(e,t)=>{const n={};return t.forEach(((t,s)=>{n[s]=()=>e.clone()})),n}},$d={kernelName:ae,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>xl(n)}}},Cd={kernelName:ie,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>xl(n)}}},Td={kernelName:oe,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,Dc(Ql(yc(1),$u(za(n,"float32")))))}}},Ed={kernelName:le,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>{const t=Dc(to(yc(1),$u(za(n,"float32"))));return so(e,t)}}}},Ad={kernelName:he,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{const t=to($u(n),$u(s));let a=ro(e,so(s,t));const i=ml(n.shape,r);return i.length>0&&(a=eu(a,i)),Fo(a,n.shape)},b:()=>{const t=to($u(n),$u(s));let a=Xl(ro(e,so(n,t)));const i=ml(s.shape,r);return i.length>0&&(a=eu(a,i)),Fo(a,s.shape)}}}},Rd={kernelName:ue,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,to($u(za(n,"float32")),1))}}},Fd={kernelName:ce,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,Ql(yc(1),$u(za(n,"float32"))))}}};const _d=jr({avgPool3dGrad_:function(e,t,n,s,r,a){const i=Ur(e,"dy","avgPool3dGrad"),o=Ur(t,"input","avgPool3dGrad");let u=i,c=o,h=!1;4===o.rank&&(h=!0,u=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2],i.shape[3]]),c=Fo(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),l(5===u.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${u.rank}.`)),l(5===c.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),null!=a&&l(f(r),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const p={dy:u,input:c},d={filterSize:n,strides:s,pad:r,dimRoundingMode:a},m=Dr.runKernel(me,p,d);return h?Fo(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),Dd={kernelName:fe,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{filterSize:r,strides:a,pad:i,dimRoundingMode:o}=n;return{x:()=>_d(e,s,r,a,i,o)}}};const Od=jr({avgPoolGrad_:function(e,t,n,s,r){const a=Ur(e,"dy","avgPoolGrad"),i=Ur(t,"input","avgPoolGrad");l(i.rank===a.rank,(()=>`Rank of input (${i.rank}) does not match rank of dy (${a.rank})`));let o=i,u=a,c=!1;3===i.rank&&(c=!0,o=Fo(i,[1,i.shape[0],i.shape[1],i.shape[2]]),u=Fo(a,[1,a.shape[0],a.shape[1],a.shape[2]])),l(4===u.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${u.rank}.`)),l(4===o.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${o.rank}.`));const h={dy:u,input:o},p={filterSize:n,strides:s,pad:r},d=Dr.runKernel(de,h,p);return c?Fo(d,[d.shape[1],d.shape[2],d.shape[3]]):d}}),Md={kernelName:pe,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{filterSize:r,strides:a,pad:i}=n;return{x:()=>Od(e,s,r,a,i)}}},Ld={kernelName:ge,inputsToSave:["a","b"],gradFunc:(e,t,n)=>{const[s,r]=t,{transposeA:a,transposeB:i}=n;return a||i?!a&&i?{a:()=>ni(e,r,!1,!1),b:()=>ni(e,s,!0,!1)}:a&&!i?{a:()=>ni(r,e,!1,!0),b:()=>ni(s,e,!1,!1)}:{a:()=>ni(r,e,!0,!0),b:()=>ni(e,s,!0,!0)}:{a:()=>ni(e,r,!1,!0),b:()=>ni(s,e,!0,!1)}}},zd={kernelName:ye,gradFunc:(e,t,n)=>{const{blockShape:s,crops:r}=n;return{x:()=>zu(e,s,r)}}},Bd={kernelName:xe,gradFunc:(e,t,n)=>{const s=n,r=s.inputShape,a=s.shape,i=Array.from(a);for(let e=r.length-1;e>=0;e--)if(r[e]===a[e])i[e]=1;else if(1!==r[e])throw new Error(`broadcastTo(): [${r}] cannot be broadcast to [${a}].`);const o=[];for(let e=0;e<i.length;e++)i[e]>1&&o.push(e);return{x:()=>eu(e,o,!0)}}},Pd={kernelName:we,gradFunc:e=>({x:()=>e.clone()})},Wd={kernelName:ke,gradFunc:e=>({x:()=>xl(e)})},Vd={kernelName:ve,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{clipValueMin:r,clipValueMax:a}=n;return{x:()=>bl(hu(Dl(s,r),Wl(s,a)),e,xl(e))}}},Ud={kernelName:Ie,inputsToSave:["x"],gradFunc:kd.gradFunc},Gd={kernelName:Se,saveAllInputs:!0,gradFunc:(e,t,n)=>{const s=t.map((e=>e.shape)),{axis:r}=n,a=x(r,t[0].shape)[0],i=s.map((e=>e[a]));return Fc(e,i,a).map((e=>()=>e))}},Hd={kernelName:$e,inputsToSave:["x","filter"],gradFunc:(e,t,n)=>{const[s,r]=t,{dilations:a,strides:i,pad:o,dataFormat:u}=n;return l(Eo(a),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${a}'`)),{x:()=>tl(s.shape,e,r,i,o,u),filter:()=>lh(s,e,r.shape,i,o,u)}}},jd={kernelName:Te,inputsToSave:["dy","filter"],gradFunc:(e,t,n)=>{const[s,r]=t,{strides:a,pad:i,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>Qo(e,r,a,i,o,1,l),filter:()=>lh(e,s,r.shape,a,i,o,l)}}};const qd=jr({conv3DBackpropFilter_:function(e,t,n,s,r){let a=e;4===e.rank&&(a=Fo(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]));let i=t;4===i.rank&&(i=Fo(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]])),l(5===a.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${a.shape}.`)),l(5===i.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${i.shape}.`)),l(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),l(a.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${a.shape[4]}) must match input depth in filter (${n[3]}.`)),l(i.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${i.shape[4]}) must match output depth for filter (${n[4]}).`));const o={x:a,dy:i},u={strides:s,pad:r,filterShape:n};return Dr.runKernel(Ae,o,u)}}),Kd={kernelName:Ee,inputsToSave:["x","filter"],gradFunc:(e,t,n)=>{const{dilations:s,strides:r,pad:a}=n;l(Eo(s),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`));const[i,o]=t;return{x:()=>rl(i.shape,e,o,r,a),filter:()=>qd(i,e,o.shape,r,a)}}},Xd={kernelName:Fe,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(Xl(vc(za(n,"float32"))),e)}}},Yd={kernelName:_e,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(Nc(za(n,"float32")),e)}}},Zd={kernelName:De,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{axis:r,exclusive:a,reverse:i}=n;return{x:()=>{const t=ou([r],s.rank);let n=ll(e,r,a,!i);return null!=t&&(n=ri(n,t)),n}}}},Jd={kernelName:ze,inputsToSave:["x","filter"],gradFunc:(e,t,n)=>{const{dilations:s,strides:r,pad:a,dimRoundingMode:i}=n,o=null==s?[1,1]:s;l(Eo(o),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`));const[u,c]=t;return l(4===u.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${u.rank}.`)),l(4===c.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${c.rank}.`)),l(u.shape[3]===c.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${u.shape[3]}) must match the inChannels dimension in filter ${c.shape[2]}.`)),l(Ao(r,o),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${r} and dilations '${o}'.`)),null!=i&&l(f(a),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${i} but got pad ${a}.`)),{x:()=>mh(u.shape,e,c,r,a,s,i),filter:()=>fh(u,e,c.shape,r,a,s,i)}}},Qd={kernelName:Ve,inputsToSave:["x","filter"],gradFunc:(e,t,n)=>{const[s,r]=t,a={x:s,filter:r,dy:e},i={x:s,filter:r,dy:e};return{x:()=>Dr.runKernel(Ue,a,n),filter:()=>Dr.runKernel(Ge,i,n)}}},ef={kernelName:qe,outputsToSave:[!0],gradFunc:(e,t)=>{const[n]=t,s={dy:e,y:n};return{x:()=>Dr.runKernel(Ke,s)}}},tf={kernelName:Xe,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t,s=ro(Sl(Xl($u(n))),2/Math.sqrt(Math.PI));return{x:()=>ro(e,s)}}},nf={kernelName:Ze,outputsToSave:[!0],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,n)}}},sf={kernelName:Je,inputsToSave:["input"],gradFunc:(e,t)=>{const[n]=t;return{input:()=>Fo(e,n.shape)}}},rf={kernelName:Qe,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,Sl(n))}}},af={kernelName:st,gradFunc:e=>({x:()=>xl(e)})},of={kernelName:rt,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{const t=so(e,za(s,"float32")),a=ml(n.shape,r);return a.length>0?Fo(eu(t,a),n.shape):t},b:()=>{let t=ro(e,za(n,"float32"));const a=ml(s.shape,r);a.length>0&&(t=Fo(eu(t,a),s.shape));const i=$u(s);return Xl(so(t,za(i,"float32")))}}}},lf={kernelName:at,inputsToSave:["x","mean","variance","scale"],gradFunc:(e,t,n)=>{const{varianceEpsilon:s}=n,[r,a,i,o]=t,l=null==o?yc(1):o,u=ml(a.shape,r.shape),c=[];if(1===a.rank){for(let e=0;e<r.shape.length-1;++e)c.push(r.shape[e]);c.push(1)}const h=Ql(r,a),p=ro(e,l),d=gc(to(i,yc(s))),f=ro(ro(ro(d,d),d),yc(-.5));return{x:()=>1===a.rank?Fo(ro(ro(e,Tl(Fo(d,[1,1,1,a.shape[0]]),c)),l),r.shape):Fo(ro(ro(e,d),l),r.shape),mean:()=>{let e=ro(ro(d,yc(-1)),p);return 1===a.rank&&(e=eu(e,u)),Fo(e,a.shape)},variance:()=>{let e=ro(ro(f,h),p);return 1===a.rank&&(e=eu(e,u)),Fo(e,a.shape)},scale:()=>{const t=ro(h,d);let n=ro(e,t);return 1===a.rank&&(n=eu(n,u)),Fo(n,a.shape)},offset:()=>{let t=e;return 1===a.rank&&(t=eu(t,u)),Fo(t,a.shape)}}}},uf={kernelName:it,inputsToSave:["x","indices"],gradFunc:(e,t,n)=>{const[s,r]=t,{axis:a}=n,i=x(a,s.shape)[0];return{x:()=>{const t=s.shape,n=r.size,o=t.slice(0,i),l=o.length,u=t.slice(a,t.length).slice(1),c=u.length,h=cf(0,l),p=cf(l+1,l+1+c),d=hf([o,[n],u]),f=Fo(e,d),m=Fo(r,[n]),g=hf([[l],h,p]),y=ri(f,g);let b=jc(y,m,s.shape[i]);const x=lu(g);return b=ri(b,x),b},indices:()=>r}}};function cf(e,t){const n=[];for(let s=e;s<t;++s)n.push(s);return n}function hf(e){const t=[];for(let n=0;n<e.length;++n)for(let s=0;s<e[n].length;++s)t.push(e[n][s]);return t}const pf={kernelName:ut,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t;return{a:()=>xl(n),b:()=>xl(s)}}},df={kernelName:ct,gradFunc:e=>({x:()=>za(e,"float32")})},ff={kernelName:dt,gradFunc:e=>({x:()=>xl(e)})},mf={kernelName:ft,gradFunc:e=>({x:()=>xl(e)})},gf={kernelName:mt,gradFunc:e=>({x:()=>xl(e)})},yf={kernelName:gt,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{alpha:r}=n,a=_l(s,0);return{x:()=>bl(a,e,ro(e,r))}}},bf={kernelName:kt,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,to(n,1))}}},xf={kernelName:wt,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,za(n,"float32"))}}},wf={kernelName:St,inputsToSave:[],outputsToSave:[!0],gradFunc:(e,t,n)=>{const[s]=t,{axis:r}=n;return{logits:()=>{const t=Sl(s);return Ql(e,ro(eu(e,r,!0),t))}}}};const kf=jr({localResponseNormalizationBackprop_:function(e,t,n,s=5,r=1,a=1,i=.5){const o={x:e,y:t,dy:n},l={depthRadius:s,bias:r,alpha:a,beta:i};return Dr.runKernel(Ct,o,l)}}),vf={kernelName:$t,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(e,t,n)=>{const[s,r]=t,{depthRadius:a,bias:i,alpha:o,beta:l}=n;return{x:()=>kf(s,r,e,a,i,o,l)}}};function Nf(e,t,n,s){return t.rank<n.rank&&(t=Fo(t,au(t.shape,s))),e.rank<n.rank&&(e=Fo(e,au(e.shape,s))),{x:()=>ro(e,za(yl(n,t),e.dtype))}}const If={kernelName:Tt,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(e,t,n)=>{const s=n,{reductionIndices:r}=s,a=t[0],i=Nf(e,t[1],a,x(r,a.shape));return{x:()=>i.x()}}},Sf={kernelName:Et,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t;return{a:()=>ro(e,za(Dl(n,s),"float32")),b:()=>ro(e,za(Pl(n,s),"float32"))}}};const $f=jr({maxPool3dGrad_:function(e,t,n,s,r,a,i){const o=Ur(e,"dy","maxPool3dGrad"),u=Ur(t,"input","maxPool3dGrad"),c=Ur(n,"output","maxPool3dGrad");let h=o,p=u,d=c,m=!1;4===u.rank&&(m=!0,h=Fo(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),p=Fo(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]]),d=Fo(c,[1,c.shape[0],c.shape[1],c.shape[2],c.shape[3]])),l(5===h.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),l(5===p.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${p.rank}.`)),l(5===d.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${d.rank}.`)),null!=i&&l(f(a),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${i} but got pad ${a}.`));const g={dy:h,input:p,output:d},y={filterSize:s,strides:r,pad:a,dimRoundingMode:i},b=Dr.runKernel(_t,g,y);return m?Fo(b,[b.shape[1],b.shape[2],b.shape[3],b.shape[4]]):b}}),Cf={kernelName:Ft,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(e,t,n)=>{const[s,r]=t,{filterSize:a,strides:i,pad:o,dimRoundingMode:l}=n;return{x:()=>$f(e,s,r,a,i,o,l)}}};const Tf=jr({maxPoolGrad_:function(e,t,n,s,r,a,i){const o=Ur(e,"dy","maxPoolGrad"),u=Ur(t,"input","maxPoolGrad"),c=Ur(n,"output","maxPoolGrad");l(u.rank===o.rank,(()=>`Rank of input (${u.rank}) does not match rank of dy (${o.rank})`)),l(4===o.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${o.rank}.`)),l(4===u.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${u.rank}.`)),null!=i&&l(f(a),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${i} but got pad ${a}.`));const h={dy:o,input:u,output:c},p={filterSize:s,strides:r,pad:a,dimRoundingMode:i};return Dr.runKernel(Rt,h,p)}}),Ef={kernelName:Yt,inputsToSave:["x"],gradFunc:(e,t,n)=>{const s=t[0],{paddings:r}=n,a=r.map((e=>e[0]));return{x:()=>Lo(e,a,s.shape)}}},Af={kernelName:Sn,gradFunc:(e,t,n)=>{const{blockShape:s,paddings:r}=n;return{x:()=>Po(e,s,r)}}},Rf={kernelName:$n,gradFunc:(e,t,n)=>{const{axis:s}=n;return{x:()=>Oo(e,s)}}};const Ff=[kd,vd,Nd,Id,Sd,$d,Cd,Td,Ed,Ad,Rd,Fd,Dd,Md,Ld,zd,Bd,Pd,Wd,Vd,Ud,Gd,jd,Hd,Kd,Xd,Yd,Zd,Jd,Qd,{kernelName:He,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{const t=so(e,za(s,"float32")),a=ml(n.shape,r);return a.length>0?Fo(eu(t,a),n.shape):t},b:()=>{let t=ro(e,za(n,"float32"));const a=ml(s.shape,r);a.length>0&&(t=Fo(eu(t,a),s.shape));const i=$u(s);return Xl(so(t,za(i,"float32")))}}}},ef,tf,nf,sf,rf,of,af,lf,uf,pf,df,ff,mf,gf,yf,bf,xf,wf,vf,If,If,Sf,Cf,{kernelName:At,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(e,t,n)=>{const[s,r]=t,{filterSize:a,strides:i,pad:o}=n;return{x:()=>Tf(e,s,r,a,i,o)}}},{kernelName:Ot,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{axis:r}=n,a=x(r,s.shape),i=p(ru(s.shape,a)[1]);return{x:()=>{const t=s.shape.slice();a.forEach((e=>{t[e]=1}));const n=Fo(e,t);return so(ro(n,ku(s.shape,"float32")),i)}}}},{kernelName:Mt,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(e,t,n)=>{const s=n,{axis:r}=s,[a,i]=t,o=Nf(e,i,a,x(r,a.shape));return{x:()=>o.x()}}},{kernelName:Lt,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t;return{a:()=>ro(e,za(Wl(n,s),"float32")),b:()=>ro(e,za(_l(n,s),"float32"))}}},{kernelName:zt,inputsToSave:["x"],gradFunc:(e,t,n)=>{const s=t[0],{paddings:r}=n,a=r.map((e=>e[0]));return{x:()=>Lo(e,a,s.shape)}}},{kernelName:Bt,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{const t=ml(n.shape,r);return t.length>0?Fo(eu(e,t),n.shape):e},b:()=>{const t=ro(e,Xl(Rl(so(n,s)))),a=ml(s.shape,r);return a.length>0?Fo(eu(t,a),s.shape):t}}}},{kernelName:Wt,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{const t=ro(e,za(s,"float32")),a=ml(n.shape,r);return a.length>0?Fo(eu(t,a),n.shape):t},b:()=>{const t=ro(e,za(n,"float32")),a=ml(s.shape,r);return a.length>0?Fo(eu(t,a),s.shape):t}}}},{kernelName:Vt,gradFunc:e=>({x:()=>Xl(e)})},{kernelName:Kt,inputsToSave:["indices"],gradFunc:(e,t)=>{const n=t[0];return{indices:()=>wu(n.shape,"float32")}}},{kernelName:qt,gradFunc:e=>({x:()=>xl(e)})},{kernelName:Xt,saveAllInputs:!0,gradFunc:(e,t,n)=>{const{axis:s}=n;return qc(e,s).map((e=>()=>e))}},Ef,Ef,{kernelName:Zt,inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(e,t)=>{const[n,s,r]=t,a=n,i=s,o=gl(a.shape,i.shape);return{a:()=>{const t=za(i,"float32");let n=ro(e,ro(t,Pu(a,Ql(t,yc(1)))));const s=ml(a.shape,o);return s.length>0&&(n=eu(n,s)),Fo(n,a.shape)},b:()=>{const t=_l(a,0),n=bl(t,Gl(a),xl(a));let s=ro(e,ro(r,n));const l=ml(i.shape,o);return l.length>0&&(s=eu(s,l)),Fo(s,i.shape)}}}},{kernelName:Jt,inputsToSave:["x","alpha"],gradFunc:(e,t)=>{const[n,s]=t,r=_l(n,0);return{x:()=>bl(r,e,ro(e,s)),alpha:()=>{let t=bl(r,xl(e),ro(e,n));const a=ml(s.shape,e.shape);return a.length>0&&(t=eu(t,a)),Fo(t,s.shape)}}}},{kernelName:nn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,Xl($u(n)))}}},{kernelName:cn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t,s=ro(Wl(n,6),zc(n));return{x:()=>ro(e,za(s,"float32"))}}},{kernelName:sn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,za(zc(n),"float32"))}}},{kernelName:rn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>Fo(e,n.shape)}}},{kernelName:ln,inputsToSave:["images"],gradFunc:(e,t,n)=>{const[s]=t,r={dy:e,images:s};return{images:()=>Dr.runKernel(un,r,n)}}},{kernelName:an,inputsToSave:["images"],gradFunc:(e,t,n)=>{const[s]=t,r={dy:e,images:s};return{images:()=>Dr.runKernel(on,r,n)}}},{kernelName:hn,gradFunc:(e,t,n)=>{const{dims:s}=n,r=x(s,e.shape);return{x:()=>cc(e,r)}}},{kernelName:pn,gradFunc:e=>({x:()=>xl(e)})},{kernelName:dn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>Xl(so(e,ro(Pu(n,1.5),2)))}}},{kernelName:mn,inputsToSave:["condition"],gradFunc:(e,t)=>{const[n]=t;return{condition:()=>za(xl(n),"float32"),t:()=>ro(e,za(n,e.dtype)),e:()=>ro(e,za(pu(n),e.dtype))}}},{kernelName:gn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>{const t=_l(n,yc(0)),s=yc(Wp),r=yc(Vp),a=ro(e,r),i=ro(ro(e,s),Sl(za(n,"float32")));return bl(t,a,i)}}}},{kernelName:kn,outputsToSave:[!0],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,ro(n,Ql(yc(1),n)))}}},{kernelName:wn,gradFunc:e=>({x:()=>xl(e)})},{kernelName:bn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(il(za(n,"float32")),e)}}},{kernelName:xn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(ol(za(n,"float32")),e)}}},{kernelName:yn,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{begin:r,size:a}=n,i=s.shape,[o,l]=_i(s,r,a),u=[];for(let t=0;t<e.rank;t++)u.push([o[t],i[t]-o[t]-l[t]]);return{x:()=>_u(e,u)}}},{kernelName:Cn,outputsToSave:[!0],gradFunc:(e,t,n)=>{const[s]=t,{dim:r}=n,a=ro(e,s);return{logits:()=>Ql(a,ro(eu(a,[r],true),s))}}},{kernelName:vn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,Mo(n))}}},Af,Af,Rf,Rf,{kernelName:Nn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,ro(Dc(za(n,"float32")),2))}}},{kernelName:_n,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=yc(2);return{a:()=>ro(e,ro(r,Ql(n,s))),b:()=>ro(e,ro(r,Ql(s,n)))}}},{kernelName:Dn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(e,ro(za(n,"float32"),2))}}},{kernelName:Yn,gradFunc:e=>({x:()=>xl(e)})},{kernelName:Bn,inputsToSave:["a","b"],gradFunc:(e,t)=>{const[n,s]=t,r=gl(n.shape,s.shape);return{a:()=>{let t=e;const s=ml(n.shape,r);return s.length>0&&(t=eu(t,s)),Fo(t,n.shape)},b:()=>{let t=e;const n=ml(s.shape,r);return n.length>0&&(t=eu(t,n)),Fo(Xl(t),s.shape)}}}},{kernelName:In,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,r=s.shape.slice(),{axis:a}=n;x(a,s.shape).forEach((e=>{r[e]=1}));const i=Fo(e,r),o=ro(i,ku(s.shape,"float32"));return{x:()=>o}}},{kernelName:Pn,inputsToSave:["x"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>so(e,$u(il(n)))}}},{kernelName:Wn,outputsToSave:[!0],gradFunc:(e,t)=>{const[n]=t;return{x:()=>ro(Ql(yc(1),$u(n)),e)}}},{kernelName:Vn,inputsToSave:["x"],gradFunc:(e,t,n)=>{const[s]=t,{reps:r}=n;return{x:()=>{let t=xl(s);if(1===s.rank)for(let n=0;n<r[0];++n)t=to(t,Lo(e,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<r[0];++n)for(let a=0;a<r[1];++a)t=to(t,Lo(e,[n*s.shape[0],a*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<r[0];++n)for(let a=0;a<r[1];++a)for(let i=0;i<r[2];++i)t=to(t,Lo(e,[n*s.shape[0],a*s.shape[1],i*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<r[0];++n)for(let a=0;a<r[1];++a)for(let i=0;i<r[2];++i)for(let o=0;o<r[3];++o)t=to(t,Lo(e,[n*s.shape[0],a*s.shape[1],i*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return t}}}},{kernelName:Hn,gradFunc:(e,t,n)=>{const s=n,{perm:r}=s,a=lu(r);return{x:()=>ri(e,a)}}},{kernelName:qn,gradFunc:(e,t,n)=>{const s=n,{axis:r}=s;return{value:()=>Lc(e,r)}}},{kernelName:Kn,inputsToSave:["segmentIds"],gradFunc:(e,t)=>{const[n]=t;return{x:()=>function(e,t){const n=bu(t,xl(t)),s=Fl(e,n);let r=Dl(t,yc(0,"int32"));const a=s.rank-r.rank;for(let e=0;e<a;++e)r=$l(r,e+1);r=hu(r,ku(s.shape,"bool"));const i=xl(s);return bl(r,s,i)}(e,n)}}},{kernelName:Xn,gradFunc:e=>({x:()=>xl(e)})}];for(const e of Ff)ls(e);let _f;function Df(){return null==_f&&(_f=eo().epsilon()),_f}fr().prototype.abs=function(){return this.throwIfDisposed(),ao(this)},fr().prototype.acos=function(){return this.throwIfDisposed(),io(this)},fr().prototype.acosh=function(){return this.throwIfDisposed(),oo(this)},fr().prototype.add=function(e){return this.throwIfDisposed(),to(this,e)},fr().prototype.all=function(e,t){return this.throwIfDisposed(),uo(this,e,t)},fr().prototype.any=function(e,t){return this.throwIfDisposed(),co(this,e,t)},fr().prototype.argMax=function(e){return this.throwIfDisposed(),ho(this,e)},fr().prototype.argMin=function(e){return this.throwIfDisposed(),po(this,e)},fr().prototype.asScalar=function(){return this.throwIfDisposed(),l(1===this.size,(()=>"The array must have only 1 element.")),Fo(this,[])},fr().prototype.asType=function(e){return this.throwIfDisposed(),za(this,e)},fr().prototype.as1D=function(){return this.throwIfDisposed(),Fo(this,[this.size])},fr().prototype.as2D=function(e,t){return this.throwIfDisposed(),Fo(this,[e,t])},fr().prototype.as3D=function(e,t,n){return this.throwIfDisposed(),Fo(this,[e,t,n])},fr().prototype.as4D=function(e,t,n,s){return this.throwIfDisposed(),Fo(this,[e,t,n,s])},fr().prototype.as5D=function(e,t,n,s,r){return this.throwIfDisposed(),Fo(this,[e,t,n,s,r])},fr().prototype.asin=function(){return this.throwIfDisposed(),fo(this)},fr().prototype.asinh=function(){return this.throwIfDisposed(),mo(this)},fr().prototype.atan=function(){return this.throwIfDisposed(),go(this)},fr().prototype.atan2=function(e){return this.throwIfDisposed(),yo(this,e)},fr().prototype.atanh=function(){return this.throwIfDisposed(),bo(this)},fr().prototype.avgPool=function(e,t,n,s){return this.throwIfDisposed(),_o(this,e,t,n,s)},fr().prototype.batchToSpaceND=function(e,t){return this.throwIfDisposed(),Po(this,e,t)},fr().prototype.batchNorm=function(e,t,n,s,r){return this.throwIfDisposed(),Wo(this,e,t,n,s,r)},fr().prototype.broadcastTo=function(e){return this.throwIfDisposed(),jo(this,e)},fr().prototype.cast=function(e){return this.throwIfDisposed(),za(this,e)},fr().prototype.ceil=function(){return this.throwIfDisposed(),qo(this)},fr().prototype.clipByValue=function(e,t){return this.throwIfDisposed(),Ko(this,e,t)},fr().prototype.concat=function(e,t){return this.throwIfDisposed(),e instanceof dr&&(e=[e]),Oo([this,...e],t)},fr().prototype.conv1d=function(e,t,n,s,r,a){return this.throwIfDisposed(),el(this,e,t,n,s,r,a)},fr().prototype.conv2dTranspose=function(e,t,n,s,r){return this.throwIfDisposed(),nl(this,e,t,n,s,r)},fr().prototype.conv2d=function(e,t,n,s,r,a){return this.throwIfDisposed(),Qo(this,e,t,n,s,r,a)},fr().prototype.cos=function(){return this.throwIfDisposed(),il(this)},fr().prototype.cosh=function(){return this.throwIfDisposed(),ol(this)},fr().prototype.cumsum=function(e,t,n){return this.throwIfDisposed(),ll(this,e,t,n)},fr().prototype.depthToSpace=function(e,t){return this.throwIfDisposed(),cl(this,e,t)},fr().prototype.depthwiseConv2d=function(e,t,n,s,r,a){return this.throwIfDisposed(),hl(this,e,t,n,s,r,a)},fr().prototype.dilation2d=function(e,t,n,s,r){return this.throwIfDisposed(),dl(this,e,t,n,s,r)},fr().prototype.divNoNan=function(e){return this.throwIfDisposed(),wl(this,e)},fr().prototype.div=function(e){return this.throwIfDisposed(),so(this,e)},fr().prototype.dot=function(e){return this.throwIfDisposed(),kl(this,e)},fr().prototype.elu=function(){return this.throwIfDisposed(),Nl(this)},fr().prototype.equal=function(e){return this.throwIfDisposed(),yl(this,e)},fr().prototype.erf=function(){return this.throwIfDisposed(),Il(this)},fr().prototype.exp=function(){return this.throwIfDisposed(),Sl(this)},fr().prototype.expandDims=function(e){return this.throwIfDisposed(),$l(this,e)},fr().prototype.expm1=function(){return this.throwIfDisposed(),Cl(this)},fr().prototype.fft=function(){return this.throwIfDisposed(),Ec(this)},fr().prototype.flatten=function(){return this.throwIfDisposed(),Fo(this,[this.size])},fr().prototype.floor=function(){return this.throwIfDisposed(),Rl(this)},fr().prototype.floorDiv=function(e){return this.throwIfDisposed(),no(this,e)},fr().prototype.gather=function(e,t){return this.throwIfDisposed(),Fl(this,e,t)},fr().prototype.greaterEqual=function(e){return this.throwIfDisposed(),Dl(this,e)},fr().prototype.greater=function(e){return this.throwIfDisposed(),_l(this,e)},fr().prototype.ifft=function(){return this.throwIfDisposed(),Ac(this)},fr().prototype.irfft=function(){return this.throwIfDisposed(),Rc(this)},fr().prototype.isFinite=function(){return this.throwIfDisposed(),Ml(this)},fr().prototype.isInf=function(){return this.throwIfDisposed(),Ll(this)},fr().prototype.isNaN=function(){return this.throwIfDisposed(),zl(this)},fr().prototype.leakyRelu=function(e){return this.throwIfDisposed(),Bl(this,e)},fr().prototype.lessEqual=function(e){return this.throwIfDisposed(),Wl(this,e)},fr().prototype.less=function(e){return this.throwIfDisposed(),Pl(this,e)},fr().prototype.localResponseNormalization=function(e,t,n,s){return this.throwIfDisposed(),Ul(this,e,t,n,s)},fr().prototype.logSigmoid=function(){return this.throwIfDisposed(),Zl(this)},fr().prototype.logSoftmax=function(e){return this.throwIfDisposed(),tu(this,e)},fr().prototype.logSumExp=function(e,t){return this.throwIfDisposed(),cu(this,e,t)},fr().prototype.log=function(){return this.throwIfDisposed(),Gl(this)},fr().prototype.log1p=function(){return this.throwIfDisposed(),Hl(this)},fr().prototype.logicalAnd=function(e){return this.throwIfDisposed(),hu(this,e)},fr().prototype.logicalNot=function(){return this.throwIfDisposed(),pu(this)},fr().prototype.logicalOr=function(e){return this.throwIfDisposed(),du(this,e)},fr().prototype.logicalXor=function(e){return this.throwIfDisposed(),fu(this,e)},fr().prototype.matMul=function(e,t,n){return this.throwIfDisposed(),ni(this,e,t,n)},fr().prototype.maxPool=function(e,t,n,s){return this.throwIfDisposed(),mu(this,e,t,n,s)},fr().prototype.max=function(e,t){return this.throwIfDisposed(),Jl(this,e,t)},fr().prototype.maximum=function(e){return this.throwIfDisposed(),bu(this,e)},fr().prototype.mean=function(e,t){return this.throwIfDisposed(),xu(this,e,t)},fr().prototype.min=function(e,t){return this.throwIfDisposed(),vu(this,e,t)},fr().prototype.minimum=function(e){return this.throwIfDisposed(),Nu(this,e)},fr().prototype.mirrorPad=function(e,t){return this.throwIfDisposed(),Iu(this,e,t)},fr().prototype.mod=function(e){return this.throwIfDisposed(),Su(this,e)},fr().prototype.mul=function(e){return this.throwIfDisposed(),ro(this,e)},fr().prototype.neg=function(){return this.throwIfDisposed(),Xl(this)},fr().prototype.norm=function(e,t,n){return this.throwIfDisposed(),Qc(this,e,t,n)},fr().prototype.notEqual=function(e){return this.throwIfDisposed(),Au(this,e)},fr().prototype.oneHot=function(e,t=1,n=0){return this.throwIfDisposed(),si(this,e,t,n)},fr().prototype.onesLike=function(){return this.throwIfDisposed(),Ru(this)},fr().prototype.pad=function(e,t){return this.throwIfDisposed(),_u(this,e,t)},fr().prototype.pool=function(e,t,n,s,r){return this.throwIfDisposed(),Bu(this,e,t,n,s,r)},fr().prototype.pow=function(e){return this.throwIfDisposed(),Pu(this,e)},fr().prototype.prelu=function(e){return this.throwIfDisposed(),Wu(this,e)},fr().prototype.prod=function(e,t){return this.throwIfDisposed(),Vu(this,e,t)},fr().prototype.reciprocal=function(){return this.throwIfDisposed(),oc(this)},fr().prototype.relu=function(){return this.throwIfDisposed(),lc(this)},fr().prototype.relu6=function(){return this.throwIfDisposed(),uc(this)},fr().prototype.reshapeAs=function(e){return this.throwIfDisposed(),Fo(this,e.shape)},fr().prototype.reshape=function(e){return this.throwIfDisposed(),Fo(this,e)},fr().prototype.resizeBilinear=function(e,t,n){return this.throwIfDisposed(),Vh(this,e,t,n)},fr().prototype.resizeNearestNeighbor=function(e,t,n){return this.throwIfDisposed(),Uh(this,e,t,n)},fr().prototype.reverse=function(e){return this.throwIfDisposed(),cc(this,e)},fr().prototype.rfft=function(){return this.throwIfDisposed(),_c(this)},fr().prototype.round=function(){return this.throwIfDisposed(),mc(this)},fr().prototype.rsqrt=function(){return this.throwIfDisposed(),gc(this)},fr().prototype.selu=function(){return this.throwIfDisposed(),bc(this)},fr().prototype.separableConv2d=function(e,t,n,s,r,a){return this.throwIfDisposed(),xc(this,e,t,n,s,r,a)},fr().prototype.sigmoid=function(){return this.throwIfDisposed(),Mo(this)},fr().prototype.sign=function(){return this.throwIfDisposed(),kc(this)},fr().prototype.sin=function(){return this.throwIfDisposed(),vc(this)},fr().prototype.sinh=function(){return this.throwIfDisposed(),Nc(this)},fr().prototype.slice=function(e,t){return this.throwIfDisposed(),Lo(this,e,t)},fr().prototype.softmax=function(e){return this.throwIfDisposed(),Tc(this,e)},fr().prototype.softplus=function(){return this.throwIfDisposed(),Yl(this)},fr().prototype.spaceToBatchND=function(e,t){return this.throwIfDisposed(),zu(this,e,t)},fr().prototype.split=function(e,t){return this.throwIfDisposed(),Fc(this,e,t)},fr().prototype.sqrt=function(){return this.throwIfDisposed(),Dc(this)},fr().prototype.square=function(){return this.throwIfDisposed(),$u(this)},fr().prototype.squaredDifference=function(e){return this.throwIfDisposed(),Oc(this,e)},fr().prototype.squeeze=function(e){return this.throwIfDisposed(),Mc(this,e)},fr().prototype.stack=function(e,t){this.throwIfDisposed();const n=e instanceof dr?[this,e]:[this,...e];return Lc(n,t)},fr().prototype.step=function(e){return this.throwIfDisposed(),zc(this,e)},fr().prototype.stridedSlice=function(e,t,n,s,r,a,i,o){return this.throwIfDisposed(),Bc(this,e,t,n,s,r,a,i,o)},fr().prototype.sub=function(e){return this.throwIfDisposed(),Ql(this,e)},fr().prototype.sum=function(e,t){return this.throwIfDisposed(),eu(this,e,t)},fr().prototype.tan=function(){return this.throwIfDisposed(),Pc(this)},fr().prototype.tanh=function(){return this.throwIfDisposed(),zo(this)},fr().prototype.tile=function(e){return this.throwIfDisposed(),Tl(this,e)},fr().prototype.toBool=function(){return this.throwIfDisposed(),za(this,"bool")},fr().prototype.toFloat=function(){return this.throwIfDisposed(),za(this,"float32")},fr().prototype.toInt=function(){return this.throwIfDisposed(),za(this,"int32")},fr().prototype.topk=function(e,t){return this.throwIfDisposed(),Uc(this,e,t)},fr().prototype.transpose=function(e){return this.throwIfDisposed(),ri(this,e)},fr().prototype.unique=function(e){return this.throwIfDisposed(),Hc(this,e)},fr().prototype.unsortedSegmentSum=function(e,t){return this.throwIfDisposed(),jc(this,e,t)},fr().prototype.unstack=function(e){return this.throwIfDisposed(),qc(this,e)},fr().prototype.where=function(e,t){return this.throwIfDisposed(),bl(e,this,t)},fr().prototype.zerosLike=function(){return this.throwIfDisposed(),xl(this)};class Of extends Error{constructor(e){super(e),Object.setPrototypeOf(this,Of.prototype)}}class Mf extends Error{constructor(e){super(e),Object.setPrototypeOf(this,Mf.prototype)}}class Lf extends Error{constructor(e){super(e),Object.setPrototypeOf(this,Lf.prototype)}}class zf extends Error{constructor(e){super(e),Object.setPrototypeOf(this,zf.prototype)}}class Bf extends Error{constructor(e){super(e),Object.setPrototypeOf(this,Bf.prototype)}}Error;function Pf(e,t){if(Array.isArray(e)){let n=[];for(let s=0;s<t;s++)n=n.concat(e);return n}{const n=new Array(t);return n.fill(e),n}}function Wf(e,t){if(!e)throw new Bf(t)}function Vf(e,t){let n=0;for(const s of e)s===t&&n++;return n}function Uf(e){return 1===e.length?e[0]:e}function Gf(e){return Array.isArray(e)?e:[e]}function Hf(e){const t=e.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==t[0]?t:"private"+t}function jf(e){return e.length<=1||-1===e.indexOf("_")?e:e.replace(/[_]+(\w|$)/g,((e,t)=>t.toUpperCase()))}let qf={};function Kf(e){if(null==e)return null;const t={};return t.className=e.getClassName(),t.config=e.getConfig(),t}function Xf(e){if(null!=e&&"object"==typeof e)if(Array.isArray(e))e.forEach((e=>Xf(e)));else{const t=Object.keys(e);for(const n of t){const t=e[n];null!=t&&"object"==typeof t&&(Array.isArray(t)||"ndarray"!==t.type||"number"!=typeof t.value?Xf(t):e[n]=t.value)}}}function Yf(e,t={},n={},s="object",r=!1){if("string"==typeof e){const r=e;let a;if(r in n)a=n[r];else if(r in qf)a=qf[r];else if(a=t[r],null==a)throw new Lf(`Unknown ${s}: ${e}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return a}{const a=e;if(null==a.className||null==a.config)throw new Lf(`${s}: Improper config format: ${JSON.stringify(a)}.\n'className' and 'config' must set.`);const i=a.className;let o,l;if(i in n?[o,l]=n[i]:i in qf?[o,l]=qf.className:i in t&&([o,l]=t[i]),null==o)throw new Lf(`Unknown ${s}: ${i}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const e={};for(const t of Object.keys(qf))e[t]=qf[t];for(const t of Object.keys(n))e[t]=n[t];a.config.customObjects=e;const t=Object.assign({},qf);for(const e of Object.keys(n))qf[e]=n[e];Xf(a.config);const s=l(o,a.config,n,r);return qf=Object.assign({},t),s}{const e=Object.assign({},qf);for(const e of Object.keys(n))qf[e]=n[e];const t=new o(a.config);return qf=Object.assign({},e),t}}}function Zf(e,t){return-1*function(e,t){return e<t?-1:e>t?1:0}(e,t)}function Jf(e){if(null==e)return e;const t=[];for(const n of e)-1===t.indexOf(n)&&t.push(n);return t}function Qf(e){if(null==e)throw new Lf(`Invalid value in obj: ${JSON.stringify(e)}`);for(const t in e)if(e.hasOwnProperty(t))return!1;return!0}function em(e,t,n){if(null!=n&&e.indexOf(n)<0)throw new Lf(`${n} is not a valid ${t}.  Valid values are ${e} or null/undefined.`)}function tm(e,t,n=0,s=1/0){return Wf(n>=0),Wf(s>=n),Array.isArray(e)&&e.length>=n&&e.length<=s&&e.every((e=>typeof e===t))}function nm(e,t){Array.isArray(e)?(l(e.length>0,(()=>`${t} is unexpectedly an empty array.`)),e.forEach(((e,n)=>nm(e,`element ${n+1} of ${t}`)))):l(Number.isInteger(e)&&e>0,(()=>`Expected ${t} to be a positive integer, but got ${sm(e)}.`))}function sm(e){return null===e?"null":Array.isArray(e)?"["+e.map((e=>sm(e))).join(",")+"]":"string"==typeof e?`"${e}"`:`${e}`}function rm(e){return"relu"===e?"relu":"linear"===e?"linear":"elu"===e?"elu":null}function am(e,t){return Xi((()=>Dc(eu(ro(e,e),t,!0))))}class im extends Mi{getConfig(){return{}}}class om extends im{constructor(e){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=e.maxValue?e.maxValue:this.defaultMaxValue,this.axis=null!=e.axis?e.axis:this.defaultAxis}apply(e){return Xi((()=>{const t=am(e,this.axis),n=Ko(t,0,this.maxValue);return ro(e,so(n,to(Df(),t)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}om.className="MaxNorm",zi(om);class lm extends im{constructor(e){super(),this.defaultAxis=0,this.axis=null!=e.axis?e.axis:this.defaultAxis}apply(e){return Xi((()=>so(e,to(Df(),am(e,this.axis)))))}getConfig(){return{axis:this.axis}}}lm.className="UnitNorm",zi(lm);class um extends im{apply(e){return lc(e)}}um.className="NonNeg",zi(um);class cm extends im{constructor(e){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=e.minValue?e.minValue:this.defaultMinValue,this.maxValue=null!=e.maxValue?e.maxValue:this.defaultMaxValue,this.rate=null!=e.rate?e.rate:this.defaultRate,this.axis=null!=e.axis?e.axis:this.defaultAxis}apply(e){return Xi((()=>{const t=am(e,this.axis),n=to(ro(this.rate,Ko(t,this.minValue,this.maxValue)),ro(1-this.rate,t));return ro(e,so(n,to(Df(),t)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}cm.className="MinMaxNorm",zi(cm);const hm={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function pm(e){return Kf(e)}function dm(e,t={}){return Yf(e,Li.getMap().classNameMap,t,"constraint")}function fm(e){if(null==e)return null;if("string"==typeof e){return dm({className:e in hm?hm[e]:e,config:{}})}return e instanceof im?e:dm(e)}var mm=Object.freeze({__proto__:null,maxNorm:function(e){return new om(e)},unitNorm:function(e){return new lm(e)},nonNeg:function(){return new um},minMaxNorm:function(e){return new cm(e)}});const gm=["channelsFirst","channelsLast"],ym=["nearest","bilinear"],bm=["valid","same","causal"],xm=["max","avg"],wm=["sum","mul","concat","ave"],km=new Map;function vm(e){em(gm,"DataFormat",e)}function Nm(e){em(bm,"PaddingMode",e)}function Im(e){em(xm,"PoolMode",e)}const Sm=[];function $m(e,t){Sm.push(e);try{const e=t();return Sm.pop(),e}catch(e){throw Sm.pop(),e}}function Cm(e){if(!Am(e))throw new Error("Not a valid tensor name: '"+e+"'");return(0===Sm.length?"":Sm.join("/")+"/")+e}function Tm(e){if(!Am(e))throw new Error("Not a valid tensor name: '"+e+"'");km.has(e)||km.set(e,0);const t=km.get(e);if(km.set(e,km.get(e)+1),t>0){const n=`${e}_${t}`;return km.set(n,1),n}return e}const Em=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function Am(e){return!!e.match(Em)}function Rm(e,t,n){null==t&&(t=0),null==n&&(n=e.length);let s=1;for(let r=t;r<n;++r)s*=e[r];return s}function Fm(e){if(0===e.length)return Number.NaN;let t=Number.POSITIVE_INFINITY;for(let n=0;n<e.length;n++){const s=e[n];s<t&&(t=s)}return t}function _m(e){if(0===e.length)return Number.NaN;let t=Number.NEGATIVE_INFINITY;for(let n=0;n<e.length;n++){const s=e[n];s>t&&(t=s)}return t}function Dm(e,t){if(t<e)throw new Lf(`end (${t}) < begin (${e}) is forbidden.`);const n=[];for(let s=e;s<t;++s)n.push(s);return n}function Om(e,t){return e.asType(t)}function Mm(e,t=-1){const n=e.shape.slice();return t<0&&(t=n.length+t+1),n.splice(t,0,1),e.reshape(n)}function Lm(e,t,n){return Xi((()=>{switch(e.rank){case 1:return Ic(e,t,n);case 2:return Sc(e,[t,0],[n,e.shape[1]]);case 3:return $c(e,[t,0,0],[n,e.shape[1],e.shape[2]]);case 4:return Cc(e,[t,0,0,0],[n,e.shape[1],e.shape[2],e.shape[3]]);case 5:return Lo(e,[t,0,0,0,0],[n,e.shape[1],e.shape[2],e.shape[3],e.shape[4]]);case 6:return Lo(e,[t,0,0,0,0,0],[n,e.shape[1],e.shape[2],e.shape[3],e.shape[4],e.shape[5]]);default:throw new Lf(`sliceAlongFirstAxis() received an unsupported tensor rank: ${e.rank}`)}}))}function zm(e,t,n){return Xi((()=>{switch(e.rank){case 1:return Ic(e,t,n);case 2:return Sc(e,[0,t],[e.shape[0],n]);case 3:return $c(e,[0,0,t],[e.shape[0],e.shape[1],n]);case 4:return Cc(e,[0,0,0,t],[e.shape[0],e.shape[1],e.shape[2],n]);default:throw new Lf(`sliceAlongLastAxis() received an unsupported tensor rank: ${e.rank}`)}}))}function Bm(e,t,n,s){return Xi((()=>{switch(e.rank){case 1:return Ic(e,t,n);case 2:switch(s){case 1:return Lm(e,t,n);case 2:return zm(e,t,n);default:throw new Lf(`The axis is not within the rank of the tensor ${s}`)}case 3:switch(s){case 1:return Lm(e,t,n);case 2:return $c(e,[0,t,0],[e.shape[0],n,e.shape[2]]);case 3:return zm(e,t,n);default:throw new Lf(`The axis is not within the rank of the tensor ${s}`)}case 4:switch(s){case 1:return Lm(e,t,n);case 2:return Cc(e,[0,t,0,0],[e.shape[0],n,e.shape[2],e.shape[3]]);case 3:return Cc(e,[0,0,t,0],[e.shape[0],e.shape[1],n,e.shape[3]]);case 4:return zm(e,t,n);default:throw new Lf(`The axis is not within the rank of the tensor ${s}`)}default:throw new Lf(`sliceAlongLastAxis() received an unsupported tensor rank: ${e.rank}`)}}))}function Pm(e,t=-1){let n;return t<0&&(n=e[0].rank,t=0!==n?n:0),t===e[0].rank&&(t=-1),Oo(e,t)}function Wm(e,t){switch(e.rank){case 1:return Xo([e,t]);case 2:return Yo([e,t],0);case 3:return Zo([e,t],0);case 4:return Jo([e,t],0);default:throw new Lf(`concatAlongFirstAxis() received an unsupported tensor rank: ${e.rank}`)}}function Vm(e,t){if(Array.isArray(t)||(t=[t]),e.rank!==t.length)throw new Lf(`The length of input n (${t.length}) does not match the number of dimensions in input x (${e.rank})`);return Tl(e,t)}function Um(e,t=0,n=1,s,r){return sc(e,t,n,s,r)}function Gm(e,t,n,s){if(e.rank<2||t.rank<2)throw new zf(`dot requires both inputs to be rank >= 2 but got x shape = ${e.shape} and y shape = ${t.shape}`);if(t.rank>=3){if(e.shape.slice(-1)[0]!==t.shape.slice(-2)[0])throw new zf(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${e.shape} and  y shape = ${t.shape}`)}if(2===e.rank&&2===t.rank){return yh({a:e,b:t,transposeA:!1,transposeB:!1,bias:s?qm(e.rank,s,"channelsLast"):null,activation:n})}{const r=e.shape.slice(),a=r.pop();e=e.reshape([-1,a]);const i=t.shape.slice(),o=i.pop(),l=i.pop(),u=[...i,o],c=Array.from({length:t.rank},((e,n)=>0===n?t.rank-2:n<=t.rank-2?n-1:n));t=t.transpose(c).reshape([l,-1]);const h=[...r,...u];return yh({a:e,b:t,transposeA:!1,transposeB:!1,bias:s?qm(e.rank,s,"channelsLast"):null,activation:n}).reshape(h)}}function Hm(e,t,n){return Xi((()=>(t=Array.isArray(t)?Wc(t,"int32"):t.toInt(),Fl(e,t,n))))}function jm(e){return ro(e,e)}function qm(e,t,n){const s=t.shape;if(1!==t.rank&&t.rank!==e)throw new Lf(`Unexpected bias dimensions: ${t.rank}; expected it to be 1 or ${e}`);if(5===e){if("channelsFirst"===n)return 1===s.length?t.reshape([1,s[0],1,1,1]):t.reshape([1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?t.reshape([1,1,1,1,s[0]]):t.reshape([1].concat(s))}else if(4===e){if("channelsFirst"===n)return 1===s.length?t.reshape([1,s[0],1,1]):t.reshape([1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?t.reshape([1,1,1,s[0]]):t.reshape([1].concat(s))}else if(3===e){if("channelsFirst"===n)return 1===s.length?t.reshape([1,s[0],1]):t.reshape([1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?t.reshape([1,1,s[0]]):t.reshape([1].concat(s))}else if(e<3)return t;throw new Lf(`Unsupported input rank by biasAdd: ${t.rank}`)}function Km(e,t,n){return Xi((()=>(null==n&&(n="channelsLast"),vm(n),e.add(qm(e.rank,t,n)))))}function Xm(e,t,n,s){return Xi((()=>rh(e,t,n,s)))}function Ym(e,t,n=!1){return n?e():t()}const Zm=["fanIn","fanOut","fanAvg"],Jm=["normal","uniform","truncatedNormal"];class Qm extends Mi{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class eg extends Qm{apply(e,t){return wu(e,t)}}eg.className="Zeros",zi(eg);class tg extends Qm{apply(e,t){return ku(e,t)}}tg.className="Ones",zi(tg);class ng extends Qm{constructor(e){if(super(),"object"!=typeof e)throw new Lf(`Expected argument of type ConstantConfig but got ${e}`);if(void 0===e.value)throw new Lf(`config must have value set but got ${e}`);this.value=e.value}apply(e,t){return Xi((()=>ro(yc(this.value),ku(e,t))))}getConfig(){return{value:this.value}}}ng.className="Constant",zi(ng);class sg extends Qm{constructor(e){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=e.minval||this.DEFAULT_MINVAL,this.maxval=e.maxval||this.DEFAULT_MAXVAL,this.seed=e.seed}apply(e,t){return rc(e,this.minval,this.maxval,t)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}sg.className="RandomUniform",zi(sg);class rg extends Qm{constructor(e){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=e.mean||this.DEFAULT_MEAN,this.stddev=e.stddev||this.DEFAULT_STDDEV,this.seed=e.seed}apply(e,t){if("float32"!==(t=t||"float32")&&"int32"!==t)throw new zf(`randomNormal does not support dType ${t}.`);return Um(e,this.mean,this.stddev,t,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}rg.className="RandomNormal",zi(rg);class ag extends Qm{constructor(e){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=e.mean||this.DEFAULT_MEAN,this.stddev=e.stddev||this.DEFAULT_STDDEV,this.seed=e.seed}apply(e,t){if("float32"!==(t=t||"float32")&&"int32"!==t)throw new zf(`truncatedNormal does not support dType ${t}.`);return Gc(e,this.mean,this.stddev,t,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}ag.className="TruncatedNormal",zi(ag);class ig extends Qm{constructor(e){super(),this.gain=null!=e.gain?e.gain:1}apply(e,t){return Xi((()=>{if(2!==e.length||e[0]!==e[1])throw new Lf("Identity matrix initializer can only be used for 2D square matrices.");return ro(this.gain,El(e[0]))}))}getConfig(){return{gain:this.gain}}}ig.className="Identity",zi(ig);class og extends Qm{constructor(e){if(super(),e.scale<0)throw new Lf(`scale must be a positive float. Got: ${e.scale}`);var t;this.scale=null==e.scale?1:e.scale,this.mode=null==e.mode?"fanIn":e.mode,t=this.mode,em(Zm,"FanMode",t),this.distribution=null==e.distribution?"normal":e.distribution,function(e){em(Jm,"Distribution",e)}(this.distribution),this.seed=e.seed}apply(e,t){const n=function(e,t="channelsLast"){let n,s;if(vm(t),2===e.length)n=e[0],s=e[1];else if(-1!==[3,4,5].indexOf(e.length)){if("channelsFirst"===t){const t=Rm(e,2);n=e[1]*t,s=e[0]*t}else if("channelsLast"===t){const t=Rm(e,0,e.length-2);n=e[e.length-2]*t,s=e[e.length-1]*t}}else{const t=Rm(e);n=Math.sqrt(t),s=Math.sqrt(t)}return[n,s]}(e),s=n[0],r=n[1];let a=this.scale;if("fanIn"===this.mode?a/=Math.max(1,s):"fanOut"===this.mode?a/=Math.max(1,r):a/=Math.max(1,(s+r)/2),"normal"===this.distribution){const n=Math.sqrt(a);if("float32"!==(t=t||"float32")&&"int32"!==t)throw new zf(`${this.getClassName()} does not support dType ${t}.`);return Gc(e,0,n,t,this.seed)}{const n=Math.sqrt(3*a);return rc(e,-n,n,t)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}og.className="VarianceScaling",zi(og);class lg extends og{constructor(e){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==e?null:e.seed})}getClassName(){return og.className}}lg.className="GlorotUniform",zi(lg);class ug extends og{constructor(e){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==e?null:e.seed})}getClassName(){return og.className}}ug.className="GlorotNormal",zi(ug);class cg extends og{constructor(e){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==e?null:e.seed})}getClassName(){return og.className}}cg.className="HeNormal",zi(cg);class hg extends og{constructor(e){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==e?null:e.seed})}getClassName(){return og.className}}hg.className="HeUniform",zi(hg);class pg extends og{constructor(e){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==e?null:e.seed})}getClassName(){return og.className}}pg.className="LeCunNormal",zi(pg);class dg extends og{constructor(e){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==e?null:e.seed})}getClassName(){return og.className}}dg.className="LeCunNormal",zi(dg);class fg extends Qm{constructor(e){if(super(),this.DEFAULT_GAIN=1,this.gain=null==e.gain?this.DEFAULT_GAIN:e.gain,this.seed=e.seed,null!=this.seed)throw new zf("Random seed is not implemented for Orthogonal Initializer yet.")}apply(e,t){return Xi((()=>{if(e.length<2)throw new zf("Shape must be at least 2D.");e[0]*e[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${e[0]*e[1]}) elements: Slowness may result.`);const t=Um(e[0]>e[1]?[e[1],e[0]]:e,0,1,"float32");let n=gp.gramSchmidt(t);return e[0]>e[1]&&(n=n.transpose()),ro(this.gain,n)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}fg.className="Orthogonal",zi(fg);const mg={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function gg(e,t={}){return Yf(e,Li.getMap().classNameMap,t,"initializer")}function yg(e){return Kf(e)}function bg(e){if("string"==typeof e){const t=e in mg?mg[e]:e;if("GlorotNormal"===t)return new ug;if("GlorotUniform"===t)return new lg;if("HeNormal"===t)return new cg;if("HeUniform"===t)return new hg;if("LeCunNormal"===t)return new pg;if("LeCunUniform"===t)return new dg;{const e={};return e.className=t,e.config={},gg(e)}}return e instanceof Qm?e:gg(e)}var xg=Object.freeze({__proto__:null,zeros:function(){return new eg},ones:function(){return new tg},constant:function(e){return new ng(e)},randomUniform:function(e){return new sg(e)},randomNormal:function(e){return new rg(e)},truncatedNormal:function(e){return new ag(e)},identity:function(e){return new ig(e)},varianceScaling:function(e){return new og(e)},glorotUniform:function(e){return new lg(e)},glorotNormal:function(e){return new ug(e)},heNormal:function(e){return new cg(e)},heUniform:function(e){return new hg(e)},leCunNormal:function(e){return new pg(e)},leCunUniform:function(e){return new dg(e)},orthogonal:function(e){return new fg(e)}});let wg=0;function kg(){return wg++}const vg={};function Ng(e=""){return e in vg||(vg[e]=0),vg[e]+=1,e+vg[e].toString()}function Ig(e){return Array.isArray(e)&&Array.isArray(e[0])}function Sg(e){return 0===e.length?[]:Array.isArray(e[0])?e:[e]}function $g(e){let t;if(Array.isArray(e)){if(1!==e.length)throw new Lf(`Expected Tensor length to be 1; got ${e.length}`);t=e[0]}else t=e;return t}function Cg(e){if(Array.isArray(e)&&Array.isArray(e[0])){if(1===e.length)return(e=e)[0];throw new Lf(`Expected exactly 1 Shape; got ${e.length}`)}return e}function Tg(e){let t=0;for(const n of e)0===n.shape.length?t+=1:t+=n.shape.reduce(((e,t)=>e*t));return t}const Eg="Variable";class Ag{constructor(e,t="float32",n="Variable",s=!0,r=null){this.dtype=null==t?"float32":t,this.shape=e.shape,this.id=kg(),n=null==n?Eg:n,this.originalName=Cm(n),this.name=Tm(this.originalName),this.trainable_=s,this.constraint=r,this.val=Kc(e,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(e){return this.assertNotDisposed(),function(e,t){if(e.shape.toString()!==t.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(e.shape)+" vs. "+JSON.stringify(t.shape))}(this.val,e),this.val.id!==e.id&&(this.val.assign(e),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(e){this.trainable_=e,this.val.trainable=e}}function Rg(e){return e.map((e=>e.read()))}function Fg(e){e.forEach((e=>{e[0].write(e[1])}))}class _g{constructor(e){this.dtype=e.dtype,this.shape=e.shape,null!=e.shape?this.ndim=e.shape.length:this.ndim=e.ndim,this.maxNDim=e.maxNDim,this.minNDim=e.minNDim,this.axes=e.axes||{}}}class Dg{constructor(e,t,n,s,r,a,i){this.dtype=e,this.shape=t,this.sourceLayer=n,this.inputs=s,this.callArgs=r,this.outputTensorIndex=i,this.id=kg(),null!=a&&(this.originalName=Cm(a),this.name=Tm(this.originalName)),this.rank=t.length}}let Og=0;class Mg{constructor(e,t){this.callArgs=t,this.id=Og++,this.outboundLayer=e.outboundLayer,this.inboundLayers=e.inboundLayers,this.nodeIndices=e.nodeIndices,this.tensorIndices=e.tensorIndices,this.inputTensors=e.inputTensors,this.outputTensors=e.outputTensors,this.inputMasks=e.inputMasks,this.outputMasks=e.outputMasks,this.inputShapes=e.inputShapes,this.outputShapes=e.outputShapes;for(const t of e.inboundLayers)null!=t&&t.outboundNodes.push(this);e.outboundLayer.inboundNodes.push(this)}getConfig(){const e=[];for(const t of this.inboundLayers)null!=t?e.push(t.name):e.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:e,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let Lg=0;class zg extends Mi{constructor(e={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=Lg++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let t=e.name;if(!t){const e=this.getClassName();t=Hf(e)+"_"+Ng(e)}if(this.name=t,this.trainable_=null==e.trainable||e.trainable,null!=e.inputShape||null!=e.batchInputShape){let t;if(null!=e.batchInputShape)t=e.batchInputShape;else if(null!=e.inputShape){let n=null;null!=e.batchSize&&(n=e.batchSize),t=[n].concat(e.inputShape)}this.batchInputShape=t;let n=e.dtype;null==n&&(n=e.inputDType),null==n&&(n="float32"),this.dtype=n}null!=e.weights?this.initialWeights=e.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(e,t){return e.name+"_ib-"+t.toString()}getNodeAtIndex(e,t){if(0===this.inboundNodes.length)throw new Mf(`The layer has never been called and thus has no defined ${t}.`);if(this.inboundNodes.length<=e)throw new Lf(`Asked to get ${t} at node ${e}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[e]}getInputAt(e){return Uf(this.getNodeAtIndex(e,"input").inputTensors)}getOutputAt(e){return Uf(this.getNodeAtIndex(e,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new Of(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new Of(`Layer ${this.name} is not connected, no input to return.`);return Uf(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new Of(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new Of(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return Uf(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((e=>e()))}get updates(){return this._updates}get built(){return this._built}set built(e){this._built=e}get trainable(){return this.trainable_}set trainable(e){this._trainableWeights.forEach((t=>t.trainable=e)),this.trainable_=e}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((e=>e.trainable)):[]}set trainableWeights(e){this._trainableWeights=e}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((e=>!e.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(e){this._nonTrainableWeights=e}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(e){if(e=Gf(e),null==this.inputSpec||0===this.inputSpec.length)return;const t=Gf(this.inputSpec);if(e.length!==t.length)throw new Lf(`Layer ${this.name} expects ${t.length} inputs, but it received ${e.length} input tensors. Input received: ${e}`);for(let n=0;n<e.length;n++){const s=e[n],r=t[n];if(null==r)continue;const a=s.rank;if(null!=r.ndim&&a!==r.ndim)throw new Lf(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${r.ndim}, found ndim=${a}`);if(null!=r.maxNDim&&a>r.maxNDim)throw new Lf(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${r.maxNDim}, found ndim=${a}`);if(null!=r.minNDim&&a<r.minNDim)throw new Lf(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${r.minNDim}, found ndim=${a}.`);if(null!=r.dtype&&s.dtype!==r.dtype)throw new Lf(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${r.dtype}, found dtype=${s.dtype}.`);if(r.axes){const e=s.shape;for(const t in r.axes){const s=Number(t),a=r.axes[t],i=s>=0?e[s]:e[e.length+s];if(null!=a&&-1===[a,null].indexOf(i))throw new Lf(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${a} but got shape ${e}.`)}}if(null!=r.shape)for(let e=0;e<r.shape.length;++e){const t=r.shape[e],a=s.shape[e];if(null!=t&&null!=a&&t!==a)throw new Lf(`Input ${n} is incompatible with layer ${this.name}: expected shape=${r.shape}, found shape=${s.shape}.`)}}}call(e,t){return e}invokeCallHook(e,t){null!=this._callHook&&this._callHook(e,t)}setCallHook(e){this._callHook=e}clearCallHook(){this._callHook=null}apply(e,t){t=t||{},this.assertNotDisposed();const n=Gf(e);let s=!0;for(const e of n)if(!(e instanceof Dg)){s=!1;break}let r=!0;for(const e of n)if(e instanceof Dg){r=!1;break}if(s===r)throw new Lf("Arguments to apply() must be all SymbolicTensors or all Tensors");return $m(this.name,(()=>{if(!this.built){this.assertInputCompatibility(e);const t=[];for(const n of Gf(e))t.push(n.shape);this.build(Uf(t)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&r&&(this._refCount=1)}if(this.assertInputCompatibility(e),r){let s=this.call(e,t);const r=Gf(s),a=[];for(let e of r)-1!==n.indexOf(e)&&(e=e.clone()),a.push(e);if(s=Uf(a),null!=this.activityRegularizer)throw new zf("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(e){e=Gf(e);const t=[];for(const n of e)t.push(n.shape);return Uf(t)}(e),s=this.computeOutputShape(n);let r;const a="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(e)?n[0]:n),r=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new Dg(a,n,this,Gf(e),t,this.name,s))):new Dg(a,s,this,Gf(e),t,this.name),this.addInboundNode(e,r,null,null,n,s,t),this._refCount++,null!=this.activityRegularizer)throw new zf("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return r}}))}warnOnIncompatibleInputShape(e){if(null!=this.batchInputShape)if(e.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(e)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let t=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=e[s]&&e[s]!==n&&(t=!0)})),t&&console.warn(`The shape of the input tensor (${JSON.stringify(e)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new Of(`The layer ${this.name} has never been called and thus has no defined output shape.`);const e=[];for(const t of this.inboundNodes){const n=JSON.stringify(t.outputShapes);-1===e.indexOf(n)&&e.push(n)}if(1===e.length){const e=this.inboundNodes[0].outputShapes;return Array.isArray(e)&&Array.isArray(e[0])&&1===e.length?e[0]:e}throw new Of(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new Mf(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return Tg(this.weights)}build(e){this.built=!0}getWeights(e=!1){return Rg(e?this.trainableWeights:this.weights)}setWeights(e){Xi((()=>{const t=this.weights;if(t.length!==e.length)throw new Lf(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${e.length}, but the layer was expecting ${t.length} weights. Provided weights: ${e}...`);if(0===t.length)return;const n=[],s=Rg(t);for(let r=0;r<s.length;++r){const a=s[r],i=t[r],o=e[r];if(!d(a.shape,o.shape))throw new Lf(`Layer weight shape ${a.shape} not compatible with provided weight shape ${o.shape}`);n.push([i,o])}Fg(n)}))}addWeight(e,t,n,s,r,a,i){if(-1!==this._addedWeightNames.indexOf(e))throw new Lf(`Duplicate weight name ${e} for layer ${this.name}`);this._addedWeightNames.push(e),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=bg("zeros"));const o=s.apply(t,n),l=new Ag(o,n,e,a,i);return o.dispose(),null!=r&&this.addLoss((()=>r.apply(l.read()))),null==a&&(a=!0),a?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(e){this.fastWeightInitDuringBuild=e}addLoss(e){null==e||Array.isArray(e)&&0===e.length||(e=Gf(e),void 0!==this._losses&&null!==this._losses&&this.losses.push(...e))}computeOutputShape(e){return e}computeMask(e,t){if(!this.supportsMasking){if(null!=t){if(!Array.isArray(t))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);t.forEach((e=>{if(null!=e)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return t}addInboundNode(e,t,n,s,r,a,i=null){const o=Gf(e);t=Gf(t),n=Gf(n),s=Gf(s),r=Sg(r),a=Sg(a);const l=[],u=[],c=[];for(const e of o)l.push(e.sourceLayer),u.push(e.nodeIndex),c.push(e.tensorIndex);new Mg({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:c,inputTensors:o,outputTensors:t,inputMasks:n,outputMasks:s,inputShapes:r,outputShapes:a},i);for(let e=0;e<t.length;e++)t[e].sourceLayer=this,t[e].nodeIndex=this.inboundNodes.length-1,t[e].tensorIndex=e}getConfig(){const e={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(e.batchInputShape=this.batchInputShape),null!=this.dtype&&(e.dtype=this.dtype),e}disposeWeights(){return this.weights.forEach((e=>e.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let e=0;return 0==--this._refCount&&(e=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:e}}}function Bg(e,t,n){if((null==t||null!=n&&n>0)&&(t=e.sourceLayer,n=e.nodeIndex),0===t.inboundNodes.length)return[e];{const e=t.inboundNodes[n];if(0===e.inboundLayers.length)return e.inputTensors;{const t=[];for(let n=0;n<e.inboundLayers.length;n++){const s=Bg(e.inputTensors[n],e.inboundLayers[n],e.nodeIndices[n]);for(const e of s)-1===t.indexOf(e)&&t.push(e)}return t}}}class Pg extends zg{constructor(e){if(super({dtype:e.dtype,name:null!=e.name?e.name:Ng("input").toString()}),null==e.batchSize&&(e.batchSize=null),null==e.sparse&&(e.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=e.sparse,null!=e.inputShape&&null!=e.batchInputShape)throw new Lf("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let t=e.batchInputShape;if(null==t){if(null==e.inputShape)throw new Lf("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");t=[e.batchSize].concat(e.inputShape)}else if(null!=e.batchSize)throw new Lf("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=e.dtype||"float32";this.batchInputShape=t,this.dtype=n,this.inputSpec=[{shape:t}];const s=new Dg(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new Mg({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[t],outputShapes:[t]})}apply(e,t){throw new Lf(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function Wg(e){if(null==e.batchShape&&null==e.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=e.batchShape&&null!=e.shape)throw new Lf("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let t=e.batchShape;null!=e.shape&&null==t&&(t=[null].concat(e.shape));let n=e.dtype;null==n&&(n="float32");return new Pg({batchInputShape:t,name:e.name,dtype:n,sparse:e.sparse}).inboundNodes[0].outputTensors[0]}async function Vg(e){if(null==e)return;const t=[],n=[],s=[];for(const r in e){const a=e[r];if("number"!=typeof a){const e=a;t.push(e.data()),n.push(r),s.push(e)}}if(t.length>0){const r=await Promise.all(t);for(let t=0;t<r.length;++t)e[n[t]]=r[t][0];Yi(s)}}function Ug(e){if(null!=e)for(const t in e){const n=e[t];"number"!=typeof n&&n.dispose()}}var Gg;Pg.className="InputLayer",zi(Pg),function(e){e[e.SILENT=0]="SILENT",e[e.VERBOSE=1]="VERBOSE"}(Gg||(Gg={}));class Hg{constructor(){this.validationData=null}setParams(e){this.params=e}async onEpochBegin(e,t){}async onEpochEnd(e,t){}async onBatchBegin(e,t){}async onBatchEnd(e,t){}async onTrainBegin(e){}async onTrainEnd(e){}setModel(e){}}class jg{constructor(e,t=10){null==e&&(e=[]),this.callbacks=e,this.queueLength=t}append(e){this.callbacks.push(e)}setParams(e){for(const t of this.callbacks)t.setParams(e)}setModel(e){for(const t of this.callbacks)t.setModel(e)}async onEpochBegin(e,t){null==t&&(t={});for(const n of this.callbacks)await n.onEpochBegin(e,t)}async onEpochEnd(e,t){null==t&&(t={});for(const n of this.callbacks)await n.onEpochEnd(e,t)}async onBatchBegin(e,t){null==t&&(t={});for(const n of this.callbacks)await n.onBatchBegin(e,t)}async onBatchEnd(e,t){null==t&&(t={});for(const n of this.callbacks)await n.onBatchEnd(e,t)}async onTrainBegin(e){null==e&&(e={});for(const t of this.callbacks)await t.onTrainBegin(e)}async onTrainEnd(e){null==e&&(e={});for(const t of this.callbacks)await t.onTrainEnd(e)}}class qg extends Hg{constructor(){super()}async onEpochBegin(e){this.seen=0,this.totals={}}async onBatchEnd(e,t){null==t&&(t={});const n=null==t.size?0:t.size;this.seen+=n;for(const e in t){const s=t[e];if("number"==typeof s)this.totals.hasOwnProperty(e)||(this.totals[e]=0),this.totals[e]=this.totals[e]+s*n;else{let t;e in this.totals?t=this.totals[e]:this.totals[e]=0;const r=Xi((()=>to(this.totals[e],ro(s,n))));this.totals[e]=r,null!=t&&t.dispose()}}}async onEpochEnd(e,t){if(null!=t)for(const e of this.params.metrics)null!=this.totals[e]&&("number"==typeof this.totals[e]?t[e]=this.totals[e]/this.seen:Xi((()=>{const n=ro(so(1,this.seen),this.totals[e]);t[e]=n,this.totals[e].dispose(),Zi(t[e])})))}}class Kg extends Hg{async onTrainBegin(e){this.epoch=[],this.history={}}async onEpochEnd(e,t){null==t&&(t={}),this.epoch.push(e);for(const e in t)null==this.history[e]&&(this.history[e]=[]),this.history[e].push(t[e])}async syncData(){const e=[],t=[],n=[];for(const s in this.history){const r=this.history[s];for(let a=0;a<r.length;++a)if("number"!=typeof r[a]){const i=r[a];e.push(i.data()),t.push(s),n.push(a)}}const s=await Promise.all(e);for(let e=0;e<s.length;++e){this.history[t[e]][n[e]].dispose(),this.history[t[e]][n[e]]=s[e][0]}}}class Xg extends Hg{constructor(e,t){if(super(),this.currentEpoch=0,this.yieldEvery=t||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=e.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");R(this.yieldEvery)&&(this.maybeWait=function(e,t){let n,s=Ys();return(...r)=>{const a=Ys();return a-s<t||(s=a,n=e(...r)),n}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=e.onTrainBegin,this.trainEnd=e.onTrainEnd,this.epochBegin=e.onEpochBegin,this.epochEnd=e.onEpochEnd,this.batchBegin=e.onBatchBegin,this.batchEnd=e.onBatchEnd,this.yield=e.onYield}async maybeWait(e,t,n){const s=[];null!=this.yield&&(await Vg(n),s.push(this.yield(e,t,n))),s.push(Rp()),await Promise.all(s)}async onEpochBegin(e,t){this.currentEpoch=e,null!=this.epochBegin&&(await Vg(t),await this.epochBegin(e,t))}async onEpochEnd(e,t){const n=[];null!=this.epochEnd&&(await Vg(t),n.push(this.epochEnd(e,t))),"epoch"===this.yieldEvery&&n.push(Rp()),await Promise.all(n)}async onBatchBegin(e,t){null!=this.batchBegin&&(await Vg(t),await this.batchBegin(e,t))}async onBatchEnd(e,t){const n=[];null!=this.batchEnd&&(await Vg(t),n.push(this.batchEnd(e,t))),"batch"===this.yieldEvery?n.push(Rp()):R(this.yieldEvery)&&n.push(this.maybeWait(this.currentEpoch,e,t)),await Promise.all(n)}async onTrainBegin(e){null!=this.trainBegin&&(await Vg(e),await this.trainBegin(e))}async onTrainEnd(e){null!=this.trainEnd&&(await Vg(e),await this.trainEnd(e))}}function Yg(e,t){if(null==e&&(e={}),e instanceof Hg)return[e];if(Array.isArray(e)&&e[0]instanceof Hg)return e;return Gf(e).map((e=>new Xg(e,t)))}class Zg{constructor(){}static registerCallbackConstructor(e,t){l(e>=0&&Number.isInteger(e),(()=>`Verbosity level is expected to be an integer >= 0, but got ${e}`)),Zg.checkForDuplicate(t),null==Zg.constructors[e]&&(Zg.constructors[e]=[]),Zg.constructors[e].push(t)}static checkForDuplicate(e){for(const t in Zg.constructors){Zg.constructors[+t].forEach((t=>{if(t===e)throw new Lf("Duplicate callback constructor.")}))}}static clear(){Zg.constructors={}}static createCallbacks(e){const t=[];for(const n in Zg.constructors){const s=+n;e>=s&&t.push(...Zg.constructors[s])}return t.map((e=>new e))}}function Jg(e,t,n,s,r,a,i,o,l){const u=new Kg,c=[new qg,...Zg.createCallbacks(t)];null!=e&&c.push(...e),c.push(u);const h=new jg(c);return h.setParams({epochs:n,initialEpoch:s,samples:r,steps:a,batchSize:i,verbose:t,doValidation:o,metrics:l}),{callbackList:h,history:u}}function Qg(e,t={},n=!1){return Yf(e,Li.getMap().classNameMap,t,"layer",n)}function ey(e,t){return Xi((()=>{"float32"!==e.dtype&&(e=e.asType("float32"));const n=eu(jm(e),t,!0),s=Al(n.shape,Df()),r=Dc(bu(n,s));return so(e,r)}))}function ty(e,t){return Xi((()=>xu(jm(Ql(t,e)),-1)))}function ny(e,t){return Xi((()=>xu(ao(Ql(t,e)),-1)))}function sy(e,t){return Xi((()=>{const n=Ql(e,t),s=Ko(ao(e),Df(),Number.MAX_VALUE),r=ao(so(n,s));return ro(100,xu(r,-1))}))}function ry(e,t){return Xi((()=>{const n=Ko(t,Df(),Number.MAX_VALUE),s=Gl(to(1,n)),r=Ko(e,Df(),Number.MAX_VALUE),a=Gl(to(1,r));return xu(jm(Ql(s,a)),-1)}))}function ay(e,t,n=!1){return Xi((()=>{if(n)t=Tc(t);else{const e=eu(t,t.shape.length-1,!0);t=so(t,e)}return t=Ko(t,Df(),1-Df()),Xl(eu(ro(e.toFloat(),Gl(t)),t.shape.length-1))}))}function iy(e,t,n=!1){return Xi((()=>{const s=Rl(function(e){const t=[Rm(e.shape)];return e.reshape(t)}(e)).toInt(),r=(t=Ko(t,Df(),1-Df())).shape;return ay(si(s,r[r.length-1]).reshape(r),t,n)}))}function oy(e,t){return Xi((()=>{let n;return n=Ko(t,Df(),1-Df()),n=Gl(so(n,Ql(1,n))),xu(function(e,t){if(!d(e.shape,t.shape))throw new Lf(`logits and labels must have the same shape, but got shapes ${JSON.stringify(e.shape)} and ${JSON.stringify(t.shape)}`);return Xi((()=>{const n=t.relu(),s=t.abs().neg();return n.sub(t.mul(e)).add(s.exp().log1p())}))}(e,n),-1)}))}function ly(e,t){return Xi((()=>{const n=Ko(e,Df(),1),s=Ko(t,Df(),1);return eu(ro(e,Gl(so(n,s))),-1)}))}function uy(e,t){return Xi((()=>{const n=ey(e,-1),s=ey(t,-1),r=ro(n,s);return Xl(eu(r,-1))}))}Zg.constructors={};const cy={meanSquaredError:ty,meanAbsoluteError:ny,meanAbsolutePercentageError:sy,meanSquaredLogarithmicError:ry,squaredHinge:function(e,t){return Xi((()=>{const n=bu(0,Ql(1,ro(e,t)));return xu(jm(n),-1)}))},hinge:function(e,t){return Xi((()=>{const n=bu(0,Ql(1,ro(e,t)));return xu(n,-1)}))},categoricalHinge:function(e,t){return Xi((()=>{const n=eu(ro(e,t),-1),s=Jl(ro(Ql(1,e),t),-1);return bu(0,to(1,Ql(s,n)))}))},logcosh:function(e,t){return Xi((()=>{const n=Math.log(2),s=Ql(t,e),r=Ql(to(s,Yl(ro(-2,s))),n);return xu(r,-1)}))},categoricalCrossentropy:ay,sparseCategoricalCrossentropy:iy,binaryCrossentropy:oy,kullbackLeiblerDivergence:ly,poisson:function(e,t){return Xi((()=>{const n=Gl(to(Df(),t));return xu(Ql(t,ro(e,n)),-1)}))},cosineProximity:uy};function hy(e){if("string"==typeof e){if(e in cy)return cy[e];let t=`Unknown loss ${e}`;throw e.toLowerCase().includes("softmaxcrossentropy")&&(t=`Unknown loss ${e}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new Lf(t)}return e}function py(e,t){return Xi((()=>{const n=ro(.5,Ru(t)),s=Om(_l(t,n),e.dtype);return xu(yl(e,s),-1)}))}function dy(e,t){return Xi((()=>Om(yl(ho(e,-1),ho(t,-1)),"float32")))}function fy(e,t){return Xi((()=>hu(e.equal(1),t.equal(1)).sum().cast("float32")))}function my(e,t){return Xi((()=>{const n=fy(e,t),s=function(e,t){return Xi((()=>hu(e.equal(0),t.equal(1)).sum().cast("float32")))}(e,t),r=n.add(s);return bl(_l(r,0),n.div(r),0).cast("float32")}))}function gy(e,t){return Xi((()=>{const n=fy(e,t),s=function(e,t){return Xi((()=>hu(e.equal(1),t.equal(0)).sum().cast("float32")))}(e,t),r=n.add(s);return bl(_l(r,0),n.div(r),0).cast("float32")}))}function yy(e,t){return oy(e,t)}function by(e,t){return e.rank===t.rank&&(e=e.squeeze([e.rank-1])),(t=t.argMax(-1)).dtype!==e.dtype&&(t=t.asType(e.dtype)),yl(e,t).asType("float32")}const xy=ay,wy=iy,ky={binaryAccuracy:py,categoricalAccuracy:dy,precision:my,categoricalCrossentropy:xy,sparseCategoricalCrossentropy:wy,mse:ty,MSE:ty,mae:ny,MAE:ny,mape:sy,MAPE:sy,cosine:uy};function vy(e){if("string"==typeof e&&e in ky)return ky[e];if("string"!=typeof e&&null!=e)return e;throw new Lf(`Unknown metric ${e}`)}function Ny(e){if(Wf(null!==e,`Unknown LossOrMetricFn ${e}`),"string"==typeof e)return e;{let t;for(const n of Object.keys(cy))if(cy[n]===e){t=n;break}if(void 0!==t)return t;for(const n of Object.keys(ky))if(ky[n]===e){t=n;break}return void 0!==t?t:e.name}}const Iy=1048576;function Sy(e,t,n=!1){if(null==e||"object"!=typeof e||Object.getPrototypeOf(e)!==Object.prototype||!$y(e))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(e);n.length>Iy&&console.warn(`User-defined metadata of model "${t}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function $y(e){if(null===e)return!0;if("object"==typeof e){if(Object.getPrototypeOf(e)===Object.prototype){const t=Object.keys(e);for(const n of t){if("string"!=typeof n)return!1;if(!$y(e[n]))return!1}return!0}if(Array.isArray(e)){for(const t of e)if(!$y(t))return!1;return!0}return!1}{const t=typeof e;return"string"===t||"number"===t||"boolean"===t}}function Cy(e,t,n,s=console.log){const r=function(e){let t=!0;const n=[],s=[];for(const t in e.nodesByDepth)n.push(e.nodesByDepth[t]);for(const e of n){if(e.length>1||1===e.length&&e[0].inboundLayers.length>1){t=!1;break}s.push(...e)}if(t)for(const n of e.layers){let e=!1;for(const r of n.inboundNodes)if(-1!==s.indexOf(r)){if(e){t=!1;break}e=!0}if(!t)break}return t}(e),a=["Layer (type)","Output shape","Param #"];let i;if(r?(t=t||65,n=n||[.45,.85,1]):(t=t||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((e=>Math.floor(t*e)))),!r){a.push("Receives inputs"),i=[];for(const t in e.nodesByDepth)i.push(...e.nodesByDepth[t])}s("_".repeat(t)),Ty(a,n,s),s("=".repeat(t));const o=e.layers;for(let e=0;e<o.length;++e)r?Ey(o[e],n,s):Ay(o[e],n,i,s),s((e===o.length-1?"=":"_").repeat(t));e.checkTrainableWeightsConsistency();const l=function(e){let t;t=null!=e.collectedTrainableWeights?Tg(e.collectedTrainableWeights):Tg(e.trainableWeights);return t}(e),u=Tg(e.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(t))}function Ty(e,t,n=console.log){let s="";for(let n=0;n<e.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=e[n],s=s.slice(0,t[n]),s+=" ".repeat(t[n]-s.length);n(s)}function Ey(e,t,n){let s;try{s=JSON.stringify(e.outputShape)}catch(e){s="multiple"}Ty([`${e.name} (${e.getClassName()})`,s,e.countParams().toString()],t,n)}function Ay(e,t,n,s){let r;try{r=JSON.stringify(e.outputShape)}catch(e){r="multiple"}const a=[];for(const t of e.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(t)))for(let e=0;e<t.inboundLayers.length;++e){const n=t.inboundLayers[e].name,s=t.nodeIndices[e],r=t.tensorIndices[e];a.push(`${n}[${s}][${r}]`)}const i=e.name,o=e.getClassName(),l=0===a.length?"":a[0];Ty([`${i} (${o})`,r,e.countParams().toString(),l],t,s);for(let e=1;e<a.length;++e)Ty(["","","",a[e]],t,s)}function Ry(e,t,n){return("inboundNodes"===e||"outputLayers"===e||"inputLayers"===e)&&0===t&&"string"==typeof n}function Fy(e,t){if(null===e)return null;if("string"==typeof e)return jf(e);if("number"==typeof e||"boolean"==typeof e)return e;if(e instanceof Array){const n=[],s=e.length;for(let r=0;r<s;++r){const s=e[r];Ry(t,r,s)?n.push(s):n.push(Fy(s,t))}return n}{const t={};for(const n of Object.keys(e)){const s=e[n];if("name"===n&&"string"==typeof s)t[n]=s;else{const e=jf(n);t[e]=Fy(s,e)}}return t}}function _y(e,t){if(null==e)return null;if("string"==typeof e)return Hf(e);if("number"==typeof e||"boolean"==typeof e)return e;if(e instanceof Array){const n=[],s=e.length;for(let r=0;r<s;++r){const s=e[r];Ry(t,r,s)?n.push(s):n.push(_y(s,t))}return n}{const t={};for(const n of Object.keys(e)){const s=e[n],r=Hf(n);t[r]="name"!==n&&"className"!==n||"string"!=typeof s?_y(s,n):s}return t}}const Dy="3.7.0";class Oy{constructor(e){if(this.id2Value={},this.id2Mask={},this.name2Id={},e instanceof Oy)for(const t in e.id2Value)this.id2Value[t]=e.id2Value[t],t in e.id2Mask&&(this.id2Mask[t]=e.id2Mask[t]);else{if(null==e)return;for(const t of e)this.add(t.key,t.value)}}add(e,t,n){if(null!=this.id2Value[e.id])throw new Lf(`Duplicate key: name=${e.name}, id=${e.id}`);return this.id2Value[e.id]=function(e,t){if(null==e.dtype||e.dtype===t.dtype)return t;try{return za(t,e.dtype)}catch(n){throw new Lf(`The dtype of the feed (${t.dtype}) can not be cast to the dtype of the key '${e.name}' (${e.dtype}).`)}}(e,t),this.name2Id[e.name]=e.id,null!=n&&(this.id2Mask[e.id]=n),this}addFeed(e){this.add(e.key,e.value)}hasKey(e){return null!=this.id2Value[e.id]}names(){return Object.keys(this.name2Id)}getValue(e){if(e instanceof Dg){if(null==this.id2Value[e.id])throw new Lf(`Nonexistent key: ${e.name}`);return this.id2Value[e.id]}{const t=this.name2Id[e];if(null==t)throw new Lf(`Feed dict has no SymbolicTensor name: ${e}`);return this.id2Value[t]}}getMask(e){if(e instanceof Dg){if(null==this.id2Value[e.id])throw new Lf(`Nonexistent key: ${e.name}`);return this.id2Mask[e.id]}{const t=this.name2Id[e];if(null==t)throw new Lf(`Feed dict has no SymbolicTensor name: ${e}`);return this.id2Mask[t]}}disposeMasks(){null!=this.id2Mask&&Yi(this.id2Mask)}}const My={},Ly={};function zy(e,t,n,s){const r=null!=n&&n.training,a=Array.isArray(e),i=a?e:[e],o=i.map((e=>e.name)),u=[],c=t.names();for(const e of o)-1!==c.indexOf(e)?u.push(t.getValue(e)):u.push(null);null!=s&&(s.maxNumTensors=-1/0,s.minNumTensors=1/0);const h=o.join(",")+"|"+t.names().join(",");let p,d;if(null==My[h]){const e=function(e,t){l(null!=e&&e.length>0,(()=>"Expected at least one fetch, got none"));let n=[],s={};if(1===e.length){const r=Py(e[0],t);n=r.sorted,s=r.recipientMap}else{const r=new Set;for(const a of e){const{sorted:e,recipientMap:i}=Py(a,t);for(const t of e)r.has(t.name)||(n.push(t),r.add(t.name));for(const e in i)null==s[e]&&(s[e]=new Set),i[e].forEach((t=>s[e].add(t)))}}return{sorted:n,recipientCounts:By(s)}}(i,t);p=e.sorted,d=e.recipientCounts,My[h]=p,Ly[h]=d}p=My[h],d={},r||Object.assign(d,Ly[h]);const f=new Oy(t);for(let e=0;e<p.length;++e){if(null!=s){const e=Ki().numTensors;e>s.maxNumTensors&&(s.maxNumTensors=e),e<s.minNumTensors&&(s.minNumTensors=e)}const a=p[e],i=a.sourceLayer;if(i instanceof Pg)continue;const l=[],c=[],h=[];let m=!1;for(const e of a.inputs){const n=f.getValue(e),s=f.getMask(e);l.push(n),c.push(s),null!=s&&(m=!0),r||(d[e.name]--,0!==d[e.name]||t.hasKey(e)||-1!==o.indexOf(e.name)||n.isDisposed||!0===e.sourceLayer.stateful||h.push(n))}m&&((n=n||{}).mask=c[0]);const g=Gf(i.apply(l,n));let y=null;i.supportsMasking&&(y=i.computeMask(l,c));const b=Wy(a),x=Array.isArray(b)?b:[b];for(let e=0;e<x.length;++e){f.hasKey(x[e])||f.add(x[e],g[e],Array.isArray(y)?y[0]:y);const t=o.indexOf(x[e].name);-1!==t&&(u[t]=g[e])}r||Yi(h)}return f.disposeMasks(),a?u:u[0]}function By(e){const t={};for(const n in e)t[n]=e[n].size;return t}function Py(e,t){const n=new Set,s=[],r={};for(const e of t.names())n.add(e);const a=[],i=[];for(a.push(e);a.length>0;){const e=a[a.length-1];if(n.has(e.name)){a.pop();continue}const t=i[i.length-1]===a.length-1;if(0===e.inputs.length||t)a.pop(),s.push(e),n.add(e.name),t&&i.pop();else{i.push(a.length-1);for(const t of e.inputs)null==r[t.name]&&(r[t.name]=new Set),r[t.name].add(e.name),n.has(t.name)||a.push(t)}}return{sorted:s,recipientMap:r}}function Wy(e){let t;if(1===e.sourceLayer.inboundNodes.length)t=e.sourceLayer.output;else{let n=null;for(let t=0;t<e.sourceLayer.inboundNodes.length;++t)for(const s of e.sourceLayer.inboundNodes[t].outputTensors)if(s.id===e.id){n=t;break}t=e.sourceLayer.getOutputAt(n)}return t}class Vy extends zg{constructor(e){if(super({}),this.containerNodes=new Set,this.name=e.name,null==this.name){const e=this.getClassName().toLowerCase();this.name=Ng(e)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(e.inputs)?this.inputs=e.inputs.slice():this.inputs=[e.inputs],Array.isArray(e.outputs)?this.outputs=e.outputs.slice():this.outputs=[e.outputs],Jf(this.inputs).length!==this.inputs.length)throw new Lf(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((e=>e.name))}`);Jf(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((e=>e.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const e of this.outputs){const t=e.sourceLayer,n=e.nodeIndex,s=e.tensorIndex;this.outputLayers.push(t),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const e of this.inputs){const t=e.sourceLayer,n=e.nodeIndex,s=e.tensorIndex;Wf(0===n,"input layer has >1 nodes"),Wf(0===s,"input layer has >1 tensors"),this.inputLayers.push(t),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t];if(!(n instanceof Pg))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${e.inputs}. Input ${t} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const e of this.outputLayers)this.outputNames.push(e.name);this.internalInputShapes=this.inputs.map((e=>e.shape)),this.internalOutputShapes=this.outputs.map((e=>e.shape));const t={},n={},s={},r={},a={},i=[],o=(e,t,n,s,r,l)=>{null!=s&&null!=r&&null!=l||(s=e.sourceLayer,r=e.nodeIndex,l=e.tensorIndex);const u=s.inboundNodes[r];if(-1!==n.indexOf(u))throw new Mf(`The tensor ${e.name} at layer "${s.name}" is part of a cycle.`);if(-1!==t.indexOf(u))return;this.containerNodes.add(Vy.nodeKey(s,r)),s.id in a||(a[s.id]=Object.keys(a).length),-1===n.indexOf(u)&&n.push(u);const c=u.inboundLayers.length;for(let e=0;e<c;e++){const s=u.inputTensors[e],r=u.inboundLayers[e],a=u.nodeIndices[e],i=u.tensorIndices[e];o(s,t,n,r,a,i)}for(t.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);i.push(u)},l=[],u=[];for(const e of this.outputs)o(e,l,u);const c=i.slice().reverse();for(const e of c){n[e.id]=e,e.id in t||(t[e.id]=0);let a=t[e.id];const i=null==s[e.outboundLayer.id]?0:s[e.outboundLayer.id];a=Math.max(a,i),s[e.outboundLayer.id]=a,r[e.outboundLayer.id]=e.outboundLayer,t[e.id]=a;for(let s=0;s<e.inboundLayers.length;s++){const r=e.inboundLayers[s],i=e.nodeIndices[s],o=r.inboundNodes[i],l=null==t[o.id]?0:t[o.id];t[o.id]=Math.max(a+1,l),n[o.id]=o}}const h={};for(const e in t){const s=t[e];s in h||(h[s]=[]),h[s].push(n[e])}const p={};for(const e in s){const t=s[e];t in p||(p[t]=[]),p[t].push(r[e])}let d=Object.keys(p).map((e=>parseInt(e,10))).sort(Zf);this.layers=[];for(const e of d){const t=p[e];t.sort(((e,t)=>{const n=a[e.id],s=a[t.id];return n<s?-1:n>s?1:0}));for(const e of t)e instanceof Vy&&this.internalContainerRefs.push(e),this.layers.push(e)}this.layersByDepth=p,d=Object.keys(h).map((e=>parseInt(e,10))).sort(Zf);const f=this.inputs.slice(),m=[];for(const e of d)for(const t of h[e]){const e=t.outboundLayer;if(null!=e){for(const n of t.inputTensors)if(-1===f.indexOf(n))throw new Mf(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${e.name}". The following previous layers were accessed without issue: ${m}`);for(const e of t.outputTensors)f.push(e);m.push(e.name)}}this.nodesByDepth=h;const g=this.layers.map((e=>e.name));for(const e of g){const t=g.filter((t=>t===e)).length;if(1!==t)throw new Mf(`The name "${e}" is used ${t} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(g))}this.outboundNodes=[],this.inboundNodes=[],new Mg({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((e=>null)),outputMasks:this.outputs.map((e=>null)),inputShapes:this.inputs.map((e=>e.shape)),outputShapes:this.outputs.map((e=>e.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const e={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const t of this.layers)e.numDisposedVariables+=t.dispose().numDisposedVariables;for(const t of this.internalContainerRefs)e.numDisposedVariables+=t.dispose().numDisposedVariables}return e.refCountAfterDispose=this._refCount,e}get trainable(){return this.trainable_}set trainable(e){this.layers.forEach((t=>{t._trainableWeights.forEach((t=>t.trainable=e))})),this.trainable_=e}get trainableWeights(){if(this._trainableWeights.length>0)throw new Lf("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let e=[];for(const t of this.layers)e=e.concat(t.trainableWeights);return e}get nonTrainableWeights(){const e=[];for(const t of this.layers)e.push(...t.nonTrainableWeights);if(!this.trainable){const t=[];for(const e of this.layers)t.push(...e.trainableWeights);return t.concat(e)}return e}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(e,t=!0){const n={};let s=0;for(const e of this.layers)for(const t of e.weights){if(null!=n[t.originalName])throw new Lf(`Duplicate weight name: ${t.originalName}`);n[t.originalName]=t,s++}const r=[];for(const s in e){let a=s;if(null==n[s]){const e=s.split("/");a=e.slice(0,-2).concat([e[e.length-1]]).join("/")}if(null!=n[a])r.push([n[a],e[s]]);else if(t)throw new Lf(`Provided weight data has no target variable: ${s}`);delete n[a]}if(t){const e=[];for(const t in n)e.push(t);if(e.length>0)throw new Lf(`${e.length} of ${s} weights are not set: ${e}`)}Fg(r)}updatedConfig(){const e=this.getConfig(),t={};return t.className=this.getClassName(),t.config=e,t.kerasVersion="tfjs-layers 3.7.0",t.backend="TensorFlow.js",t}toJSON(e,t=!0){const n=_y(this.updatedConfig());return t?JSON.stringify(n):n}call(e,t){return Xi((()=>{e=Gf(e);const n=new Oy;for(let t=0;t<this.inputs.length;++t)n.add(this.inputs[t],e[t]);return zy(this.outputs,n,t)}))}computeMask(e,t){return Xi((()=>{let n;return e=Gf(e),n=null==t?Pf(null,e.length):Gf(t),this.runInternalGraph(e,n)[1]}))}computeOutputShape(e){const t=Sg(e);if(t.length!==this.inputLayers.length)throw new Lf(`Invalid inputShape argument ${e}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let e=0;e<t.length;e++){const s=this.inputLayers[e],r=t[e];n[s.name+"_0_0"]=r}const s=Object.keys(this.nodesByDepth).map((e=>parseInt(e,10))).sort(Zf);if(s.length>1)for(const e of s){const t=this.nodesByDepth[e];for(const e of t){const t=e.outboundLayer;if(-1!==this.inputLayers.map((e=>e.id)).indexOf(t.id))continue;const s=[];for(let t=0;t<e.inboundLayers.length;t++){const r=e.inboundLayers[t],a=e.nodeIndices[t],i=e.tensorIndices[t],o=n[`${r.name}_${a}_${i}`];s.push(o)}const r=Sg(t.computeOutputShape(Uf(s))),a=t.inboundNodes.indexOf(e);for(let e=0;e<r.length;e++){n[`${t.name}_${a}_${e}`]=r[e]}}}const r=[],a=[];for(let e=0;e<this.outputLayers.length;e++){const t=this.outputLayers[e],n=this.outputLayersNodeIndices[e],s=this.outputLayersTensorIndices[e],r=`${t.name}_${n}_${s}`;a.push(r)}for(let e=0;e<a.length;e++){const t=a[e];Wf(t in n),r.push(n[t])}return Uf(r)}runInternalGraph(e,t){null==t&&(t=Pf(null,e.length));const n={};for(let s=0;s<this.inputs.length;++s){const r=this.inputs[s],a=e[s],i=t[s];n[r.id]=[a,i]}const s=Object.keys(this.nodesByDepth).map((e=>parseInt(e,10))).sort(Zf);for(const e of s){const t=this.nodesByDepth[e];for(const e of t){const t=e.outboundLayer,s=e.inputTensors,r=e.outputTensors,a=new Array;for(const e of s)e.id in n&&a.push(n[e.id]);if(a.length===s.length){let s,i,o,l,u={};if(null!=e.callArgs&&(u=e.callArgs),1===a.length){const[e,n]=a[0];null==u.mask&&(u.mask=n),o=Gf(t.call(e,u)),l=Gf(t.computeMask(e,n)),s=[e],i=[n]}else s=a.map((e=>e[0])),i=a.map((e=>e[1])),null==u.mask&&(u.mask=i),o=Gf(t.call(s,u)),l=Gf(t.computeMask(s,i));if(t.activityRegularizer)throw new zf("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let e=0;e<r.length;++e){const t=r[e],s=o[e],a=l[e];n[t.id]=[s,a]}}}}const r=[],a=[],i=[];for(const e of this.outputs){Wf(e.id in n,`Could not compute output ${e.name} : ${e.id}`);const[t,s]=n[e.id];i.push(t.shape),r.push(t),a.push(s)}return[r,a,i]}buildNodeConversionMap(e){const t={};let n;for(const e of this.layers){n=e instanceof Vy?1:0;for(let s=0;s<e.inboundNodes.length;s++){const r=Vy.nodeKey(e,s);this.containerNodes.has(r)&&(t[r]=n,n+=1)}}return t}getLayer(e,t){if(null!=t){if(this.layers.length<=t)throw new Lf(`Was asked to retrieve layer at index ${t}, but model only has ${this.layers.length} layer(s).`);return this.layers[t]}if(null==e)throw new Lf("Provide either a layer name or layer index");for(const t of this.layers)if(t.name===e)return t;throw new Lf(`No such layer: ${e}`)}calculateLosses(){return Xi((()=>{const e=[];for(const t of this.layers)for(let n=0;n<t.inboundNodes.length;++n){const s=Vy.nodeKey(t,n);this.containerNodes.has(s)&&e.push(...t.calculateLosses())}return e}))}getConfig(){const e={name:this.name},t=this.buildNodeConversionMap(this.layers),n=[];for(const e of this.layers){const s=e.getClassName(),r=e.getConfig(),a=[];for(let n=0;n<e.inboundNodes.length;n++){const s=e.inboundNodes[n],r=Vy.nodeKey(e,n);let i={};if(this.containerNodes.has(r)){if(s.callArgs)try{JSON.stringify(s.callArgs),i=s.callArgs}catch(t){console.warn(`Layer ${e.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),i={}}if(s.inboundLayers.length>0){const e=[];for(let n=0;n<s.inboundLayers.length;n++){const r=s.inboundLayers[n],a=s.nodeIndices[n],o=s.tensorIndices[n];let l=t[Vy.nodeKey(r,a)];null==l&&(l=0),e.push([r.name,l,o,i])}a.push(e)}}}const i={};i.name=e.name,i.className=s,i.config=r,i.inboundNodes=a,n.push(i)}e.layers=n;const s=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e],r=this.inputLayersNodeIndices[e],a=Vy.nodeKey(n,r);if(!this.containerNodes.has(a))continue;let i=t[a];null==i&&(i=0);const o=this.inputLayersTensorIndices[e];s.push([n.name,i,o])}e.inputLayers=s;const r=[];for(let e=0;e<this.outputLayers.length;e++){const n=this.outputLayers[e],s=this.outputLayersNodeIndices[e],a=Vy.nodeKey(n,s);if(!this.containerNodes.has(a))continue;let i=t[a];null==i&&(i=0);const o=this.outputLayersTensorIndices[e];r.push([n.name,i,o])}return e.outputLayers=r,e}static fromConfig(e,t,n={},s=!1){const r={},a={};function i(e,t){e.name in a?a[e.name].push(t):a[e.name]=[t]}function o(e,t){const n=[];let s;for(const a of t){const o=a[0],l=a[1],u=a[2];if(s=null==a[3]?{}:a[3],!(o in r))return void i(e,t);const c=r[o];if(c.inboundNodes.length<=l)return void i(e,t);const h=c.inboundNodes[l];n.push(h.outputTensors[u])}n.length>0&&e.apply(Uf(n),s)}function l(e){const n=e.name,a=Qg(e,null!=t.customObjects?t.customObjects:{});a.setFastWeightInitDuringBuild(s),r[n]=a;e.inboundNodes.forEach((e=>{if(!(e instanceof Array))throw new Lf(`Corrupted configuration, expected array for nodeData: ${e}`);i(a,e)}))}const u=t.name,c=t.layers;for(const e of c)l(e);for(;!Qf(a);)for(const e of c){const t=r[e.name];if(t.name in a){const e=a[t.name];delete a[t.name];for(const n of e)o(t,n)}}const h=[],p=[],d=t.inputLayers;for(const e of d){const t=e[0],n=e[1],s=e[2];Wf(t in r);const a=r[t].inboundNodes[n].outputTensors;h.push(a[s])}const f=t.outputLayers;for(const e of f){const t=e[0],n=e[1],s=e[2];Wf(t in r);const a=r[t].inboundNodes[n].outputTensors;p.push(a[s])}return new e({inputs:h,outputs:p,name:u})}get stateful(){if(this._stateful)throw new Lf("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const e of this.layers)if(e.stateful)return!0;return!1}resetStates(){Xi((()=>{this.layers.forEach((e=>{e.stateful&&e.resetStates()}))}))}}function Uy(e,t,n){const s=t.length;if(null==e||Array.isArray(e)&&0===e.length)return t.map((e=>null));if(1===s)return Array.isArray(e)&&1===e.length?e:"object"==typeof e&&t[0]in e?[e[t[0]]]:[e];if(Array.isArray(e)){if(e.length!==s)throw new Error(`Provided ${n} is an array of ${e.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return e}if("object"==typeof e&&Object.keys(e).length>0&&"object"==typeof e[Object.keys(e)[0]]){const n=[];return t.forEach((t=>{t in e?n.push(e[t]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${t} keys. Provided ${n} not understood: ${JSON.stringify(e)}`)}function Gy(e,t){return Uy(e,t,"classWeight")}async function Hy(e,t,n,s){if(null!=t||null!=s)throw new Error("Support sampleWeight is not implemented yet");if(null!=n){const t=Xi((()=>{if(1===e.shape.length)return e.clone();if(2===e.shape.length){if(e.shape[1]>1){const t=1;return e.argMax(t)}if(1===e.shape[1])return e.reshape([e.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${e.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${e.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),s=Array.from(await t.data());Yi(t);const r=[];return s.forEach((e=>{if(null==n[e])throw new Error(`classWeight must contain all classes in the training data. The class ${e} exists in the data but not in classWeight`);r.push(n[e])})),Wc(r,"float32")}return null}function jy(e,t){return ro(e,t)}function qy(e,t){let n,s;const r=t;n=r.xs,s=r.ys,l(null!=n&&null!=s,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${t}`));const a=Ky("input",e.inputNames,n),i=Ky("output",e.outputNames,s),o=a[0].shape[0];l(a.length===e.inputs.length,(()=>`LayersModel has ${e.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(e.inputNames)})`)),l(i.length===e.outputs.length,(()=>`LayersModel has ${e.outputs.length} outputs, but the dataset provides ${i.length} outputs.  (Expected output keys: ${JSON.stringify(e.outputNames)})`));for(let t=0;t<a.length;t++)l(a[t].shape[0]===o,(()=>`Batch size mismatch: input ${e.inputNames[t]} has ${a[t].shape[0]}; expected  ${o} based on input ${e.inputNames[0]}.`));for(let t=0;t<i.length;t++)l(i[t].shape[0]===o,(()=>`Batch size mismatch: output ${e.outputNames[t]} has ${i[t].shape[0]}; expected  ${o} based on input ${e.inputNames[0]}.`));return{xs:a,ys:i}}function Ky(e,t,n){if(n instanceof dr)return[n];if(Array.isArray(n))return l(n.length===t.length,(()=>`Received an array of ${n.length} Tensors, but expected ${t.length} to match the ${e} keys ${t}.`)),n;{const s=[];for(const r of t){if(null==n[r])throw new Lf(`The feature data generated by the dataset lacks the required ${e} key '${r}'.`);s.push(n[r])}return s}}async function Xy(e,t,n){const s=null!=n.batchesPerEpoch;if(l(null!=e.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),l(null!=n,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),l(null!=n.epochs&&n.epochs>0&&Number.isInteger(n.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${n.epochs}`)),l(!s||n.batchesPerEpoch>0&&Number.isInteger(n.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${n.batchesPerEpoch}`)),l(null==n.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),e.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");e.isTraining=!0;try{const r=null!=n.validationData;let a,i;if(r)if(Yy(n.validationData))l(null==n.validationBatches||n.validationBatches>0&&Number.isInteger(n.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${n.validationBatches}`));else{const e=function(e){if(3===e.length)throw new zf("Validation with sample weights is not implemented yet.");return{xs:e[0],ys:e[1]}}(n.validationData);a=e.xs,i=e.ys}const o=e.makeTrainFunction(),u=e.getDedupedMetricsNames();let c;c=r?u.slice().concat(u.map((e=>"val_"+e))):u.slice();const h=Yg(n.callbacks,n.yieldEvery),p=null==n.verbose?1:n.verbose,{callbackList:d,history:f}=Jg(h,p,n.epochs,null,null,function(e,t){let n=null;null!=t.batchesPerEpoch?n=t.batchesPerEpoch:Number.isFinite(e.size)&&(n=e.size);return n}(t,n),null,r,c);d.setModel(e),e.history=f,await d.onTrainBegin(),e.stopTraining_=!1;let m=null==n.initialEpoch?0:n.initialEpoch,g=await t.iterator();for(;m<n.epochs;){const l={};await d.onEpochBegin(m);let c=0,h=0;for(s||(g=await t.iterator());!s||c<n.batchesPerEpoch;){const t=await g.next();if(s&&t.done){console.warn(`You provided \`batchesPerEpoch\` as ${n.batchesPerEpoch}, but your dataset iterator ran out of data after ${c} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+n.batchesPerEpoch*n.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=t.value){const{xs:s,ys:r}=qy(e,t.value),a={};a.batch=h,a.size=s[0].shape[0],await d.onBatchBegin(h,a);const i=[];if(null!=n.classWeight){const t=Gy(n.classWeight,e.outputNames);for(let e=0;e<t.length;++e)i.push(await Hy(r[e],null,t[e]))}const l=s.concat(r).concat(i),p=o(l);Yi(l);for(let e=0;e<u.length;++e){const t=u[e],n=p[e];a[t]=n,Zi(n)}await d.onBatchEnd(h,a),Ug(a),h++,c++}if(s?c>=n.batchesPerEpoch:t.done){if(r){let t;t=Yy(n.validationData)?Gf(await e.evaluateDataset(n.validationData,{batches:n.validationBatches})):Gf(e.evaluate(a,i,{batchSize:null==n.validationBatchSize?32:n.validationBatchSize,verbose:0}));for(let n=0;n<e.metricsNames.length;++n)l[`val_${e.metricsNames[n]}`]=t[n]}break}if(e.stopTraining_)break}if(await d.onEpochEnd(m,l),m++,e.stopTraining_)break}return await d.onTrainEnd(),await e.history.syncData(),e.history}finally{e.isTraining=!1}}function Yy(e){return"function"==typeof e.iterator}function Zy(e){l(e>0&&Number.isInteger(e),(()=>`batchSize is required to be a positive integer, but got ${e}`))}function Jy(e,t,n){return null==e?[null]:Array.isArray(e)?e.map((e=>Lm(e,t,n-t))):Lm(e,t,n-t)}function Qy(e,t){return Xi((()=>null==e?null:Array.isArray(e)?e.map((e=>Qy(e,t))):Hm(e,"int32"===t.dtype?t:t.toInt())))}function eb(e,t){const n=[];let s=0,r=null;for(;s<e;)r=s+t,r>=e&&(r=e),n.push([s,r]),s=r;return n}async function tb(e,t,n,s={}){if(e.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let a,i,o,l,u,c,h;e.isTraining=!0;try{const p=null==s.batchSize?32:s.batchSize;Zy(p);const d=!1,f=await e.standardizeUserData(t,n,s.sampleWeight,s.classWeight,d,p);a=f[0],i=f[1],h=f[2];let m,g=!1;if(null!=s.validationData&&s.validationData.length>0){if(g=!0,2!==s.validationData.length)throw 3===s.validationData.length?new zf("validationData including sample weights is not supported yet."):new Lf(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${s.validationData} is invalid.`);o=s.validationData[0],l=s.validationData[1];const t=!0,n=await e.standardizeUserData(o,l,null,null,t,p);u=n[0],c=n[1],m=u.concat(c)}else if(null!=s.validationSplit&&s.validationSplit>0&&s.validationSplit<1){g=!0;const e=Math.floor(a[0].shape[0]*(1-s.validationSplit)),t=a[0].shape[0];u=Jy(a,e,t),a=Jy(a,0,e),c=Jy(i,e,t),i=Jy(i,0,e),m=u.concat(c)}else null!=s.validationSteps&&(g=!0);const y=a.concat(i).concat(h);e.checkTrainableWeightsConsistency();const b=e.makeTrainFunction(),x=e.getDedupedMetricsNames();let w,k;g?(e.makeTestFunction(),w=e.testFunction,k=x.slice().concat(x.map((e=>"val_"+e)))):(w=null,m=[],k=x.slice());const v=Yg(s.callbacks,s.yieldEvery);return await async function(e,t,n,s,a,i,o,l,u,c,h,p,d,f,m){null==a&&(a=32),null==i&&(i=1),null==h&&(h=!0),null==d&&(d=0);let g=!1;if(null!=u&&null!=c&&(g=!0),null!=m&&(g=!0,null==f))throw new Lf("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const y=e.checkNumSamples(n,a,f,"steps_per_epoch");let b;null!=y&&(b=Dm(0,y)),null==o&&(o=1);const{callbackList:x,history:w}=Jg(l,o,i,d,y,f,a,g,p);x.setModel(e),e.history=w,await x.onTrainBegin(),e.stopTraining_=!1;for(let o=d;o<i;++o){await x.onEpochBegin(o);const i={};if(null!=f)throw new zf("stepsPerEpoch mode is not implemented yet.");{if("batch"===h)throw new zf("batch shuffling is not implemneted yet");h&&r(b);const o=Wc(b),l=eb(y,a);for(let r=0;r<l.length;++r){const h={};if(await x.onBatchBegin(r,h),Xi((()=>{const p=l[r][0],d=l[r][1],f=Lm(o,p,d-p);h.batch=r,h.size=d-p;const m=Qy(n,f),y=t(m);for(let e=0;e<s.length;++e){const t=s[e],n=y[e];h[t]=n,Zi(n)}if(r===l.length-1&&g){const t=e.testLoop(u,c,a);for(let e=0;e<s.length;++e){const n=s[e],r=t[e];Zi(r),i["val_"+n]=r}}})),await x.onBatchEnd(r,h),Ug(h),e.stopTraining_)break}o.dispose()}if(await x.onEpochEnd(o,i),e.stopTraining_)break}return await x.onTrainEnd(),await e.history.syncData(),e.history}(e,b,y,x,p,s.epochs,s.verbose,v,w,m,s.shuffle,k,s.initialEpoch,null,null)}finally{e.isTraining=!1,sb(a,t),sb(i,n),sb(u,o),sb(c,l),null!=h&&Yi(h)}}function nb(e){const t=[];e instanceof dr&&(e=[e]);for(let n=0;n<e.length;++n){const s=e[n];if(1===s.rank)t.push(Mm(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");t.push(s)}}return t}function sb(e,t){if(null==e)return;const n=[];if(t instanceof dr)n.push(t.id);else if(Array.isArray(t))t.forEach((e=>n.push(e.id)));else if(null!=t)for(const e in t){const s=t[e];n.push(s.id)}const s=[];if(e instanceof dr)-1===n.indexOf(e.id)&&s.push(e);else if(Array.isArray(e))e.forEach((e=>{-1===n.indexOf(e.id)&&s.push(e)}));else if(null!=e)for(const t in e){const r=e[t];-1===n.indexOf(r.id)&&s.push(r)}s.forEach((e=>{e.isDisposed||e.dispose()}))}function rb(e){return Array.isArray(e)}function ab(e){return!function(e){return e instanceof dr}(e)&&!rb(e)}function ib(e,t,n,s=!0,r=""){if(null==t||0===t.length){if(null!=e){let t=!1;if(rb(e)&&e.length>0)t=!0;else if(ab(e)){for(const n in e)if(e.hasOwnProperty(n)){t=!0;break}}else t=!0;if(t)throw new Lf(`Error when checking model ${r} expected no data, but got ${e}`)}return[]}if(null==e)return t.map((e=>null));let a;if(ab(e)){e=e,a=[];for(const n of t){if(null==e[n])throw new Lf(`No data provided for "${n}". Need data for each key in: ${t}`);a.push(e[n])}}else if(rb(e)){if((e=e).length!==t.length)throw new Lf(`Error when checking model ${r}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${t.length} Tensor(s), but instead got the following list of Tensor(s): ${e}`);a=e}else{if(e=e,t.length>1)throw new Lf(`The model ${r} expects ${t.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${e.shape}`);a=[e]}if(a=nb(a),null!=n)for(let e=0;e<t.length;++e){if(null==n[e])continue;const i=a[e];if(i.shape.length!==n[e].length)throw new Lf(`Error when checking ${r}: expected ${t[e]} to have ${n[e].length} dimension(s). but got array with shape ${i.shape}`);for(let a=0;a<n[e].length;++a){if(0===a&&!s)continue;const o=i.shape[a],l=n[e][a];if(null!=l&&l>=0&&o!==l)throw new Lf(`Error when checking ${r}: expected ${t[e]} to have shape [${n[e]}], but got array with shape [${i.shape}].`)}}return a}function ob(e,t,n,s=!0,r=""){let a;if(Array.isArray(e)){if(e.length!==t.length)throw new Lf(`Error when checking model ${r}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${t.length} Tensor(s), but instead got ${e.length} Tensors(s).`);a=e}else{if(t.length>1)throw new Lf(`The model expects ${t.length} ${r} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(e.shape)}.`);a=[e]}if(null!=n)for(let e=0;e<t.length;++e){if(null==n[e])continue;const i=a[e];if(i.shape.length!==n[e].length)throw new Lf(`Error when checking ${r}: expected ${t[e]} to have ${n[e].length} dimension(s), but got array with shape ${JSON.stringify(i.shape)}`);for(let a=0;a<n[e].length;++a){if(0===a&&!s)continue;const o=i.shape[a],l=n[e][a];if(null!=l&&l!==o)throw new Lf(`Error when checking ${r}: expected ${t[e]} to have shape ${JSON.stringify(n[e])} but got array with shape ${JSON.stringify(i.shape)}.`)}}}class lb extends Vy{constructor(e){super(e),this.isTraining=!1}summary(e,t,n=console.log){if(!this.built)throw new Lf("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");Cy(this,e,t,n)}compile(e){if(null==e.loss&&(e.loss=[]),this.loss=e.loss,"string"==typeof e.optimizer)this.optimizer_=function(e){const t={Adagrad:()=>Ep.adagrad(.01),Adadelta:()=>Ep.adadelta(1,.95,Df()),Adam:()=>Ep.adam(.001,.9,.999,Df()),Adamax:()=>Ep.adamax(.002,.9,.999,Df(),0),RMSProp:()=>Ep.rmsprop(.001,.9,0,Df()),SGD:()=>Ep.sgd(.01)};if(t.adagrad=t.Adagrad,t.adadelta=t.Adadelta,t.adam=t.Adam,t.adamax=t.Adamax,t.rmsprop=t.RMSProp,t.sgd=t.SGD,e in t)return t[e]();throw new Lf(`Unknown Optimizer ${e}`)}(e.optimizer),this.isOptimizerOwned=!0;else{if(!(e.optimizer instanceof wp))throw new Lf("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=e.optimizer,this.isOptimizerOwned=!1}let t=[];if(Array.isArray(e.loss)||"string"==typeof e.loss||"function"==typeof e.loss)if(Array.isArray(e.loss)){if(e.loss.length!==this.outputs.length)throw new Lf(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${e.loss}.`);const n=e.loss;t=n.map((e=>hy(e)))}else{const n=hy(e.loss);this.outputs.forEach((e=>{t.push(n)}))}else{e.loss=e.loss;for(const t in e.loss)if(-1===this.outputNames.indexOf(t))throw new Lf(`Unknown entry in loss dictionary: "${t}". Only expected the following keys: ${this.outputNames}`);for(const n of this.outputNames)null==e.loss[n]&&console.warn(`Output "${n}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${n} during training`),t.push(hy(e.loss[n]))}this.lossFunctions=t,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let e=0;e<this.outputs.length;++e){const t=this.internalOutputShapes[e],n=this.outputNames[e];this.feedOutputNames.push(n),this.feedOutputShapes.push(t),this.feedLossFns.push(this.lossFunctions[e])}const n=[];this.metrics=e.metrics,this.metricsNames=["loss"],this.metricsTensors=[],$m("loss",(()=>{for(let e=0;e<this.outputs.length;++e){if(-1!==n.indexOf(e))continue;const t=this.lossFunctions[e];this.outputs.length>1&&(this.metricsTensors.push([t,e]),this.metricsNames.push(this.outputNames[e]+"_loss"))}}));const s=function(e,t){if(null==e||Array.isArray(e)&&0===e.length)return t.map((e=>[]));let n;if("string"==typeof e||"function"==typeof e)n=[e];else{if(!Array.isArray(e)&&"object"!=typeof e)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${e}`);n=e}if(Array.isArray(n))return t.map((e=>n));{const e=[];for(const s of t){let t=n.hasOwnProperty(s)?n[s]:[];Array.isArray(t)||(t=[t]),e.push(t)}return e}}(e.metrics,this.outputNames),r=(e,t,n)=>{this.outputNames.length>1&&(t=this.outputNames[e]+"_"+t),this.metricsNames.push(t),this.metricsTensors.push([n,e])};$m("metric",(()=>{for(let e=0;e<this.outputs.length;++e){if(-1!==n.indexOf(e))continue;(t=>{let n,s,a;for(const i of t){if("string"==typeof i&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(i)){const t=this.internalOutputShapes[e];let r;1===t[t.length-1]||this.lossFunctions[e]===oy?-1!==["accuracy","acc"].indexOf(i)?s=py:-1!==["crossentropy","ce"].indexOf(i)&&(s=yy):this.lossFunctions[e]===iy?-1!==["accuracy","acc"].indexOf(i)?s=by:-1!==["crossentropy","ce"].indexOf(i)&&(s=wy):-1!==["accuracy","acc"].indexOf(i)?s=dy:-1!==["crossentropy","ce"].indexOf(i)&&(s=xy),-1!==["accuracy","acc"].indexOf(i)?r="acc":-1!==["crossentropy","ce"].indexOf(i)&&(r="ce"),a=s,n=""+r}else{const e=vy(i);a=e,n=""+Ny(i)}let t;$m(n,(()=>{t=a})),r(e,n,t)}})(s[e])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(e,t,n={}){const s=null==n.batchSize?32:n.batchSize;Zy(s);const r=this.standardizeUserDataXY(e,t,!0,s);try{const a=r[0].concat(r[1]);this.makeTestFunction();const i=this.testFunction;return Uf(this.testLoop(i,a,s,n.verbose,n.steps))}finally{sb(r[0],e),sb(r[1],t)}}async evaluateDataset(e,t){return this.makeTestFunction(),async function(e,t,n){const s=null!=(n=n||{}).batches,r=e.testFunction;let a=[];if(n.verbose>0)throw new zf("Verbose mode is not implemented yet.");l(!s||n.batches>0&&Number.isInteger(n.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(n.batches)}`));const i="function"==typeof t.next?t:await t.iterator();let o=0,u=0;for(;!s||u<n.batches;){const t=await i.next();if(a=Xi((()=>{if(t.value){const{xs:n,ys:s}=qy(e,t.value),i=n.concat(s),l=Xi((()=>r(i)));if(Yi(i),0===u)for(let e=0;e<l.length;++e)a.push(yc(0));const c=i[0].shape[0];for(let e=0;e<l.length;++e){const t=l[e],n=a[e];a[e]=Xi((()=>to(a[e],ro(c,t)))),u>0&&Yi(n)}Yi(l),o+=c,++u}return a})),t.done){s&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${n.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let e=0;e<a.length;++e){const t=a[e];a[e]=so(a[e],o),Yi(t)}return Uf(a)}(this,e,t)}checkNumSamples(e,t,n,s="steps"){let r;if(null!=n){if(r=null,null!=t)throw new Lf(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${t}`)}else{if(null==e)throw new Lf(`Either the input data should have a defined shape, or ${s} shoud be specified.`);r=Array.isArray(e)?e[0].shape[0]:e.shape[0]}return r}execute(e,t){if(Array.isArray(t)&&0===t.length)throw new Lf("`outputs` is an empty Array, which is not allowed.");const n=Array.isArray(t),s=n?t:[t],r=this.retrieveSymbolicTensors(s),a=new Oy;if(e instanceof dr&&(e=[e]),Array.isArray(e)){if(e.length!==this.inputs.length)throw new Lf(`The number of inputs provided (${e.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let t=0;t<this.inputs.length;++t)a.add(this.inputs[t],e[t])}else for(const t of this.inputs){const n=e[t.name];if(null==n)throw new Lf(`No value is provided for the model's input ${t.name}`);a.add(t,n)}const i=zy(r,a);return n?i:i[0]}retrieveSymbolicTensors(e){const t=Pf(null,e.length);let n=e.length;for(const s of this.layers){const r=Array.isArray(s.output)?s.output:[s.output],a=r.map((e=>e.name));for(let s=0;s<e.length;++s){const i=a.indexOf(e[s]);if(-1!==i&&(t[s]=r[i],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw t.forEach(((t,s)=>{null==t&&n.push(e[s])})),new Lf(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return t}predictLoop(e,t=32,n=!1){return Xi((()=>{const s=this.checkNumSamples(e);if(n)throw new zf("Verbose predictLoop() is not implemented yet.");const r=eb(s,t),a=this.outputs.map((e=>[]));for(let t=0;t<r.length;++t){Xi((()=>{const n=r[t][0],s=r[t][1],a=Jy(e,n,s),i=[];if(Array.isArray(a))for(let e=0;e<a.length;++e)i.push({key:this.inputs[e],value:a[e]});else i.push({key:this.inputs[0],value:a});const o=new Oy(i);return zy(this.outputs,o)})).forEach(((e,t)=>a[t].push(e)))}return Uf(a.map((e=>Oo(e,0))))}))}predict(e,t={}){const n=nb(e);ob(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==t.batchSize?32:t.batchSize;return Zy(s),this.predictLoop(n,s)}finally{sb(n,e)}}predictOnBatch(e){ob(e,this.inputNames,this.feedInputShapes,!0);const t=(Array.isArray(e)?e[0]:e).shape[0];return this.predictLoop(e,t)}standardizeUserDataXY(e,t,n=!0,s){if(null==this.optimizer_)throw new Mf("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let e=0;e<this.feedOutputShapes.length;++e){const t=this.feedOutputShapes[e];this.feedLossFns[e]===iy?r.push(t.slice(0,t.length-1).concat([1])):r.push(t)}if(function(e,t,n){const s=Jf(e.map((e=>e.shape[0])));s.sort();const r=Jf(t.map((e=>e.shape[0])));if(r.sort(),s.length>1)throw new Lf(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(e.map((e=>e.shape)))}`);if(r.length>1)throw new Lf(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((e=>e.shape)))}`);if(s.length>0&&r.length>0&&!d(s,r))throw new Lf(`Input Tensors should have the same number of samples as target Tensors. Found ${s[0]} input sample(s) and ${r[0]} target sample(s).`)}(e=ib(e,this.feedInputNames,this.feedInputShapes,!1,"input"),t=ib(t,this.feedOutputNames,r,!1,"target")),function(e,t,n){const s=[ty,oy,ay];for(let r=0;r<e.length;++r){const a=e[r],i=t[r],o=n[r];if(null!=i){if(i===ay&&1===a.shape[a.shape.length-1])throw new Lf(`You are passing a target array of shape ${a.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(i)){const e=a.shape.slice(1),t=o.slice(1);for(let n=0;n<e.length;++n){const s=e[n],r=t[n];if(null!=r&&s!==r)throw new Lf(`A target Tensor with shape ${a.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(t,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=s&&s>0&&e[0].shape[0]%s!=0)throw new Lf(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${s}. Found: ${e[0].shape[0]} sample(s).`);return[e,t]}async standardizeUserData(e,t,n,s,r=!0,a){const[i,o]=this.standardizeUserDataXY(e,t,r,a);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const e=Gy(s,this.outputNames);l=[];for(let t=0;t<e.length;++t)l.push(await Hy(o[t],null,e[t]))}return[i,o,l]}testLoop(e,t,n,s=0,r){return Xi((()=>{const a=this.checkNumSamples(t,n,r,"steps"),i=[];if(s>0)throw new zf("Verbose mode is not implemented yet.");if(null!=r)throw new zf("steps mode in testLoop() is not implemented yet");{const s=eb(a,n),r=Wc(Dm(0,a));for(let n=0;n<s.length;++n){const a=s[n][0],o=s[n][1],l=Lm(r,a,o-a),u=Qy(t,l),c=e(u);if(0===n)for(let e=0;e<c.length;++e)i.push(yc(0));for(let e=0;e<c.length;++e){const t=c[e];i[e]=to(i[e],ro(o-a,t))}}for(let e=0;e<i.length;++e)i[e]=so(i[e],a)}return i}))}getDedupedMetricsNames(){const e=this.metricsNames,t=[];for(let n=0;n<e.length;++n){const s=e[n];let r=s;if(Vf(e,s)>1){r+=`_${Vf(e.slice(0,n),s)}`}t.push(r)}return t}makeTrainFunction(){return e=>{const t=[],n=e.slice(0,this.inputs.length),s=e.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=e.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),a=[],i=this.collectedTrainableWeights.map((e=>e.read()));return[this.optimizer_.minimize((()=>{const e=[];for(let t=0;t<this.inputs.length;++t)e.push({key:this.inputs[t],value:n[t]});const i=new Oy(e),o=zy(this.outputs,i,{training:!0});let l;for(let e=0;e<this.lossFunctions.length;++e){let n=(0,this.lossFunctions[e])(s[e],o[e]);null!=r[e]&&(n=jy(n,r[e]));const a=xu(n);t.push(a),l=0===e?n:to(l,n)}for(let e=0;e<this.metricsTensors.length;++e){let n;if(this.outputs.length>1&&e<this.outputs.length)n=t[e];else{const t=this.metricsTensors[e][0],r=this.metricsTensors[e][1];n=xu(t(s[r],o[r]))}Zi(n),a.push(n)}return l=xu(l),this.calculateLosses().forEach((e=>{l=to(l,e)})),l}),!0,i)].concat(a)}}makeTestFunction(){this.testFunction=e=>Xi((()=>{const t=[];let n;const s=e.slice(0,this.inputs.length),r=e.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let e=0;e<this.inputs.length;++e)a.push({key:this.inputs[e],value:s[e]});const i=new Oy(a),o=zy(this.outputs,i);for(let e=0;e<this.lossFunctions.length;++e){const s=this.lossFunctions[e],a=xu(s(r[e],o[e]));n=0===e?a:to(n,a),t.push(n)}for(let e=0;e<this.metricsTensors.length;++e){const n=this.metricsTensors[e][0],s=this.metricsTensors[e][1],a=xu(n(r[s],o[s]));t.push(a)}return t}))}async fit(e,t,n={}){return tb(this,e,t,n)}async fitDataset(e,t){return Xy(this,e,t)}async trainOnBatch(e,t){const n=await this.standardizeUserData(e,t),s=n[0],r=n[1],a=this.makeTrainFunction()(s.concat(r)),i=[];for(const e of a){const t=await e.data();i.push(t[0])}return Yi(a),Uf(i)}getNamedWeights(e){const t=[],n=null!=e&&e.trainableOnly,s=n?this.trainableWeights:this.weights,r=this.getWeights(n);for(let e=0;e<s.length;++e)n&&!s[e].trainable||t.push({name:s[e].originalName,tensor:r[e]});return t}set stopTraining(e){this.stopTraining_=e}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(e){this.optimizer_!==e&&(this.optimizer_=e,this.isOptimizerOwned=!1)}dispose(){const e=super.dispose();if(0===e.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const t=Ki().numTensors;this.optimizer_.dispose(),e.numDisposedVariables+=t-Ki().numTensors}return e}getLossIdentifiers(){let e;if("string"==typeof this.loss)e=Hf(this.loss);else if(Array.isArray(this.loss)){for(const e of this.loss)if("string"!=typeof e)throw new Error("Serialization of non-string loss is not supported.");e=this.loss.map((e=>Hf(e)))}else{const t=Object.keys(this.loss);e={};const n=this.loss;for(const s of t){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");e[s]=Hf(n[s])}}return e}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[Hf(Ny(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((e=>Hf(Ny(e))));{const e={};for(const t in this.metrics)e[t]=Hf(Ny(this.metrics[t]));return e}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(e){if(null!=e.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=e.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=e.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const t=Qg(Fy(e.optimizer_config));let n,s;if("string"==typeof e.loss)n=jf(e.loss);else if(Array.isArray(e.loss))n=e.loss.map((e=>jf(e)));else if(null!=e.loss){n={};for(const t in e.loss)n[t]=jf(e.loss[t])}if(Array.isArray(e.metrics))s=e.metrics.map((e=>jf(e)));else if(null!=e.metrics){s={};for(const t in e.metrics)s[t]=jf(e.metrics[t])}this.compile({loss:n,metrics:s,optimizer:t})}async save(e,t){if("string"==typeof e){const t=oa(e);if(0===t.length)throw new Lf(`Cannot find any save handlers for URL '${e}'`);if(t.length>1)throw new Lf(`Found more than one (${t.length}) save handlers for URL '${e}'`);e=t[0]}if(null==e.save)throw new Lf("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const n=await Zr(this.getNamedWeights(t)),s={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.7.0",convertedBy:null};if(null!=t&&t.includeOptimizer&&null!=this.optimizer){s.trainingConfig=this.getTrainingConfig();const e="optimizer",{data:t,specs:r}=await Zr(await this.optimizer.getWeights(),e);n.specs.push(...r),n.data=na([n.data,t])}if(null!=this.userDefinedMetadata){const e=!0;Sy(this.userDefinedMetadata,this.name,e),s.userDefinedMetadata=this.userDefinedMetadata}return s.weightData=n.data,s.weightSpecs=n.specs,e.save(s)}setUserDefinedMetadata(e){Sy(e,this.name),this.userDefinedMetadata=e}getUserDefinedMetadata(){return this.userDefinedMetadata}}lb.className="Model",zi(lb);class ub extends lb{}async function cb(e,t){if(null==t&&(t={}),"string"==typeof e){const n=la(e,t);if(0===n.length)n.push(Ja(e,t));else if(n.length>1)throw new Lf(`Found more than one (${n.length}) load handlers for URL '${e}'`);e=n[0]}return async function(e,t,n){null==n&&(n={});if(null==e.load)throw new Lf("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const s=await e.load();let r=s.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==n.strict||n.strict,i=null!=s.weightData&&null!=s.weightSpecs&&a,o=Qg(Fy(r),t,i),l=s.trainingConfig;null!=l&&o.loadTrainingConfig(l);null!=s.userDefinedMetadata&&o.setUserDefinedMetadata(s.userDefinedMetadata);if(null!=s.weightData){if(null==s.weightSpecs)throw new Lf("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:e,optimizerWeights:t}=function(e,t){const n=Jr(e,t),s={},r=[];return t.forEach((e=>{"optimizer"===e.group?r.push({name:e.name,tensor:n[e.name]}):s[e.name]=n[e.name]})),{modelWeights:s,optimizerWeights:r}}(s.weightData,s.weightSpecs);o.loadWeights(e,a),null!=o.optimizer&&t.length>0&&await o.optimizer.setWeights(t),Yi(e),Yi(t.map((e=>e.tensor)))}return o}(e,void 0,t)}ub.className="Functional",zi(ub);class hb extends lb{constructor(e){if(super({inputs:[],outputs:[]}),e=e||{},this.trainable=!0,this.built=!1,this.name=null!=e.name?e.name:Ng("sequential_"),null!=e.layers)for(const t of e.layers)this.add(t)}checkShape(e){if(e.inboundNodes[0].outputTensors[0].shape.some((e=>e<0)))throw new Lf(`Negative dimension size caused by adding layer ${e.name} with input shape [${e.inboundNodes[0].inputTensors[0].shape}]`)}add(e){const t=e instanceof hb||e instanceof lb;let n;if(t){if(n=e,1!==n.outputs.length)throw new Lf("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new Lf("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===e.inboundNodes.length){if(null==e.batchInputShape)throw new Lf("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const t=Wg({batchShape:e.batchInputShape,dtype:e.dtype,name:e.name+"_input"});e.apply(t)}if(t)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==e.inboundNodes.length)throw new Lf(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${e.name} which has ${e.inboundNodes.length} pre-existing inbound connections.`);if(1!==e.inboundNodes[0].outputTensors.length)throw new Lf("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(e),this.outputs=[e.inboundNodes[0].outputTensors[0]],this.inputs=Bg(this.outputs[0])}this.inboundNodes=[],new Mg({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:Pf(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((e=>e.shape)),outputShapes:this.outputs[0].shape})}else{const t=e.apply(this.outputs[0]);if(Array.isArray(t))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(e),this.outputs=[t],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(e),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const e=this.layers.length-1;this.layers[e].outboundNodes=[],this.outputs=[this.layers[e].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(e,t){return null==this.model&&this.build(),this.model.call(e,t)}build(e){if(Cg(e),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new lb({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(e,t,n=console.log){this.built||this.build(),super.summary(e,t,n)}setWeights(e){null==this.model&&this.build(),this.model.setWeights(e)}evaluate(e,t,n={}){if(!this.built)throw new Mf("The model needs to be compiled before being used.");return this.model.evaluate(e,t,n)}async evaluateDataset(e,t){if(!this.built)throw new Mf("The model needs to be compiled before being used.");return this.model.evaluateDataset(e,t)}predict(e,t={}){return null==this.model&&this.build(),this.model.predict(e,t)}predictOnBatch(e){return null==this.model&&this.build(),this.model.predictOnBatch(e)}compile(e){this.build(),this.model.compile(e),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(e){this.model.optimizer=e}async fit(e,t,n={}){if(!this.built)throw new Mf("The model needs to be compiled before being used.");return this.model.fit(e,t,n)}async fitDataset(e,t){if(!this.built)throw new Mf("The model needs to be compiled before being used.");return this.model.fitDataset(e,t)}async trainOnBatch(e,t){return this.model.trainOnBatch(e,t)}static fromConfig(e,t,n={},s=!1){let r,a={};if(t instanceof Array){if(null==t[0].className||"Merge"===t[0].className)throw new Lf("Legacy serialization format not supported yet.");r=t}else l(null!=t.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=t.layers,delete t.layers,a=t;const i=new e(a);if(!(i instanceof hb))throw new zf(`Sequential.fromConfig called on non-Sequential input: ${i}`);for(const e of r){const t=Qg(e,void 0,s);s&&t.setFastWeightInitDuringBuild(!0),i.add(t)}return i}set stopTraining(e){if(null==this.model)throw new Lf("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=e}get stopTraining(){if(null==this.model)throw new Lf("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const e=[];for(const t of this.layers){const n={};n.className=t.getClassName(),n.config=t.getConfig(),e.push(n)}return{name:this.name,layers:e}}}function pb(e){return Wg(e)}hb.className="Sequential",zi(hb);class db extends Mi{getConfig(){return{}}}class fb extends db{apply(e,t=1){return function(e,t=1){if(1!==t)throw new zf(`Support for alpha values other than 1 (${t}) is not implemented yet.`);return Nl(e)}(e,t)}}fb.className="elu",zi(fb);class mb extends db{apply(e){return bc(e)}}mb.className="selu",zi(mb);class gb extends db{apply(e){return lc(e)}}gb.className="relu",zi(gb);class yb extends db{apply(e){return Xi((()=>Nu(6,lc(e))))}}yb.className="relu6",zi(yb);class bb extends db{apply(e){return e}}bb.className="linear",zi(bb);class xb extends db{apply(e){return Mo(e)}}xb.className="sigmoid",zi(xb);class wb extends db{apply(e){return function(e){return Xi((()=>{const t=to(.5,ro(.2,e));return Ko(t,0,1)}))}(e)}}wb.className="hardSigmoid",zi(wb);class kb extends db{apply(e){return Yl(e)}}kb.className="softplus",zi(kb);class vb extends db{apply(e){return function(e){return Xi((()=>so(e,ao(e).add(1))))}(e)}}vb.className="softsign",zi(vb);class Nb extends db{apply(e){return zo(e)}}Nb.className="tanh",zi(Nb);class Ib extends db{apply(e,t=-1){return Tc(e,t)}}Ib.className="softmax",zi(Ib);class Sb extends db{apply(e,t=-1){return tu(e,t)}}Sb.className="logSoftmax",zi(Sb);class $b extends db{apply(e,t=1){return Xi((()=>Mo(e.mul(t)).mul(e)))}}$b.className="swish",zi($b);class Cb extends db{apply(e){return Xi((()=>ro(e,zo(Yl(e)))))}}function Tb(e){return e.getClassName()}function Eb(e,t={}){return Yf(e,Li.getMap().classNameMap,t,"activation")}function Ab(e){if(null==e){const e={className:"linear",config:{}};return Eb(e)}if("string"==typeof e){const t={};return t.className=e,t.config={},Eb(t)}return e instanceof db?e:Eb(e)}function Rb(e){if(null!=e&&"object"!=typeof e)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${e}`)}Cb.className="mish",zi(Cb);class Fb extends Mi{}class _b extends Fb{constructor(e){super(),Rb(e),this.l1=null==e||null==e.l1?.01:e.l1,this.l2=null==e||null==e.l2?.01:e.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(e){return Xi((()=>{let t=wu([1]);return this.hasL1&&(t=to(t,eu(ro(this.l1,ao(e))))),this.hasL2&&(t=to(t,eu(ro(this.l2,jm(e))))),t.asScalar()}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(e,t){return new e({l1:t.l1,l2:t.l2})}}_b.className="L1L2",zi(_b);const Db={l1l2:"L1L2"};function Ob(e){return Kf(e)}function Mb(e,t={}){return Yf(e,Li.getMap().classNameMap,t,"regularizer")}function Lb(e){if(null==e)return null;if("string"==typeof e){return Mb({className:e in Db?Db[e]:e,config:{}})}return e instanceof Fb?e:Mb(e)}class zb extends zg{constructor(e){super(null==e?{}:e),this.supportsMasking=!0,null!=e&&(this.maxValue=e.maxValue)}call(e,t){e=$g(e);let n=lc(e);return null!=this.maxValue&&(n=Ko(n,0,this.maxValue)),n}computeOutputShape(e){return e}getConfig(){const e={maxValue:this.maxValue},t=super.getConfig();return Object.assign(e,t),e}}zb.className="ReLU",zi(zb);class Bb extends zg{constructor(e){super(null==e?{}:e),this.DEFAULT_ALPHA=.3,null==e&&(e={}),this.alpha=null==e.alpha?this.DEFAULT_ALPHA:e.alpha}call(e,t){const n=$g(e);return Bl(n,this.alpha)}computeOutputShape(e){return e}getConfig(){const e={alpha:this.alpha},t=super.getConfig();return Object.assign(e,t),e}}Bb.className="LeakyReLU",zi(Bb);class Pb extends zg{constructor(e){if(super(null==e?{}:e),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==e&&(e={}),this.supportsMasking=!0,this.alphaInitializer=bg(e.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=Lb(e.alphaRegularizer),this.alphaConstraint=fm(e.alphaConstraint),null==e.sharedAxes)this.sharedAxes=null;else if(Array.isArray(e.sharedAxes))this.sharedAxes=e.sharedAxes;else{if("number"!=typeof e.sharedAxes)throw new Lf(`Expected sharedAxes to be a number or an array of numbers, but got ${e.sharedAxes}`);this.sharedAxes=[e.sharedAxes]}}build(e){const t=(e=Cg(e)).slice(1);if(null!=this.sharedAxes)for(const e of this.sharedAxes)t[e-1]=1;this.alpha=this.addWeight("alpha",t,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let t=1;t<e.length;++t)n[t]=e[t];this.inputSpec=[new _g({ndim:e.length,axes:n})],this.built=!0}call(e,t){return e=$g(e),Wu(e,this.alpha.read())}getConfig(){const e={alphaInitializer:yg(this.alphaInitializer),alphaRegularizer:Ob(this.alphaRegularizer),alphaConstraint:pm(this.alphaConstraint),sharedAxes:this.sharedAxes},t=super.getConfig();return Object.assign(e,t),e}}Pb.className="PReLU",zi(Pb);class Wb extends zg{constructor(e){if(super(null==e?{}:e),this.DEFAULT_ALPHA=1,null==e&&(e={}),null!=e.alpha&&e.alpha!==this.DEFAULT_ALPHA)throw new zf(`Non-default alpha value (${e.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==e.alpha?this.DEFAULT_ALPHA:e.alpha}call(e,t){const n=$g(e);return Nl(n)}computeOutputShape(e){return e}getConfig(){const e={alpha:this.alpha},t=super.getConfig();return Object.assign(e,t),e}}Wb.className="ELU",zi(Wb);class Vb extends zg{constructor(e){super(null==e?{}:e),this.DEFAULT_THETA=1,null==e&&(e={}),this.theta=null==e.theta?this.DEFAULT_THETA:e.theta}call(e,t){const n=$g(e);return n.mul(Om(n.greater(this.theta),"float32"))}computeOutputShape(e){return e}getConfig(){const e={theta:this.theta},t=super.getConfig();return Object.assign(e,t),e}}Vb.className="ThresholdedReLU",zi(Vb);class Ub extends zg{constructor(e){super(null==e?{}:e),this.DEFAULT_AXIS=1,null==e&&(e={}),this.softmax=(new Ib).apply,this.axis=null==e.axis?this.DEFAULT_AXIS:e.axis}call(e,t){const n=$g(e);return this.softmax(n,this.axis)}computeOutputShape(e){return e}getConfig(){const e={axis:this.axis},t=super.getConfig();return Object.assign(e,t),e}}function Gb(e,t,n){if("number"==typeof e)return Pf(e,t);if(e.length!==t)throw new Lf(`The ${n} argument must be an integer or tuple of ${t} integers. Received: ${e.length} elements.`);for(let r=0;r<t;++r){const a=e[r];if((s=a)!==parseInt(s.toString(),10))throw new Lf(`The ${n} argument must be an integer or tuple of ${t} integers. Received: ${JSON.stringify(e)} including a non-integer number ${a}`)}return e;var s}function Hb(e,t,n,s,r=1){if(null==e)return e;let a;return a="same"===n?e:e-(t+(t-1)*(r-1))+1,Math.floor((a+s-1)/s)}function jb(e,t,n,s){if(null==e)return null;if("valid"===s)e=e*t+_m([n-t,0]);else{if("same"!==s)throw new Lf(`Unsupport padding mode: ${s}.`);e*=t}return e}function qb(e,t){return Xi((()=>(vm(t),"channelsFirst"===t?ri(e,[0,2,3,1]):e)))}function Kb(e,t){return Xi((()=>(vm(t),"channelsFirst"===t?ri(e,[0,2,3,4,1]):e)))}function Xb(e,t,n,s=1,r="valid",a,i=1){return Xi((()=>{if(null==a&&(a="channelsLast"),vm(a),3!==e.shape.length)throw new Lf(`The input of a conv1dWithBias operation should be 3, but is ${e.shape.length} instead.`);if(3!==t.shape.length)throw new Lf(`The kernel for a conv1dWithBias operation should be 3, but is ${t.shape.length} instead`);if(null!=n&&1!==n.shape.length)throw new Lf(`The bias for a conv1dWithBias operation should be 1, but is ${t.shape.length} instead`);if("channelsFirst"===a&&(e=ri(e,[0,2,1])),"causal"===r)throw new zf("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let o=el(e,t,s,"same"===r?"same":"valid","NWC",i);return null!=n&&(o=Km(o,n)),o}))}function Yb(e,t,n,s=[1,1],r="valid",a,i,o=null){return Xi((()=>{if(null==a&&(a="channelsLast"),vm(a),3!==e.rank&&4!==e.rank)throw new Lf(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${e.rank}.`);if(3!==t.rank&&4!==t.rank)throw new Lf(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${e.rank}.`);let l=qb(e,a);if("causal"===r)throw new zf("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return l=dh({x:l,filter:t,strides:s,pad:"same"===r?"same":"valid",dilations:i,dataFormat:"NHWC",bias:n,activation:o}),"channelsFirst"===a&&(l=ri(l,[0,3,1,2])),l}))}function Zb(e,t,n,s=[1,1,1],r="valid",a,i){return Xi((()=>{if(null==a&&(a="channelsLast"),vm(a),4!==e.rank&&5!==e.rank)throw new Lf(`conv3dWithBias expects input to be of rank 4 or 5, but received ${e.rank}.`);if(4!==t.rank&&5!==t.rank)throw new Lf(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${e.rank}.`);let o=Kb(e,a);if("causal"===r)throw new zf("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return o=sl(o,t,s,"same"===r?"same":"valid","NDHWC",i),null!=n&&(o=Km(o,n)),"channelsFirst"===a&&(o=ri(o,[0,4,1,2,3])),o}))}Ub.className="Softmax",zi(Ub);class Jb extends zg{constructor(e,t){if(super(t),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",Jb.verifyArgs(t),this.rank=e,nm(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new zf(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=Gb(t.kernelSize,e,"kernelSize"),this.strides=Gb(null==t.strides?1:t.strides,e,"strides"),this.padding=null==t.padding?"valid":t.padding,Nm(this.padding),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,vm(this.dataFormat),this.activation=Ab(t.activation),this.useBias=null==t.useBias||t.useBias,this.biasInitializer=bg(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=fm(t.biasConstraint),this.biasRegularizer=Lb(t.biasRegularizer),this.activityRegularizer=Lb(t.activityRegularizer),this.dilationRate=Gb(null==t.dilationRate?1:t.dilationRate,e,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new Lf(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new Lf(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new Lf(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(e){if(Wf("kernelSize"in e,"required key 'kernelSize' not in config"),"number"!=typeof e.kernelSize&&!tm(e.kernelSize,"number",1,3))throw new Lf(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(e.kernelSize)}.`)}getConfig(){const e={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:Tb(this.activation),useBias:this.useBias,biasInitializer:yg(this.biasInitializer),biasRegularizer:Ob(this.biasRegularizer),activityRegularizer:Ob(this.activityRegularizer),biasConstraint:pm(this.biasConstraint)},t=super.getConfig();return Object.assign(e,t),e}}class Qb extends Jb{constructor(e,t){super(e,t),this.kernel=null,Qb.verifyArgs(t),this.filters=t.filters,nm(this.filters,"filters"),this.kernelInitializer=bg(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=fm(t.kernelConstraint),this.kernelRegularizer=Lb(t.kernelRegularizer)}build(e){e=Cg(e);const t="channelsFirst"===this.dataFormat?1:e.length-1;if(null==e[t])throw new Lf(`The channel dimension of the input should be defined. Found ${e[t]}`);const n=e[t],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[t]:n}}],this.built=!0}call(e,t){return Xi((()=>{let t;e=$g(e);const n=null==this.bias?null:this.bias.read(),s=rm(this.activation.getClassName());if(null!=s&&2===this.rank)t=Yb(e,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,s);else{if(1===this.rank)t=Xb(e,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)t=Yb(e,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new zf("convolutions greater than 3D are not implemented yet.");t=Zb(e,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(t=this.activation.apply(t))}return t}))}computeOutputShape(e){e=Cg(e);const t=[],n="channelsLast"===this.dataFormat?e.slice(1,e.length-1):e.slice(2);for(let e=0;e<n.length;++e){const s=Hb(n[e],this.kernelSize[e],this.padding,this.strides[e],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[e]);t.push(s)}let s=[e[0]];return"channelsLast"===this.dataFormat?(s=s.concat(t),s.push(this.filters)):(s.push(this.filters),s=s.concat(t)),s}getConfig(){const e={filters:this.filters,kernelInitializer:yg(this.kernelInitializer),kernelRegularizer:Ob(this.kernelRegularizer),kernelConstraint:pm(this.kernelConstraint)},t=super.getConfig();return Object.assign(e,t),e}static verifyArgs(e){if(!("filters"in e)||"number"!=typeof e.filters||e.filters<1)throw new Lf(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(e.filters)}`)}}class ex extends Qb{constructor(e){super(2,e),ex.verifyArgs(e)}getConfig(){const e=super.getConfig();return delete e.rank,e}static verifyArgs(e){if("number"!=typeof e.kernelSize&&!tm(e.kernelSize,"number",1,2))throw new Lf(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(e.kernelSize)}.`)}}ex.className="Conv2D",zi(ex);class tx extends Qb{constructor(e){super(3,e),tx.verifyArgs(e)}getConfig(){const e=super.getConfig();return delete e.rank,e}static verifyArgs(e){if("number"!=typeof e.kernelSize&&(!Array.isArray(e.kernelSize)||1!==e.kernelSize.length&&3!==e.kernelSize.length))throw new Lf(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(e.kernelSize)}.`)}}tx.className="Conv3D",zi(tx);class nx extends ex{constructor(e){if(super(e),this.inputSpec=[new _g({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new Lf(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(e){if(4!==(e=Cg(e)).length)throw new Lf("Input should have rank 4; Received input shape: "+JSON.stringify(e));const t="channelsFirst"===this.dataFormat?1:e.length-1;if(null==e[t])throw new Lf("The channel dimension of the inputs should be defined. Found `None`.");const n=e[t],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new _g({ndim:4,axes:{[t]:n}})],this.built=!0}call(e,t){return Xi((()=>{let t=$g(e);if(4!==t.shape.length)throw new Lf(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${t.shape.length}`);const n=t.shape,s=n[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const i=n[r],o=n[a],l=this.kernelSize[0],u=this.kernelSize[1],c=this.strides[0],h=this.strides[1],p=[s,jb(i,c,l,this.padding),jb(o,h,u,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(t=ri(t,[0,2,3,1]));let d=nl(t,this.kernel.read(),p,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(d=ri(d,[0,3,1,2])),null!=this.bias&&(d=Km(d,this.bias.read(),this.dataFormat)),null!=this.activation&&(d=this.activation.apply(d)),d}))}computeOutputShape(e){const t=(e=Cg(e)).slice();let n,s,r;"channelsFirst"===this.dataFormat?(n=1,s=2,r=3):(n=3,s=1,r=2);const a=this.kernelSize[0],i=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return t[n]=this.filters,t[s]=jb(t[s],o,a,this.padding),t[r]=jb(t[r],l,i,this.padding),t}getConfig(){const e=super.getConfig();return delete e.dilationRate,e}}nx.className="Conv2DTranspose",zi(nx);class sx extends tx{constructor(e){if(super(e),this.inputSpec=[new _g({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new Lf(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(e){if(5!==(e=Cg(e)).length)throw new Lf("Input should have rank 5; Received input shape: "+JSON.stringify(e));const t="channelsFirst"===this.dataFormat?1:e.length-1;if(null==e[t])throw new Lf("The channel dimension of the inputs should be defined. Found `None`.");const n=e[t],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new _g({ndim:5,axes:{[t]:n}})],this.built=!0}call(e,t){return Xi((()=>{let t=$g(e);if(5!==t.shape.length)throw new Lf(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${t.shape.length}`);const n=t.shape,s=n[0];let r,a,i;"channelsFirst"===this.dataFormat?(i=2,r=3,a=4):(i=1,r=2,a=3);const o=n[i],l=n[r],u=n[a],c=this.kernelSize[0],h=this.kernelSize[1],p=this.kernelSize[2],d=this.strides[0],f=this.strides[1],m=this.strides[2],g=[s,jb(o,d,c,this.padding),jb(l,f,h,this.padding),jb(u,m,p,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(t=ri(t,[0,2,3,4,1]));let y=al(t,this.kernel.read(),g,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(y=ri(y,[0,4,1,2,3])),null!==this.bias&&(y=Km(y,this.bias.read(),this.dataFormat)),null!==this.activation&&(y=this.activation.apply(y)),y}))}computeOutputShape(e){const t=(e=Cg(e)).slice();let n,s,r,a;"channelsFirst"===this.dataFormat?(n=1,s=2,r=3,a=4):(n=4,s=1,r=2,a=3);const i=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],c=this.strides[1],h=this.strides[2];return t[n]=this.filters,t[s]=jb(t[s],u,i,this.padding),t[r]=jb(t[r],c,o,this.padding),t[a]=jb(t[a],h,l,this.padding),t}getConfig(){const e=super.getConfig();return delete e.dilationRate,e}}sx.className="Conv3DTranspose",zi(sx);class rx extends Qb{constructor(e,t){if(super(e,t),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==t.filters)throw new Lf("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=t.kernelInitializer||null!=t.kernelRegularizer||null!=t.kernelConstraint)throw new Lf("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=t.padding&&"same"!==t.padding&&"valid"!==t.padding)throw new Lf(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(t.padding)}`);this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=bg(t.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=Lb(t.depthwiseRegularizer),this.depthwiseConstraint=fm(t.depthwiseConstraint),this.pointwiseInitializer=bg(t.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=Lb(t.pointwiseRegularizer),this.pointwiseConstraint=fm(t.pointwiseConstraint)}build(e){if((e=Cg(e)).length<this.rank+2)throw new Lf(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(e)}`);const t="channelsFirst"===this.dataFormat?1:e.length-1;if(null==e[t]||e[t]<0)throw new Lf(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(e[t])}`);const n=e[t],s=this.kernelSize.concat([n,this.depthMultiplier]),r=[];for(let e=0;e<this.rank;++e)r.push(1);r.push(n*this.depthMultiplier,this.filters);const a=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,a,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",r,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,a,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,a,this.biasConstraint):this.bias=null,this.inputSpec=[new _g({ndim:this.rank+2,axes:{[t]:n}})],this.built=!0}call(e,t){return Xi((()=>{let t;if(e=$g(e),1===this.rank)throw new zf("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(e=ri(e,[0,2,3,1])),t=xc(e,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(t=Km(t,this.bias.read(),this.dataFormat)),null!=this.activation&&(t=this.activation.apply(t)),"channelsFirst"===this.dataFormat&&(t=ri(t,[0,3,1,2])),t}))}getConfig(){const e=super.getConfig();return delete e.rank,delete e.kernelInitializer,delete e.kernelRegularizer,delete e.kernelConstraint,e.depthwiseInitializer=yg(this.depthwiseInitializer),e.pointwiseInitializer=yg(this.pointwiseInitializer),e.depthwiseRegularizer=Ob(this.depthwiseRegularizer),e.pointwiseRegularizer=Ob(this.pointwiseRegularizer),e.depthwiseConstraint=pm(this.depthwiseConstraint),e.pointwiseConstraint=pm(this.pointwiseConstraint),e}}rx.className="SeparableConv";class ax extends rx{constructor(e){super(2,e)}}ax.className="SeparableConv2D",zi(ax);class ix extends Qb{constructor(e){super(1,e),ix.verifyArgs(e),this.inputSpec=[{ndim:3}]}getConfig(){const e=super.getConfig();return delete e.rank,delete e.dataFormat,e}static verifyArgs(e){if("number"!=typeof e.kernelSize&&!tm(e.kernelSize,"number",1,1))throw new Lf(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(e.kernelSize)}.`)}}ix.className="Conv1D",zi(ix);class ox extends zg{constructor(e){super(e),"number"==typeof e.cropping?this.cropping=[[e.cropping,e.cropping],[e.cropping,e.cropping]]:"number"==typeof e.cropping[0]?this.cropping=[[e.cropping[0],e.cropping[0]],[e.cropping[1],e.cropping[1]]]:this.cropping=e.cropping,this.dataFormat=void 0===e.dataFormat?"channelsLast":e.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(e){return"channelsFirst"===this.dataFormat?[e[0],e[1],e[2]-this.cropping[0][0]-this.cropping[0][1],e[3]-this.cropping[1][0]-this.cropping[1][1]]:[e[0],e[1]-this.cropping[0][0]-this.cropping[0][1],e[2]-this.cropping[1][0]-this.cropping[1][1],e[3]]}call(e,t){return Xi((()=>{if(e=$g(e),"channelsLast"===this.dataFormat){const t=Bm(e,this.cropping[0][0],e.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return Bm(t,this.cropping[1][0],e.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const t=Bm(e,this.cropping[0][0],e.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return Bm(t,this.cropping[1][0],e.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const e={cropping:this.cropping,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(e,t),e}}ox.className="Cropping2D",zi(ox);class lx extends zg{constructor(e){var t;super(e),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==e.size?this.DEFAULT_SIZE:e.size,this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,vm(this.dataFormat),this.interpolation=null==e.interpolation?"nearest":e.interpolation,t=this.interpolation,em(ym,"InterpolationFormat",t)}computeOutputShape(e){if("channelsFirst"===this.dataFormat){const t=null==e[2]?null:this.size[0]*e[2],n=null==e[3]?null:this.size[1]*e[3];return[e[0],e[1],t,n]}{const t=null==e[1]?null:this.size[0]*e[1],n=null==e[2]?null:this.size[1]*e[2];return[e[0],t,n,e[3]]}}call(e,t){return Xi((()=>{let t=$g(e);const n=t.shape;if("channelsFirst"===this.dataFormat){t=ri(t,[0,2,3,1]);const e=this.size[0]*n[2],s=this.size[1]*n[3],r="nearest"===this.interpolation?t.resizeNearestNeighbor([e,s]):t.resizeBilinear([e,s]);return ri(r,[0,3,1,2])}{const e=this.size[0]*n[1],s=this.size[1]*n[2];return"nearest"===this.interpolation?t.resizeNearestNeighbor([e,s]):t.resizeBilinear([e,s])}}))}getConfig(){const e={size:this.size,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(e,t),e}}lx.className="UpSampling2D",zi(lx);class ux extends Jb{constructor(e){super(2,e),this.depthwiseKernel=null,this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=bg(e.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=fm(e.depthwiseConstraint),this.depthwiseRegularizer=Lb(e.depthwiseRegularizer)}build(e){if((e=Cg(e)).length<4)throw new Lf(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(e)}.`);const t="channelsFirst"===this.dataFormat?1:3;if(null==e[t]||e[t]<0)throw new Lf(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${e[t]}).`);const n=e[t],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(e,t){return Xi((()=>{let t=function(e,t,n=[1,1],s="valid",r,a){return Xi((()=>{null==r&&(r="channelsLast"),vm(r);let i=qb(e,r);if(4!==e.rank)throw new Lf(`Input for depthwiseConv2d is required to be 4-D, but is instead ${e.rank}-D`);if(4!==t.rank)throw new Lf(`depthwiseKernel is required to be 4-D, but is instead ${t.rank}-D`);return i=hl(i,t,n,"same"===s?"same":"valid","NHWC",a),"channelsFirst"===r&&(i=ri(i,[0,3,1,2])),i}))}(e=$g(e),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(t=Km(t,this.bias.read(),this.dataFormat)),null!=this.activation&&(t=this.activation.apply(t)),t}))}computeOutputShape(e){e=Cg(e);const t="channelsFirst"===this.dataFormat?e[2]:e[1],n="channelsFirst"===this.dataFormat?e[3]:e[2],s="channelsFirst"===this.dataFormat?e[1]*this.depthMultiplier:e[3]*this.depthMultiplier,r=Hb(t,this.kernelSize[0],this.padding,this.strides[0]),a=Hb(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[e[0],s,r,a]:[e[0],r,a,s]}getConfig(){const e=super.getConfig();return e.depthMultiplier=this.depthMultiplier,e.depthwiseInitializer=yg(this.depthwiseInitializer),e.depthwiseRegularizer=Ob(this.depthwiseRegularizer),e.depthwiseConstraint=pm(this.depthwiseRegularizer),e}}function cx(e,t,n,s){if(Array.isArray(e)){if(null!=t||null!=n)throw new Lf("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=e.slice(e.length-s,e.length),e=e.slice(0,e.length-s)),e.length>1&&(t=e.slice(1,e.length)),e=e[0]}function r(e){return null==e||Array.isArray(e)?e:[e]}return{inputs:e,initialState:t=r(t),constants:n=r(n)}}function hx(e,t,n,s=!1,r,a,i=!1,o=!1){return Xi((()=>{const l=t.shape.length;if(l<3)throw new Lf(`Input should be at least 3D, but is ${l}D.`);const u=[1,0].concat(Dm(2,l));if(t=ri(t,u),null!=a)throw new zf("The rnn() functoin of the deeplearn.js backend does not support constants yet.");i&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=r.asType("bool").asType("float32")).rank===l-1&&(r=$l(r,-1)),r=ri(r,u)),s&&(t=cc(t,0),null!=r&&(r=cc(r,0)));const c=[];let h,p=n;const d=t.shape[0],f=qc(t);let m,g;null!=r&&(m=qc(r));for(let t=0;t<d;++t){const n=f[t],s=Xi((()=>e(n,p)));if(null==r)h=s[0],p=s[1];else{const e=Xi((()=>{const e=m[t],n=Ru(e).sub(e);return{output:s[0].mul(e).add(p[0].mul(n)),newStates:p.map(((t,r)=>s[1][r].mul(e).add(t.mul(n))))}}));h=e.output,p=e.newStates}o&&c.push(h)}if(o){g=Lc(c,1)}return[h,g,p]}))}ux.className="DepthwiseConv2D",zi(ux);class px extends zg{constructor(e){let t;if(super(e),null==e.cell)throw new Lf("cell property is missing for the constructor of RNN.");if(t=Array.isArray(e.cell)?new wx({cells:e.cell}):e.cell,null==t.stateSize)throw new Lf("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=t,this.returnSequences=null!=e.returnSequences&&e.returnSequences,this.returnState=null!=e.returnState&&e.returnState,this.goBackwards=null!=e.goBackwards&&e.goBackwards,this._stateful=null!=e.stateful&&e.stateful,this.unroll=null!=e.unroll&&e.unroll,this.supportsMasking=!0,this.inputSpec=[new _g({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return Dm(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((e=>null))}return this.states_}setStates(e){this.states_=e}computeOutputShape(e){Ig(e)&&(e=e[0]),e=e;let t=this.cell.stateSize;Array.isArray(t)||(t=[t]);const n=t[0];let s;if(s=this.returnSequences?[e[0],e[1],n]:[e[0],n],this.returnState){const n=[];for(const s of t)n.push([e[0],s]);return[s].concat(n)}return s}computeMask(e,t){return Xi((()=>{Array.isArray(t)&&(t=t[0]);const e=this.returnSequences?t:null;if(this.returnState){const t=this.states.map((e=>null));return[e].concat(t)}return e}))}get states(){if(null==this.states_){const e=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,t=[];for(let n=0;n<e;++n)t.push(null);return t}return this.states_}set states(e){this.states_=e}build(e){if(null!=this.numConstants)throw new zf("Constants support is not implemented in RNN yet.");Ig(e)&&(e=e[0]),e=e;const t=this.stateful?e[0]:null,n=e.slice(2);this.inputSpec[0]=new _g({shape:[t,null,...n]});const s=[e[0]].concat(e.slice(2));let r;if(this.cell.build(s),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!d(this.stateSpec.map((e=>e.shape[e.shape.length-1])),r))throw new Lf(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((e=>new _g({shape:[null,e]})));this.stateful&&this.resetStates()}resetStates(e,t=!1){Xi((()=>{if(!this.stateful)throw new Of("Cannot call resetStates() on an RNN Layer that is not stateful.");const n=this.inputSpec[0].shape[0];if(null==n)throw new Lf("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((e=>wu([n,e]))):this.states_=[wu([n,this.cell.stateSize])];else if(null==e)Yi(this.states_),null!=this.keptStates&&(Yi(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((e=>wu([n,e]))):this.states_[0]=wu([n,this.cell.stateSize]);else{if(Array.isArray(e)||(e=[e]),e.length!==this.states_.length)throw new Lf(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${e.length} state value(s). Input received: ${e}`);!0===t?this.keptStates.push(this.states_.slice()):Yi(this.states_);for(let t=0;t<this.states_.length;++t){const s=e[t],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[t]:this.cell.stateSize,a=[n,r];if(!d(s.shape,a))throw new Lf(`State ${t} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${s.shape}`);this.states_[t]=s}}this.states_=this.states_.map((e=>Zi(e.clone())))}))}apply(e,t){let n=null==t?null:t.initialState,s=null==t?null:t.constants;null==t&&(t={});const r=cx(e,n,s,this.numConstants);e=r.inputs,n=r.initialState,s=r.constants;let a=[],i=[];if(null!=n){t.initialState=n,a=a.concat(n),this.stateSpec=[];for(const e of n)this.stateSpec.push(new _g({shape:e.shape}));i=i.concat(this.stateSpec)}null!=s&&(t.constants=s,a=a.concat(s),this.numConstants=s.length);if(a[0]instanceof Dg){const n=[e].concat(a),s=this.inputSpec.concat(i),r=this.inputSpec;this.inputSpec=s;const o=super.apply(n,t);return this.inputSpec=r,o}return super.apply(e,t)}call(e,t){return Xi((()=>{const n=null==t?null:t.mask,s=null==t?null:t.training;let r=null==t?null:t.initialState;e=$g(e),null==r&&(r=this.stateful?this.states_:this.getInitialState(e));const a=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(r.length!==a)throw new Lf(`RNN Layer has ${a} state(s) but was passed ${r.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const i={training:s},o=hx(((e,t)=>{const n=this.cell.call([e].concat(t),i);return[n[0],n.slice(1)]}),e,r,this.goBackwards,n,null,this.unroll,this.returnSequences),l=o[0],u=o[1],c=o[2];this.stateful&&this.resetStates(c,s);const h=this.returnSequences?u:l;return this.returnState?[h].concat(c):h}))}getInitialState(e){return Xi((()=>{let t=wu(e.shape);return t=eu(t,[1,2]),t=Mm(t),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((e=>e>1?Vm(t,[1,e]):t)):this.cell.stateSize>1?[Vm(t,[1,this.cell.stateSize])]:[t]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(e){super.setFastWeightInitDuringBuild(e),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(e)}getConfig(){const e=super.getConfig(),t={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(t.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===px.className&&(t.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,e,t)}static fromConfig(e,t,n={}){const s=Qg(t.cell,n);return new e(Object.assign(t,{cell:s}))}}px.className="RNN",zi(px);class dx extends zg{}class fx extends dx{constructor(e){super(e),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=e.units,nm(this.units,"units"),this.activation=Ab(null==e.activation?this.DEFAULT_ACTIVATION:e.activation),this.useBias=null==e.useBias||e.useBias,this.kernelInitializer=bg(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=bg(e.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=bg(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Lb(e.kernelRegularizer),this.recurrentRegularizer=Lb(e.recurrentRegularizer),this.biasRegularizer=Lb(e.biasRegularizer),this.kernelConstraint=fm(e.kernelConstraint),this.recurrentConstraint=fm(e.recurrentConstraint),this.biasConstraint=fm(e.biasConstraint),this.dropout=Fm([1,_m([0,null==e.dropout?0:e.dropout])]),this.recurrentDropout=Fm([1,_m([0,null==e.recurrentDropout?0:e.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(e){e=Cg(e),this.kernel=this.addWeight("kernel",[e[e.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(e,t){return Xi((()=>{if(2!==(e=e).length)throw new Lf(`SimpleRNNCell expects 2 input Tensors, got ${e.length}.`);let n=e[1];e=e[0];const s=null!=t.training&&t.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=kx({ones:()=>Ru(e),rate:this.dropout,training:s})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=kx({ones:()=>Ru(n),rate:this.recurrentDropout,training:s}));const a=this.dropoutMask,i=this.recurrentDropoutMask;r=Gm(null!=a?ro(e,a):e,this.kernel.read()),null!=this.bias&&(r=Km(r,this.bias.read())),null!=i&&(n=ro(n,i));let o=to(r,Gm(n,this.recurrentKernel.read()));return null!=this.activation&&(o=this.activation.apply(o)),[o,o]}))}getConfig(){const e=super.getConfig(),t={units:this.units,activation:Tb(this.activation),useBias:this.useBias,kernelInitializer:yg(this.kernelInitializer),recurrentInitializer:yg(this.recurrentInitializer),biasInitializer:yg(this.biasInitializer),kernelRegularizer:Ob(this.kernelRegularizer),recurrentRegularizer:Ob(this.recurrentRegularizer),biasRegularizer:Ob(this.biasRegularizer),activityRegularizer:Ob(this.activityRegularizer),kernelConstraint:pm(this.kernelConstraint),recurrentConstraint:pm(this.recurrentConstraint),biasConstraint:pm(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},e,t)}}fx.className="SimpleRNNCell",zi(fx);class mx extends px{constructor(e){e.cell=new fx(e),super(e)}call(e,t){return Xi((()=>{null!=this.cell.dropoutMask&&(Yi(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(Yi(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==t?null:t.mask,s=null==t?null:t.training,r=null==t?null:t.initialState;return super.call(e,{mask:n,training:s,initialState:r})}))}static fromConfig(e,t){return new e(t)}}mx.className="SimpleRNN",zi(mx);class gx extends dx{constructor(e){if(super(e),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",e.resetAfter)throw new Lf("GRUCell does not support reset_after parameter set to true.");this.units=e.units,nm(this.units,"units"),this.activation=Ab(void 0===e.activation?this.DEFAULT_ACTIVATION:e.activation),this.recurrentActivation=Ab(void 0===e.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:e.recurrentActivation),this.useBias=null==e.useBias||e.useBias,this.kernelInitializer=bg(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=bg(e.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=bg(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Lb(e.kernelRegularizer),this.recurrentRegularizer=Lb(e.recurrentRegularizer),this.biasRegularizer=Lb(e.biasRegularizer),this.kernelConstraint=fm(e.kernelConstraint),this.recurrentConstraint=fm(e.recurrentConstraint),this.biasConstraint=fm(e.biasConstraint),this.dropout=Fm([1,_m([0,null==e.dropout?0:e.dropout])]),this.recurrentDropout=Fm([1,_m([0,null==e.recurrentDropout?0:e.recurrentDropout])]),this.implementation=e.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(e){const t=(e=Cg(e))[e.length-1];this.kernel=this.addWeight("kernel",[t,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(e,t){return Xi((()=>{if(2!==(e=e).length)throw new Lf(`GRUCell expects 2 input Tensors (inputs, h, c), got ${e.length}.`);const n=null!=t.training&&t.training;let s=e[1];e=e[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=kx({ones:()=>Ru(e),rate:this.dropout,training:n,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=kx({ones:()=>Ru(s),rate:this.recurrentDropout,training:n,count:3}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let i,o,l;0<this.dropout&&this.dropout<1&&(e=ro(e,r[0]));let u=Gm(e,this.kernel.read());this.useBias&&(u=Km(u,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=ro(s,a[0]));const c=this.recurrentKernel.read(),[h,p]=Fc(c,[2*this.units,this.units],c.rank-1),d=Gm(s,h),[f,m,g]=Fc(u,3,u.rank-1),[y,b]=Fc(d,2,d.rank-1);i=this.recurrentActivation.apply(to(f,y)),o=this.recurrentActivation.apply(to(m,b));const x=Gm(ro(o,s),p);l=this.activation.apply(to(g,x));const w=to(ro(i,s),ro(to(1,Xl(i)),l));return[w,w]}))}getConfig(){const e=super.getConfig(),t={units:this.units,activation:Tb(this.activation),recurrentActivation:Tb(this.recurrentActivation),useBias:this.useBias,kernelInitializer:yg(this.kernelInitializer),recurrentInitializer:yg(this.recurrentInitializer),biasInitializer:yg(this.biasInitializer),kernelRegularizer:Ob(this.kernelRegularizer),recurrentRegularizer:Ob(this.recurrentRegularizer),biasRegularizer:Ob(this.biasRegularizer),activityRegularizer:Ob(this.activityRegularizer),kernelConstraint:pm(this.kernelConstraint),recurrentConstraint:pm(this.recurrentConstraint),biasConstraint:pm(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},e,t)}}gx.className="GRUCell",zi(gx);class yx extends px{constructor(e){0===e.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),e.cell=new gx(e),super(e)}call(e,t){return Xi((()=>{null!=this.cell.dropoutMask&&(Yi(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(Yi(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==t?null:t.mask,s=null==t?null:t.training,r=null==t?null:t.initialState;return super.call(e,{mask:n,training:s,initialState:r})}))}static fromConfig(e,t){return 0===t.implmentation&&(t.implementation=1),new e(t)}}yx.className="GRU",zi(yx);class bx extends dx{constructor(e){super(e),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=e.units,nm(this.units,"units"),this.activation=Ab(void 0===e.activation?this.DEFAULT_ACTIVATION:e.activation),this.recurrentActivation=Ab(void 0===e.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:e.recurrentActivation),this.useBias=null==e.useBias||e.useBias,this.kernelInitializer=bg(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=bg(e.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=bg(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=e.unitForgetBias,this.kernelRegularizer=Lb(e.kernelRegularizer),this.recurrentRegularizer=Lb(e.recurrentRegularizer),this.biasRegularizer=Lb(e.biasRegularizer),this.kernelConstraint=fm(e.kernelConstraint),this.recurrentConstraint=fm(e.recurrentConstraint),this.biasConstraint=fm(e.biasConstraint),this.dropout=Fm([1,_m([0,null==e.dropout?0:e.dropout])]),this.recurrentDropout=Fm([1,_m([0,null==e.recurrentDropout?0:e.recurrentDropout])]),this.implementation=e.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(e){var t;const n=(e=Cg(e))[e.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const e=this.biasInitializer,n=this.units;s=new((t=class extends Qm{apply(t,s){const r=e.apply([n]),a=(new tg).apply([n]),i=e.apply([2*n]);return Wm(Wm(r,a),i)}}).className="CustomInit",t)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(e,t){return Xi((()=>{const n=null!=t.training&&t.training;if(3!==(e=e).length)throw new Lf(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${e.length}.`);let s=e[1];const r=e[2];e=e[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=kx({ones:()=>Ru(e),rate:this.dropout,training:n,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=kx({ones:()=>Ru(s),rate:this.recurrentDropout,training:n,count:4}));const a=this.dropoutMask,i=this.recurrentDropoutMask;let o,l,u,c;0<this.dropout&&this.dropout<1&&(e=ro(e,a[0]));let h=Gm(e,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=ro(s,i[0])),h=to(h,Gm(s,this.recurrentKernel.read())),this.useBias&&(h=Km(h,this.bias.read()));const[p,d,f,m]=Fc(h,4,h.rank-1);o=this.recurrentActivation.apply(p),l=this.recurrentActivation.apply(d),u=to(ro(l,r),ro(o,this.activation.apply(f))),c=this.recurrentActivation.apply(m);const g=ro(c,this.activation.apply(u));return[g,g,u]}))}getConfig(){const e=super.getConfig(),t={units:this.units,activation:Tb(this.activation),recurrentActivation:Tb(this.recurrentActivation),useBias:this.useBias,kernelInitializer:yg(this.kernelInitializer),recurrentInitializer:yg(this.recurrentInitializer),biasInitializer:yg(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:Ob(this.kernelRegularizer),recurrentRegularizer:Ob(this.recurrentRegularizer),biasRegularizer:Ob(this.biasRegularizer),activityRegularizer:Ob(this.activityRegularizer),kernelConstraint:pm(this.kernelConstraint),recurrentConstraint:pm(this.recurrentConstraint),biasConstraint:pm(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},e,t)}}bx.className="LSTMCell",zi(bx);class xx extends px{constructor(e){0===e.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),e.cell=new bx(e),super(e)}call(e,t){return Xi((()=>{null!=this.cell.dropoutMask&&(Yi(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(Yi(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==t?null:t.mask,s=null==t?null:t.training,r=null==t?null:t.initialState;return super.call(e,{mask:n,training:s,initialState:r})}))}static fromConfig(e,t){return 0===t.implmentation&&(t.implementation=1),new e(t)}}xx.className="LSTM",zi(xx);class wx extends dx{constructor(e){super(e),this.cells=e.cells}get stateSize(){const e=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?e.push(...t.stateSize):e.push(t.stateSize);return e}call(e,t){return Xi((()=>{let n=(e=e).slice(1);const s=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?s.push(n.splice(0,e.stateSize.length)):s.push(n.splice(0,1));s.reverse();const r=[];let a;for(let i=0;i<this.cells.length;++i){const o=this.cells[i];n=s[i],a=0===i?[e[0]].concat(n):[a[0]].concat(n),a=o.call(a,t),r.push(a.slice(1))}n=[];for(const e of r.slice().reverse())n.push(...e);return[a[0]].concat(n)}))}build(e){let t;Ig(e)&&(e=e[0]),e=e,this.cells.forEach(((n,s)=>{$m(`RNNCell_${s}`,(()=>{n.build(e),t=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,e=[e[0],t]}))})),this.built=!0}getConfig(){const e=super.getConfig(),t={cells:this.cells.map((e=>({className:e.getClassName(),config:e.getConfig()})))};return Object.assign({},e,t)}static fromConfig(e,t,n={}){const s=[];for(const e of t.cells)s.push(Qg(e,n));return new e({cells:s})}get trainableWeights(){if(!this.trainable)return[];const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e}get nonTrainableWeights(){const e=[];for(const t of this.cells)e.push(...t.nonTrainableWeights);if(!this.trainable){const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t.concat(e)}return e}getWeights(){const e=[];for(const t of this.cells)e.push(...t.weights);return Rg(e)}setWeights(e){const t=[];for(const n of this.cells){const s=n.weights.length,r=e.splice(s);for(let e=0;e<n.weights.length;++e)t.push([n.weights[e],r[e]])}Fg(t)}}function kx(e){const{ones:t,rate:n,training:s=!1,count:r=1}=e,a=()=>Xm(t(),n),i=()=>Ym(a,t,s);if(!r||r<=1)return Zi(i().clone());return Array(r).fill(void 0).map(i).map((e=>Zi(e.clone())))}wx.className="StackedRNNCells",zi(wx);var vx=function(e,t){var n={};for(var s in e)Object.prototype.hasOwnProperty.call(e,s)&&t.indexOf(s)<0&&(n[s]=e[s]);if(null!=e&&"function"==typeof Object.getOwnPropertySymbols){var r=0;for(s=Object.getOwnPropertySymbols(e);r<s.length;r++)t.indexOf(s[r])<0&&Object.prototype.propertyIsEnumerable.call(e,s[r])&&(n[s[r]]=e[s[r]])}return n};class Nx extends px{constructor(e){if(e.unroll)throw new zf("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(e.cell))throw new zf("It is not possible at the moment to stack convolutional cells.");super(e),this.inputSpec=[new _g({ndim:5})]}call(e,t){return Xi((()=>{if(null!=this.cell.dropoutMask&&(Yi(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(Yi(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),t&&t.constants)throw new Lf("ConvRNN2D cell does not support constants");const n=null==t?null:t.mask,s=null==t?null:t.training,r=null==t?null:t.initialState;return super.call(e,{mask:n,training:s,initialState:r})}))}computeOutputShape(e){let t=this.computeSingleOutputShape(e);return this.returnSequences||(t=[t[0],...t.slice(2)]),this.returnState&&(t=[t,...Array(2).fill([e[0],...t.slice(-3)])]),t}getInitialState(e){return Xi((()=>{const{stateSize:t}=this.cell,n=e.shape,s=this.computeSingleOutputShape(n),r=wu([s[0],...s.slice(2)]);return Array.isArray(t)?Array(t.length).fill(r):[r]}))}resetStates(e,t=!1){Xi((()=>{if(!this.stateful)throw new Of("Cannot call resetStates() on an RNN Layer that is not stateful.");const n=this.inputSpec[0].shape,s=this.computeSingleOutputShape(n),r=[s[0],...s.slice(2)];if(null==n[0])throw new Lf("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>wu(r))):this.states_=[wu(r)];else if(null==e)Yi(this.states_),null!=this.keptStates&&(Yi(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>wu(r))):this.states_[0]=wu(r);else{if(Array.isArray(e)||(e=[e]),e.length!==this.states_.length)throw new Lf(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${e.length} state value(s). Input received: ${e}`);t?this.keptStates.push(this.states_.slice()):Yi(this.states_);for(let t=0;t<this.states_.length;++t){const n=e[t],s=r;if(!d(n.shape,s))throw new Lf(`State ${t} is incompatible with layer ${this.name}: expected shape=${s}, received shape=${n.shape}`);this.states_[t]=n}}this.states_=this.states_.map((e=>Zi(e.clone())))}))}computeSingleOutputShape(e){const{dataFormat:t,filters:n,kernelSize:s,padding:r,strides:a,dilationRate:i}=this.cell,o="channelsFirst"===t,l=e[o?3:2],u=e[o?4:3],c=Hb(l,s[0],r,a[0],i[0]),h=Hb(u,s[1],r,a[1],i[1]);return[...e.slice(0,2),...o?[n,c,h]:[c,h,n]]}}Nx.className="ConvRNN2D";class Ix extends bx{constructor(e){const{filters:t,kernelSize:n,strides:s,padding:r,dataFormat:a,dilationRate:i}=e;super(Object.assign({},e,{units:t})),this.filters=t,nm(this.filters,"filters"),this.kernelSize=Gb(n,2,"kernelSize"),this.kernelSize.forEach((e=>nm(e,"kernelSize"))),this.strides=Gb(s||1,2,"strides"),this.strides.forEach((e=>nm(e,"strides"))),this.padding=r||"valid",Nm(this.padding),this.dataFormat=a||"channelsLast",vm(this.dataFormat),this.dilationRate=Gb(i||1,2,"dilationRate"),this.dilationRate.forEach((e=>nm(e,"dilationRate")))}build(e){var t;e=Cg(e);const n="channelsFirst"===this.dataFormat?1:e.length-1;if(null==e[n])throw new Lf(`The channel dimension of the input should be defined. Found ${e[n]}`);const s=e[n],r=this.kernelSize.concat([s,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const a=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",a,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let e;if(this.unitForgetBias){const n=this.biasInitializer,s=this.filters;e=new((t=class extends Qm{apply(e,t){return Pm([n.apply([s]),ku([s]),n.apply([2*s])])}}).className="CustomInit",t)}else e=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,e,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(e,t){return Xi((()=>{if(3!==e.length)throw new Lf(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${e.length}.`);const n=t.training||!1,s=e[0],r=e[1],a=e[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=kx({ones:()=>Ru(s),rate:this.dropout,training:n,count:4}));const i=this.dropoutMask,o=(e,t,n)=>t&&t[n]?ro(t[n],e):e;let l=o(s,i,0),u=o(s,i,1),c=o(s,i,2),h=o(s,i,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=kx({ones:()=>Ru(r),rate:this.recurrentDropout,training:n,count:4}));const p=this.recurrentDropoutMask;let d=o(r,p,0),f=o(r,p,1),m=o(r,p,2),g=o(r,p,3);const[y,b,x,w]=Fc(this.kernel.read(),4,3),[k,v,N,I]=this.useBias?Fc(this.bias.read(),4):[null,null,null,null];l=this.inputConv(l,y,k,this.padding),u=this.inputConv(u,b,v,this.padding),c=this.inputConv(c,x,N,this.padding),h=this.inputConv(h,w,I,this.padding);const[S,$,C,T]=Fc(this.recurrentKernel.read(),4,3);d=this.recurrentConv(d,S),f=this.recurrentConv(f,$),m=this.recurrentConv(m,C),g=this.recurrentConv(g,T);const E=this.recurrentActivation.apply(to(l,d)),A=this.recurrentActivation.apply(to(u,f)),R=to(ro(A,a),ro(E,this.activation.apply(to(c,m)))),F=ro(this.recurrentActivation.apply(to(h,g)),this.activation.apply(R));return[F,F,R]}))}getConfig(){const e=super.getConfig(),{units:t}=e,n=vx(e,["units"]),s={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},n,s)}inputConv(e,t,n,s){const r=Qo(e,t,this.strides,s||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?Km(r,n,this.dataFormat):r}recurrentConv(e,t){return Qo(e,t,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Ix.className="ConvLSTM2DCell",zi(Ix);class Sx extends Nx{constructor(e){const t=new Ix(e);super(Object.assign({},e,{cell:t}))}static fromConfig(e,t){return new e(t)}}Sx.className="ConvLSTM2D",zi(Sx);class $x extends zg{constructor(e){super(e),this.rate=Math.max(Math.min(e.rate,1),0),this.noiseShape=e.noiseShape,this.seed=e.seed,this.supportsMasking=!0}getNoiseShape(e){if(null==this.noiseShape)return this.noiseShape;const t=e.shape,n=[];for(let e=0;e<this.noiseShape.length;++e)n.push(null==this.noiseShape[e]?t[e]:this.noiseShape[e]);return n}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e);if(0<this.rate&&this.rate<1){const e=null!=t.training&&t.training,s=this.getNoiseShape(n);return Ym((()=>Xm(n,this.rate,s,this.seed)),(()=>n),e)}return e}))}getConfig(){const e={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},t=super.getConfig();return Object.assign(e,t),e}dispose(){return super.dispose()}}$x.className="Dropout",zi($x);class Cx extends $x{constructor(e){super(e),this.inputSpec=[{ndim:3}]}getNoiseShape(e){const t=e.shape;return[t[0],1,t[2]]}}Cx.className="SpatialDropout1D",zi(Cx);class Tx extends zg{constructor(e){if(super(e),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==e.batchInputShape&&null==e.inputShape&&null!=e.inputDim){let t=null;null!=e.batchSize&&(t=e.batchSize),this.batchInputShape=[t,e.inputDim]}this.units=e.units,nm(this.units,"units"),this.activation=Ab(e.activation),null!=e.useBias&&(this.useBias=e.useBias),this.kernelInitializer=bg(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=bg(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=fm(e.kernelConstraint),this.biasConstraint=fm(e.biasConstraint),this.kernelRegularizer=Lb(e.kernelRegularizer),this.biasRegularizer=Lb(e.biasRegularizer),this.activityRegularizer=Lb(e.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(e){const t=(e=Cg(e))[e.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[t,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:t}}],this.built=!0}computeOutputShape(e){const t=(e=Cg(e)).slice();return t[t.length-1]=this.units,t}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e),s=rm(this.activation.getClassName());let r;return null!=s?r=Gm(n,this.kernel.read(),s,this.bias?this.bias.read():null):(r=Gm(n,this.kernel.read()),null!=this.bias&&(r=Km(r,this.bias.read())),null!=this.activation&&(r=this.activation.apply(r))),r}))}getConfig(){const e={units:this.units,activation:Tb(this.activation),useBias:this.useBias,kernelInitializer:yg(this.kernelInitializer),biasInitializer:yg(this.biasInitializer),kernelRegularizer:Ob(this.kernelRegularizer),biasRegularizer:Ob(this.biasRegularizer),activityRegularizer:Ob(this.activityRegularizer),kernelConstraint:pm(this.kernelConstraint),biasConstraint:pm(this.biasConstraint)},t=super.getConfig();return Object.assign(e,t),e}}Tx.className="Dense",zi(Tx);class Ex extends zg{constructor(e){super(e=e||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=e.dataFormat}computeOutputShape(e){e=Cg(e);for(const t of e.slice(1))if(null==t)throw new Lf(`The shape of the input to "Flatten" is not fully defined (got ${e.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[e[0],Rm(e,1)]}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);let n=$g(e);if("channelsFirst"===this.dataFormat&&n.rank>1){const e=[0];for(let t=2;t<n.rank;++t)e.push(t);e.push(1),n=n.transpose(e)}return function(e){if(e.rank<=1)throw new Lf(`batchFlatten requires a minimum rank of 2. Got rank: ${e.rank}.`);const t=[e.shape[0],Rm(e.shape,1)];return e.reshape(t)}(n)}))}getConfig(){const e={};null!=this.dataFormat&&(e.dataFormat=this.dataFormat);const t=super.getConfig();return Object.assign(e,t),e}}Ex.className="Flatten",zi(Ex);class Ax extends zg{constructor(e){super(e),this.supportsMasking=!0,this.activation=Ab(e.activation)}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e);return this.activation.apply(n)}))}getConfig(){const e={activation:Tb(this.activation)},t=super.getConfig();return Object.assign(e,t),e}}Ax.className="Activation",zi(Ax);class Rx extends zg{constructor(e){super(e),this.n=e.n,this.inputSpec=[{ndim:2}]}computeOutputShape(e){return[e[0],this.n,e[1]]}call(e,t){return Xi((()=>{return e=$g(e),t=e,n=this.n,Xi((()=>{if(2!==t.shape.length)throw new Lf(`repeat() expects a rank-2 tensor, but received a rank-${t.shape.length} tensor.`);return Vm(Mm(t,1),[1,n,1])}));var t,n}))}getConfig(){const e={n:this.n},t=super.getConfig();return Object.assign(e,t),e}}Rx.className="RepeatVector",zi(Rx);class Fx extends zg{constructor(e){super(e),this.targetShape=e.targetShape;for(let e=0;e<this.targetShape.length;++e)this.isUnknown(this.targetShape[e])&&(this.targetShape[e]=null)}isUnknown(e){return e<0||null==e}fixUnknownDimension(e,t){const n="Total size of new array must be unchanged.",s=t.slice();let r=1,a=null;for(let e=0;e<s.length;++e){const t=s[e];if(this.isUnknown(t)){if(null!==a)throw new Lf("Can only specifiy one unknown dimension.");a=e}else r*=t}const i=Rm(e);if(null!==a){if(0===r||i%r!=0)throw new Lf(n);s[a]=i/r}else if(i!==r)throw new Lf(n);return s}computeOutputShape(e){let t=!1;for(let n=0;n<e.length;++n)if(this.isUnknown(e[n])){t=!0;break}return t?e.slice(0,1).concat(this.targetShape):e.slice(0,1).concat(this.fixUnknownDimension(e.slice(1),this.targetShape))}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e),s=n.shape,r=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return n.reshape(r)}))}getConfig(){const e={targetShape:this.targetShape},t=super.getConfig();return Object.assign(e,t),e}}Fx.className="Reshape",zi(Fx);class _x extends zg{constructor(e){if(super(e),null==e.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(e.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${e.dims} instead.`);const t=Dm(1,e.dims.length+1);if(!d(e.dims.slice().sort(),t))throw new Error("Invalid permutation `dims`: "+JSON.stringify(e.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=e.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new _g({ndim:this.dims.length+1})]}computeOutputShape(e){const t=(e=Cg(e)).slice();return this.dims.forEach(((n,s)=>{t[s+1]=e[n]})),t}call(e,t){return ri($g(e),this.dimsIncludingBatch)}getConfig(){const e={dims:this.dims},t=super.getConfig();return Object.assign(e,t),e}}_x.className="Permute",zi(_x);class Dx extends zg{constructor(e){super(null==e?{}:e),this.supportsMasking=!0,this.maskValue=null!=e?null==e.maskValue?0:e.maskValue:0}computeOutputShape(e){return e}getConfig(){const e=super.getConfig(),t={maskValue:this.maskValue};return Object.assign(t,e),t}computeMask(e,t){const n=$g(e);return co(Au(n,this.maskValue),-1)}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e),s=co(Au(n,this.maskValue),-1,!0);return n.mul(s.asType(n.dtype))}))}}Dx.className="Masking",zi(Dx);class Ox extends zg{constructor(e){if(super(e),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==e.batchInputShape&&null==e.inputShape){let t=null;null!=e.batchSize&&(t=e.batchSize),null==e.inputLength?this.batchInputShape=[t,null]:this.batchInputShape=[t].concat(Gf(e.inputLength))}this.inputDim=e.inputDim,nm(this.inputDim,"inputDim"),this.outputDim=e.outputDim,nm(this.outputDim,"outputDim"),this.embeddingsInitializer=bg(e.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=Lb(e.embeddingsRegularizer),this.activityRegularizer=Lb(e.activityRegularizer),this.embeddingsConstraint=fm(e.embeddingsConstraint),this.maskZero=e.maskZero,this.supportsMasking=e.maskZero,this.inputLength=e.inputLength}build(e){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(e){}computeMask(e,t){return Xi((()=>this.maskZero?(e=$g(e),Au(e,xl(e))):null))}computeOutputShape(e){if(e=Cg(e),null==this.inputLength)return[...e,this.outputDim];const t=Gf(this.inputLength);if(t.length!==e.length-1)throw new Lf(`"inputLength" is ${this.inputLength}, but received input shape has shape ${e}`);{let n=0;for(let s=0;s<t.length;++s){const r=t[s],a=e[s+1];if(null!=r&&null!=a&&r!==a)throw new Lf(`"inputLength" is ${this.inputLength}, but received input shape has shape ${e}`);null==r&&(t[n]=a),n++}}return[e[0],...t,this.outputDim]}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);let n=$g(e);"int32"!==n.dtype&&(n=Om(n,"int32"));return Hm(this.embeddings.read(),n.as1D()).reshape(Cg(this.computeOutputShape(n.shape)))}))}getConfig(){const e={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:yg(this.embeddingsInitializer),embeddingsRegularizer:Ob(this.embeddingsRegularizer),activityRegularizer:Ob(this.activityRegularizer),embeddingsConstraint:pm(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},t=super.getConfig();return Object.assign(e,t),e}}Ox.className="Embedding",zi(Ox);class Mx extends zg{constructor(e){super(e||{}),this.supportsMasking=!0}mergeFunction(e){throw new zf}computeElementwiseOpOutputShape(e,t){if(null==e||null==t)return null;if(e.length<t.length)return this.computeElementwiseOpOutputShape(t,e);if(0===t.length)return e;const n=e.slice(0,e.length-t.length);for(let s=0;s<t.length;++s){const r=e[e.length-t.length+s],a=t[s];if(null==r||null==a||r<0||a<0)n.push(null);else if(1===r)n.push(a);else if(1===a)n.push(r);else{if(r!==a)throw new Lf("Operands could not be broadcast together with shapes "+JSON.stringify(e)+" "+JSON.stringify(t));n.push(r)}}return n}build(e){if(Array.isArray(e)&&!Array.isArray(e[0])&&(e=[Cg(e)]),(e=e).length<2)throw new Lf(`A merge layer should be called on an Array of at least 2 inputs. Got ${e.length} input(s).`);let t=[];for(const n of e)null!=n&&null!==n[0]&&t.push(n[0]);if(t=Jf(t),t.length>1)throw new Lf(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(e)}.`);let n=null==e[0]?null:e[0].slice(1);for(let t=1;t<e.length;++t){const s=null==e[t]?null:e[t].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=e.map((e=>e.length));-1===e.indexOf(null)&&1===Jf(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(e,t){return Xi((()=>{if(e=e,this.reshapeRequired){const t=[],n=e.map((e=>e.rank));if(-1===n.indexOf(null)){const s=_m(n);for(let n of e){const e=n.rank;for(let t=0;t<s-e;++t)n=Mm(n,1);t.push(n)}return this.mergeFunction(t)}{let n=!1;for(const s of e){const e=s.rank;if(null==e){const e=s.shape,r=e[0],a=e.slice(1).concat([r]);let i=s.reshape([r].concat(Rm(e.slice(1))));i=ri(i,[1,0]),i=i.reshape(a),t.push(i),n=!0}else if(e>1){const r=Dm(1,e).concat([0]);t.push(ri(s,r)),n=!0}else t.push(s)}let s=this.mergeFunction(t);const r=s.rank;if(n)if(null==r){const e=s.shape,t=e[e.length-1],n=[t].concat(e.slice(0,e.length-1));s=ri(s.reshape([-1,t]),[1,0]).reshape(n)}else if(r>1){const e=[r-1].concat(Dm(0,r-1));s=ri(s,e)}return s}}return this.mergeFunction(e)}))}computeOutputShape(e){let t;t=null==(e=e)[0]?null:e[0].slice(1);for(let n=1;n<e.length;++n){const s=null==e[n]?null:e[n].slice(1);t=this.computeElementwiseOpOutputShape(t,s)}let n=[];for(const t of e)null!=t&&null!==t[0]&&n.push(t[0]);return n=Jf(n),t=1===n.length?n.concat(t):[null].concat(t),t}computeMask(e,t){return Xi((()=>{if(null==t)return null;if(!Array.isArray(t))throw new Lf("`mask` should be an Array");if(!Array.isArray(e))throw new Lf("`inputs` should be an Array");if(t.length!==e.length)throw new Lf(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${e.length} vs ${t.length})`);if(t.every((e=>null==e)))return null;let n=(t=t.map((e=>null==e?e:$l(e,0))))[0];for(let e=1;e<t.length-1;++e)n=hu(n,t[e]);return n}))}}class Lx extends Mx{constructor(e){super(e)}mergeFunction(e){return Xi((()=>{let t=e[0].clone();for(let n=1;n<e.length;++n)t=to(t,e[n]);return t}))}}Lx.className="Add",zi(Lx);class zx extends Mx{constructor(e){super(e)}mergeFunction(e){return Xi((()=>{let t=e[0].clone();for(let n=1;n<e.length;++n)t=ro(t,e[n]);return t}))}}zx.className="Multiply",zi(zx);class Bx extends Mx{constructor(e){super(e)}mergeFunction(e){return Xi((()=>{let t=e[0].clone();for(let n=1;n<e.length;++n)t=to(t,e[n]);return ro(1/e.length,t)}))}}Bx.className="Average",zi(Bx);class Px extends Mx{constructor(e){super(e)}mergeFunction(e){return Xi((()=>{let t=e[0];for(let n=1;n<e.length;++n)t=bu(t,e[n]);return t}))}}Px.className="Maximum",zi(Px);class Wx extends Mx{constructor(e){super(e)}mergeFunction(e){return Xi((()=>{let t=e[0];for(let n=1;n<e.length;++n)t=Nu(t,e[n]);return t}))}}Wx.className="Minimum",zi(Wx);class Vx extends Mx{constructor(e){super(e),this.DEFAULT_AXIS=-1,null==e&&(e={}),this.axis=null==e.axis?this.DEFAULT_AXIS:e.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(e){if(!Array.isArray(e)||!Array.isArray(e[0])||1===e.length)throw new Lf("A `Concatenate` layer should be called on a list of at least 2 inputs");e=e;let t=!0;for(const n of e)if(null!=n){t=!1;break}if(t)return;const n=[];for(let t=0;t<e.length;++t){const s=e[t].slice();s.splice(this.axis,1);let r=!1;for(const e of n)if(d(e,s)){r=!0;break}r||n.push(s)}if(n.length>1)throw new Lf("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(e))}mergeFunction(e){return Xi((()=>Pm(e,this.axis)))}computeOutputShape(e){if(!Array.isArray(e)||!Array.isArray(e[0]))throw new Lf("A `Concatenate` layer should be called on a list of inputs.");const t=e,n=t[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const e of t.slice(1)){if(null==n[s]||null==e[s]){n[s]=null;break}n[s]+=e[s]}return n}computeMask(e,t){if(null==t)return null;if(!Array.isArray(t))throw new Lf("`mask` should be an array for Concatenate");if(!Array.isArray(e))throw new Lf("`inputs` should be an array for Concatenate");if(t.length!==e.length)throw new Lf(`Mismatch in the length of mask (${t.length}) and the legnth of inputs (${e.length})`);return Xi((()=>{let n=!0;if(t.forEach((e=>{null==e||(n=!1)})),n)return null;const s=[];for(let n=0;n<e.length;++n)null==t[n]?s.push(Ru(e[n]).asType("bool")):t[n].rank<e[n].rank?s.push($l(t[n],-1)):s.push(t[n]);const r=Oo(s,this.axis);return uo(r,-1,!1)}))}getConfig(){const e={axis:this.axis},t=super.getConfig();return Object.assign(e,t),e}}function Ux(e,t){for(;e<0;)e+=t;return e}Vx.className="Concatenate",zi(Vx);class Gx extends Mx{constructor(e){super(e),this.axes=e.axes,this.normalize=null!=e.normalize&&e.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(e){l(Array.isArray(e)&&2===e.length&&Array.isArray(e[0])&&Array.isArray(e[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const t=e[0],n=e[1];if(t.length>3||n.length>3)throw new zf("Dot layer does not support tensors of 4D or higher rank yet.");const s=this.interpretAxes(t,n);if(t[s[0]]!==n[s[1]])throw new Lf(`Dimension incompatibility: ${t[s[0]]} !== ${n[s[1]]}`)}mergeFunction(e){if(2!==e.length)throw new Lf(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${e.length} input(s).`);let t,n=e[0],s=e[1];return t=Array.isArray(this.axes)?this.axes.map(((t,n)=>Ux(t,e[n].shape.length))):[Ux(this.axes,n.shape.length),Ux(this.axes,s.shape.length)],this.normalize&&(n=ey(n,t[0]),s=ey(s,t[1])),function(e,t,n){if(e.shape.length>3||t.shape.length>3)throw new zf("batchDot is not implemented for tensors of 4D or higher rank yet");if(l(e.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${e.shape.length}`)),l(e.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${t.shape.length}`)),"number"==typeof n&&(n=[n,n]),"complex64"===e.dtype||"complex64"===t.dtype)throw new zf("batchDot is not implemented for complex64-type Tensors yet.");const s=e.shape.length,r=t.shape.length;null==n&&(n=[s-1,r-2]);const a=n;return Xi((()=>{let n,i;if(s>r){n=s-r;const e=[];for(let t=0;t<n;++t)e.push(1);t=t.reshape(t.shape.concat(e))}else if(r>s){n=r-s;const t=[];for(let e=0;e<n;++e)t.push(1);e=e.reshape(e.shape.concat(t))}else n=0;if(2===e.shape.length&&2===t.shape.length)i=a[0]===a[1]?e.mul(t).sum(a[0]):e.transpose([1,0]).mul(t).sum(a[1]);else{const n=a[0]!==e.shape.length-1,s=a[1]===t.shape.length-1;i=e.matMul(t,n,s)}if(n>0){let e;e=s>r?s+r-3:s-1;const t=[];for(let s=e;s<e+n;++s)t.push(s);i=i.squeeze(t)}return 1===i.shape.length&&(i=i.expandDims(1)),i}))}(n,s,t)}interpretAxes(e,t){let n;return n=Array.isArray(this.axes)?this.axes:[Ux(this.axes,e.length),Ux(this.axes,t.length)],n}computeOutputShape(e){l(Array.isArray(e)&&2===e.length&&Array.isArray(e[0])&&Array.isArray(e[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const t=e[0].slice(),n=e[1].slice();if(t.length>3||n.length>3)throw new zf("Dot layer does not support tensors of 4D or higher rank yet.");const s=this.interpretAxes(t,n);t.splice(s[0],1),n.splice(s[1],1),n.splice(0,1);const r=t.concat(n);return 1===r.length&&r.push(1),r}computeMask(e,t){return null}getConfig(){const e={axes:this.axes,normalize:this.normalize},t=super.getConfig();return Object.assign(e,t),e}}Gx.className="Dot",zi(Gx);class Hx extends zg{constructor(e){super(e),this.supportsMasking=!0,this.stddev=e.stddev}computeOutputShape(e){return e}getConfig(){const e=super.getConfig(),t={stddev:this.stddev};return Object.assign(t,e),t}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e);return Ym((()=>Um(n.shape,0,this.stddev).add(n)),(()=>n),t.training||!1)}))}}Hx.className="GaussianNoise",zi(Hx);class jx extends zg{constructor(e){super(e),this.supportsMasking=!0,this.rate=e.rate}computeOutputShape(e){return e}getConfig(){const e=super.getConfig(),t={rate:this.rate};return Object.assign(t,e),t}call(e,t){return Xi((()=>{this.invokeCallHook(e,t);const n=$g(e);if(this.rate>0&&this.rate<1){return Ym((()=>{const e=Math.sqrt(this.rate/(1-this.rate));return n.mul(Um(n.shape,1,e))}),(()=>n),t.training||!1)}return n}))}}jx.className="GaussianDropout",zi(jx);class qx extends zg{constructor(e){super(e),this.supportsMasking=!0,this.rate=e.rate,this.noiseShape=e.noiseShape}_getNoiseShape(e){return this.noiseShape||$g(e).shape}computeOutputShape(e){return e}getConfig(){const e=super.getConfig(),t={rate:this.rate};return Object.assign(t,e),t}call(e,t){return Xi((()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(e);return Ym((()=>{const t=$g(e),s=-1.7580993408473766;let r=Dl(rc(n),this.rate);r=Om(r,"float32");const a=((1-this.rate)*(1+this.rate*s**2))**-.5,i=-a*s*this.rate;return t.mul(r).add(r.add(-1).mul(s)).mul(a).add(i)}),(()=>$g(e)),t.training||!1)}return e}))}}function Kx(e,t,n,s,r,a=.001){let i;if(2===e.rank)i=Vo(e,t,n,s,r,a);else if(3===e.rank)i=Uo(e,t,n,s,r,a);else{if(4!==e.rank)throw new zf(`batchNormalization is not implemented for array of rank ${e.rank} yet`);i=Go(e,t,n,s,r,a)}return i}function Xx(e,t,n,s,r=.001){return d(s.slice().sort(),Dm(0,e.rank-1))?function(e,t,n,s,r=.001){return Xi((()=>{const a=Cu(e,s),i=a.mean,o=a.variance;return[Kx(e,i,o,n,t,r),i,o]}))}(e,t,n,s,r):function(e,t,n,s,r=.001){return Xi((()=>{const a=Cu(e,s),i=a.mean,o=a.variance,l=[];for(const t of Dm(0,e.rank))-1!==s.indexOf(t)?l.push(1):l.push(e.shape[t]);const u=i.reshape(l),c=o.reshape(l),h=null==t?null:t.reshape(l),p=null==n?null:n.reshape(l);return[Kx(e,u,c,p,h,r),i,o]}))}(e,t,n,s,r)}qx.className="AlphaDropout",zi(qx);class Yx extends zg{constructor(e){null==e&&(e={}),super(e),this.supportsMasking=!0,this.axis=null==e.axis?-1:e.axis,this.momentum=null==e.momentum?.99:e.momentum,this.epsilon=null==e.epsilon?.001:e.epsilon,this.center=null==e.center||e.center,this.scale=null==e.scale||e.scale,this.betaInitializer=bg(e.betaInitializer||"zeros"),this.gammaInitializer=bg(e.gammaInitializer||"ones"),this.movingMeanInitializer=bg(e.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=bg(e.movingVarianceInitializer||"ones"),this.betaConstraint=fm(e.betaConstraint),this.gammaConstraint=fm(e.gammaConstraint),this.betaRegularizer=Lb(e.betaRegularizer),this.gammaRegularizer=Lb(e.gammaRegularizer)}build(e){e=Cg(e);const t=this.axis>=0?this.axis:this.axis+e.length,n=e[t];if(null==n)throw new Lf(`Axis ${t} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(e)}.`);this.inputSpec=[new _g({ndim:e.length,axes:{[t]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(e,t){return Xi((()=>{const n=null!=t.training&&t.training,s=$g(e),r=s.shape,a=r.length,i=Dm(0,a),o=this.axis>=0?this.axis:this.axis+a;i.splice(o,1);const l=Pf(1,a);l[o]=r[o];const u=i.slice();u.sort();const c=!d(u,Dm(0,a).slice(0,a-1));if(!n)return(()=>{if(c){const e=this.movingMean.read().reshape(l),t=this.movingVariance.read().reshape(l),n=this.center?this.beta.read().reshape(l):null,r=this.scale?this.gamma.read().reshape(l):null;return Kx(s,e,t,n,r,this.epsilon)}return Kx(s,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[h,p,f]=Xx(s,this.gamma.read(),this.beta.read(),i,this.epsilon),m=(e,t,n)=>{Xi((()=>{const s=1-n,r=e.read(),a=r.sub(t).mul(s);e.write(r.sub(a))}))};return(()=>{m(this.movingMean,p,this.momentum),m(this.movingVariance,f,this.momentum)})(),h}))}getConfig(){const e={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:yg(this.betaInitializer),gammaInitializer:yg(this.gammaInitializer),movingMeanInitializer:yg(this.movingMeanInitializer),movingVarianceInitializer:yg(this.movingVarianceInitializer),betaRegularizer:Ob(this.betaRegularizer),gammaRegularizer:Ob(this.gammaRegularizer),betaConstraint:pm(this.betaConstraint),gammaConstraint:pm(this.gammaConstraint)},t=super.getConfig();return Object.assign(e,t),e}}Yx.className="BatchNormalization",zi(Yx);class Zx extends zg{constructor(e){if(null==e&&(e={}),super(e),this.axis=null==e.axis?-1:e.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const e of this.axis)if(!Number.isInteger(e))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==e.epsilon?.001:e.epsilon,this.center=null==e.center||e.center,this.scale=null==e.scale||e.scale,this.betaInitializer=bg(e.betaInitializer||"zeros"),this.gammaInitializer=bg(e.gammaInitializer||"ones"),this.betaRegularizer=Lb(e.betaRegularizer),this.gammaRegularizer=Lb(e.gammaRegularizer),this.supportsMasking=!0}build(e){const t=(e=Cg(e)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let e=0;e<this.axis.length;++e)this.axis[e]<0&&(this.axis[e]+=t);for(const e of this.axis)if(e<0||e>=t)throw new Error(`Invalid axis: ${e}`);if(this.axis.length!==Jf(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((t=>e[t]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(e,t){const n=$g(e),s=n.shape,r=s.length;return Xi((()=>{let{mean:e,variance:t}=Cu(n,this.axis,!0);const a=Pf(1,r);for(const e of this.axis)a[e]=s[e];const i=e=>null!=e&&e.shape.length!==r&&this.axis!==[r-1]?e.reshape(a):e;let o=i(this.gamma.read()),l=i(this.beta.read());const u=[],c=[];for(let e=0;e<r;++e)-1!==this.axis.indexOf(e)?(u.push(s[e]),c.push(1)):(u.push(1),c.push(s[e]));return e=e.tile(u),t=t.tile(u),o=o.tile(c),l=l.tile(c),Kx(n,e,t,l,o,this.epsilon)}))}getConfig(){const e={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:yg(this.betaInitializer),gammaInitializer:yg(this.gammaInitializer),betaRegularizer:Ob(this.betaRegularizer),gammaRegularizer:Ob(this.gammaRegularizer)},t=super.getConfig();return Object.assign(e,t),e}}Zx.className="LayerNormalization",zi(Zx);class Jx extends zg{constructor(e){if(null==e&&(e={}),super(e),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,null==e.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof e.padding)this.padding=[[e.padding,e.padding],[e.padding,e.padding]];else{if(e.padding=e.padding,2!==e.padding.length)throw new Lf(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${e.padding.length} array.`);let t,n;if("number"==typeof e.padding[0])t=[e.padding[0],e.padding[0]],n=[e.padding[1],e.padding[1]];else{if(e.padding=e.padding,2!==e.padding[0].length)throw new Lf(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${e.padding[0].length} array.`);if(t=e.padding[0],2!==e.padding[1].length)throw new Lf(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${e.padding[1].length} array.`);n=e.padding[1]}this.padding=[t,n]}this.inputSpec=[new _g({ndim:4})]}computeOutputShape(e){let t,n;return e=Cg(e),"channelsFirst"===this.dataFormat?(t=null!=e[2]&&e[2]>=0?e[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=e[3]&&e[3]>=0?e[3]+this.padding[1][0]+this.padding[1][1]:null,[e[0],e[1],t,n]):(t=null!=e[1]&&e[1]>=0?e[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=e[2]&&e[2]>=0?e[2]+this.padding[1][0]+this.padding[1][1]:null,[e[0],t,n,e[3]])}call(e,t){return Xi((()=>{return t=$g(e),n=this.padding,s=this.dataFormat,Xi((()=>{if(4!==t.rank)throw new Lf(`temporalPadding expects input tensor to be 4-D, but received a ${t.rank}-D tensor.`);if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new Lf("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==s&&(s="channelsLast"),"channelsLast"!==s&&"channelsFirst"!==s)throw new Lf(`Unknown data format: ${s}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let e;return e="channelsFirst"===s?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],_u(t,e)}));var t,n,s}))}getConfig(){const e={padding:this.padding,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(e,t),e}}function Qx(e,t,n,s,r,a){return Xi((()=>{let i;vm(r),Im(a),Nm(s),null==n&&(n=[1,1]),null==s&&(s="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),e=qb(e,r);const o="same"===s?"same":"valid";return i="max"===a?mu(e,t,n,o):_o(e,t,n,o),"channelsFirst"===r&&(i=ri(i,[0,3,1,2])),i}))}function ew(e,t,n,s,r,a){return Xi((()=>{let i;vm(r),Im(a),Nm(s),null==n&&(n=[1,1,1]),null==s&&(s="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),e=Kb(e,r);const o="same"===s?"same":"valid";return i="max"===a?gu(e,t,n,o):Do(e,t,n,o),"channelsFirst"===r&&(i=ri(i,[0,4,1,2,3])),i}))}Jx.className="ZeroPadding2D",zi(Jx);class tw extends zg{constructor(e){if(null==e.poolSize&&(e.poolSize=2),super(e),"number"==typeof e.poolSize)this.poolSize=[e.poolSize];else{if(!Array.isArray(e.poolSize)||1!==e.poolSize.length||"number"!=typeof e.poolSize[0])throw new Lf(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(e.poolSize)}`);this.poolSize=e.poolSize}if(nm(this.poolSize,"poolSize"),null==e.strides)this.strides=this.poolSize;else if("number"==typeof e.strides)this.strides=[e.strides];else{if(!Array.isArray(e.strides)||1!==e.strides.length||"number"!=typeof e.strides[0])throw new Lf(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(e.strides)}`);this.strides=e.strides}nm(this.strides,"strides"),this.padding=null==e.padding?"valid":e.padding,Nm(this.padding),this.inputSpec=[new _g({ndim:3})]}computeOutputShape(e){const t=Hb((e=Cg(e))[1],this.poolSize[0],this.padding,this.strides[0]);return[e[0],t,e[2]]}call(e,t){return Xi((()=>{this.invokeCallHook(e,t),e=Mm($g(e),2);const n=this.poolingFunction($g(e),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return Mc(n,[2])}))}getConfig(){const e={poolSize:this.poolSize,padding:this.padding,strides:this.strides},t=super.getConfig();return Object.assign(e,t),e}}class nw extends tw{constructor(e){super(e)}poolingFunction(e,t,n,s,r){return vm(r),Nm(s),Qx(e,t,n,s,r,"max")}}nw.className="MaxPooling1D",zi(nw);class sw extends tw{constructor(e){super(e)}poolingFunction(e,t,n,s,r){return vm(r),Nm(s),Qx(e,t,n,s,r,"avg")}}sw.className="AveragePooling1D",zi(sw);class rw extends zg{constructor(e){if(null==e.poolSize&&(e.poolSize=[2,2]),super(e),this.poolSize=Array.isArray(e.poolSize)?e.poolSize:[e.poolSize,e.poolSize],null==e.strides)this.strides=this.poolSize;else if(Array.isArray(e.strides)){if(2!==e.strides.length)throw new Lf(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${e.strides.length}.`);this.strides=e.strides}else this.strides=[e.strides,e.strides];nm(this.poolSize,"poolSize"),nm(this.strides,"strides"),this.padding=null==e.padding?"valid":e.padding,this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,vm(this.dataFormat),Nm(this.padding),this.inputSpec=[new _g({ndim:4})]}computeOutputShape(e){e=Cg(e);let t="channelsFirst"===this.dataFormat?e[2]:e[1],n="channelsFirst"===this.dataFormat?e[3]:e[2];return t=Hb(t,this.poolSize[0],this.padding,this.strides[0]),n=Hb(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[e[0],e[1],t,n]:[e[0],t,n,e[3]]}call(e,t){return Xi((()=>(this.invokeCallHook(e,t),this.poolingFunction($g(e),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const e={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(e,t),e}}class aw extends rw{constructor(e){super(e)}poolingFunction(e,t,n,s,r){return vm(r),Nm(s),Qx(e,t,n,s,r,"max")}}aw.className="MaxPooling2D",zi(aw);class iw extends rw{constructor(e){super(e)}poolingFunction(e,t,n,s,r){return vm(r),Nm(s),Qx(e,t,n,s,r,"avg")}}iw.className="AveragePooling2D",zi(iw);class ow extends zg{constructor(e){if(null==e.poolSize&&(e.poolSize=[2,2,2]),super(e),this.poolSize=Array.isArray(e.poolSize)?e.poolSize:[e.poolSize,e.poolSize,e.poolSize],null==e.strides)this.strides=this.poolSize;else if(Array.isArray(e.strides)){if(3!==e.strides.length)throw new Lf(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${e.strides.length}.`);this.strides=e.strides}else this.strides=[e.strides,e.strides,e.strides];nm(this.poolSize,"poolSize"),nm(this.strides,"strides"),this.padding=null==e.padding?"valid":e.padding,this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,vm(this.dataFormat),Nm(this.padding),this.inputSpec=[new _g({ndim:5})]}computeOutputShape(e){e=Cg(e);let t="channelsFirst"===this.dataFormat?e[2]:e[1],n="channelsFirst"===this.dataFormat?e[3]:e[2],s="channelsFirst"===this.dataFormat?e[4]:e[3];return t=Hb(t,this.poolSize[0],this.padding,this.strides[0]),n=Hb(n,this.poolSize[1],this.padding,this.strides[1]),s=Hb(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[e[0],e[1],t,n,s]:[e[0],t,n,s,e[4]]}call(e,t){return Xi((()=>(this.invokeCallHook(e,t),this.poolingFunction($g(e),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const e={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(e,t),e}}class lw extends ow{constructor(e){super(e)}poolingFunction(e,t,n,s,r){return vm(r),Nm(s),ew(e,t,n,s,r,"max")}}lw.className="MaxPooling3D",zi(lw);class uw extends ow{constructor(e){super(e)}poolingFunction(e,t,n,s,r){return vm(r),Nm(s),ew(e,t,n,s,r,"avg")}}uw.className="AveragePooling3D",zi(uw);class cw extends zg{constructor(e){super(e),this.inputSpec=[new _g({ndim:3})]}computeOutputShape(e){return[e[0],e[2]]}call(e,t){throw new zf}}class hw extends cw{constructor(e){super(e||{})}call(e,t){return Xi((()=>{const t=$g(e);return xu(t,1)}))}}hw.className="GlobalAveragePooling1D",zi(hw);class pw extends cw{constructor(e){super(e||{})}call(e,t){return Xi((()=>{const t=$g(e);return Jl(t,1)}))}}pw.className="GlobalMaxPooling1D",zi(pw);class dw extends zg{constructor(e){super(e),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,vm(this.dataFormat),this.inputSpec=[new _g({ndim:4})]}computeOutputShape(e){return e=e,"channelsLast"===this.dataFormat?[e[0],e[3]]:[e[0],e[1]]}call(e,t){throw new zf}getConfig(){const e={dataFormat:this.dataFormat},t=super.getConfig();return Object.assign(e,t),e}}class fw extends dw{call(e,t){return Xi((()=>{const t=$g(e);return"channelsLast"===this.dataFormat?xu(t,[1,2]):xu(t,[2,3])}))}}fw.className="GlobalAveragePooling2D",zi(fw);class mw extends dw{call(e,t){return Xi((()=>{const t=$g(e);return"channelsLast"===this.dataFormat?Jl(t,[1,2]):Jl(t,[2,3])}))}}mw.className="GlobalMaxPooling2D",zi(mw);class gw extends zg{constructor(e){super(e),this.layer=e.layer}build(e){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(e){null!=this.layer&&(this.layer.trainable=e)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(e){this.layer.setWeights(e)}getConfig(){const e={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},t=super.getConfig();return Object.assign(e,t),e}setFastWeightInitDuringBuild(e){super.setFastWeightInitDuringBuild(e),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(e)}static fromConfig(e,t,n={}){const s=Qg(t.layer,n);delete t.layer;const r={layer:s};return Object.assign(r,t),new e(r)}}class yw extends gw{constructor(e){super(e),this.supportsMasking=!0}build(e){if((e=Cg(e)).length<3)throw new Lf(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(e)}`);this.inputSpec=[{shape:e}];const t=[e[0]].concat(e.slice(2));this.layer.built||(this.layer.build(t),this.layer.built=!0),super.build(e)}computeOutputShape(e){const t=[(e=Cg(e))[0]].concat(e.slice(2)),n=this.layer.computeOutputShape(t),s=e[1];return[n[0],s].concat(n.slice(1))}call(e,t){return Xi((()=>hx(((e,n)=>[$g(this.layer.call(e,t)),[]]),e=$g(e),[],!1,null,null,!1,!0)[1]))}}yw.className="TimeDistributed",zi(yw);class bw extends gw{constructor(e){super(e);const t=e.layer.getConfig(),n={};n.className=e.layer.getClassName(),n.config=t,this.forwardLayer=Qg(n),t.goBackwards=!0!==t.goBackwards;const s={};var r;if(s.className=e.layer.getClassName(),s.config=t,this.backwardLayer=Qg(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===e.mergeMode?"concat":e.mergeMode,r=this.mergeMode,em(wm,"BidirectionalMergeMode",r),e.weights)throw new zf("weights support is not implemented for Bidirectional layer yet.");this._stateful=e.layer.stateful,this.returnSequences=e.layer.returnSequences,this.returnState=e.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=e.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(e){this._trainable=e,null!=this.forwardLayer&&(this.forwardLayer.trainable=e),null!=this.backwardLayer&&(this.backwardLayer.trainable=e)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(e){const t=e.length,n=Math.floor(t/2);this.forwardLayer.setWeights(e.slice(0,n)),this.backwardLayer.setWeights(e.slice(n))}computeOutputShape(e){let t,n,s,r=this.forwardLayer.computeOutputShape(e);return Array.isArray(r)&&Array.isArray(r[0])||(r=[r]),r=r,this.returnState?(s=r.slice(1),t=r[0]):t=r[0],t=t,"concat"===this.mergeMode?(t[t.length-1]*=2,n=[t]):n=null==this.mergeMode?[t,t.slice()]:[t],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[t].concat(s).concat(s.slice()):Uf(n)}apply(e,t){let n=null==t?null:t.initialState,s=null==t?null:t.constants;null==t&&(t={});const r=cx(e,n,s,this.numConstants);if(e=r.inputs,n=r.initialState,s=r.constants,Array.isArray(e)&&(n=e.slice(1),e=e[0]),(null==n||0===n.length)&&null==s)return super.apply(e,t);const a=[],i=[];if(null!=n){const e=n.length;if(e%2>0)throw new Lf("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");t.initialState=n,a.push(...n);const s=n.map((e=>new _g({shape:e.shape})));this.forwardLayer.stateSpec=s.slice(0,e/2),this.backwardLayer.stateSpec=s.slice(e/2),i.push(...s)}if(null!=s)throw new zf("Support for constants in Bidirectional layers is not implemented yet.");const o=a[0]instanceof Dg;for(const e of a)if(e instanceof Dg!==o)throw new Lf("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[e].concat(a),s=this.inputSpec.concat(i),r=this.inputSpec;this.inputSpec=s;const o=super.apply(n,t);return this.inputSpec=r,o}return super.apply(e,t)}call(e,t){return Xi((()=>{const n=t.initialState;let s,r,a,i;if(null==n)s=this.forwardLayer.call(e,t),r=this.backwardLayer.call(e,t);else{const a=n.slice(0,n.length/2),i=n.slice(n.length/2);s=this.forwardLayer.call(e,Object.assign(t,{initialState:a})),r=this.backwardLayer.call(e,Object.assign(t,{initialState:i}))}return this.returnState&&(Array.isArray(s)&&(a=s.slice(1).concat(r.slice(1))),s=s[0],r=r[0]),this.returnSequences&&(r=cc(r,1)),"concat"===this.mergeMode?i=Pm([s,r]):"sum"===this.mergeMode?i=to(s,r):"ave"===this.mergeMode?i=ro(.5,to(s,r)):"mul"===this.mergeMode?i=ro(s,r):null==this.mergeMode&&(i=[s,r]),this.returnState?null==this.mergeMode?i.concat(a):[i].concat(a):i}))}resetStates(e){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(e){$m(this.forwardLayer.name,(()=>{this.forwardLayer.build(e)})),$m(this.backwardLayer.name,(()=>{this.backwardLayer.build(e)})),this.built=!0}computeMask(e,t){let n;if(Array.isArray(t)&&(t=t[0]),n=this.returnSequences?null==this.mergeMode?[t,t]:t:null==this.mergeMode?[null,null]:null,this.returnState){const e=this.forwardLayer.states.map((e=>null));return Array.isArray(n)?n.concat(e).concat(e):[n].concat(e).concat(e)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(e){super.setFastWeightInitDuringBuild(e),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(e),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(e)}getConfig(){const e={mergeMode:this.mergeMode},t=super.getConfig();return Object.assign(e,t),e}static fromConfig(e,t){const n=Qg(t.layer);if(delete t.layer,null!=t.numConstants)throw new zf("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=t;return s.layer=n,new e(s)}}function xw(e){return new sw(e)}function ww(e){return new iw(e)}function kw(e){return new uw(e)}function vw(e){return new pw(e)}function Nw(e){return new mw(e)}function Iw(e){return new nw(e)}function Sw(e){return new aw(e)}bw.className="Bidirectional",zi(bw);const $w=vw,Cw=Nw,Tw=Iw,Ew=Sw;var Aw=Object.freeze({__proto__:null,inputLayer:function(e){return new Pg(e)},elu:function(e){return new Wb(e)},reLU:function(e){return new zb(e)},leakyReLU:function(e){return new Bb(e)},prelu:function(e){return new Pb(e)},softmax:function(e){return new Ub(e)},thresholdedReLU:function(e){return new Vb(e)},conv1d:function(e){return new ix(e)},conv2d:function(e){return new ex(e)},conv2dTranspose:function(e){return new nx(e)},conv3d:function(e){return new tx(e)},conv3dTranspose:function(e){return new sx(e)},separableConv2d:function(e){return new ax(e)},cropping2D:function(e){return new ox(e)},upSampling2d:function(e){return new lx(e)},depthwiseConv2d:function(e){return new ux(e)},activation:function(e){return new Ax(e)},dense:function(e){return new Tx(e)},dropout:function(e){return new $x(e)},spatialDropout1d:function(e){return new Cx(e)},flatten:function(e){return new Ex(e)},repeatVector:function(e){return new Rx(e)},reshape:function(e){return new Fx(e)},permute:function(e){return new _x(e)},embedding:function(e){return new Ox(e)},add:function(e){return new Lx(e)},average:function(e){return new Bx(e)},concatenate:function(e){return new Vx(e)},maximum:function(e){return new Px(e)},minimum:function(e){return new Wx(e)},multiply:function(e){return new zx(e)},dot:function(e){return new Gx(e)},batchNormalization:function(e){return new Yx(e)},layerNormalization:function(e){return new Zx(e)},zeroPadding2d:function(e){return new Jx(e)},averagePooling1d:xw,avgPool1d:function(e){return xw(e)},avgPooling1d:function(e){return xw(e)},averagePooling2d:ww,avgPool2d:function(e){return ww(e)},avgPooling2d:function(e){return ww(e)},averagePooling3d:kw,avgPool3d:function(e){return kw(e)},avgPooling3d:function(e){return kw(e)},globalAveragePooling1d:function(e){return new hw(e)},globalAveragePooling2d:function(e){return new fw(e)},globalMaxPooling1d:vw,globalMaxPooling2d:Nw,maxPooling1d:Iw,maxPooling2d:Sw,maxPooling3d:function(e){return new lw(e)},gru:function(e){return new yx(e)},gruCell:function(e){return new gx(e)},lstm:function(e){return new xx(e)},lstmCell:function(e){return new bx(e)},simpleRNN:function(e){return new mx(e)},simpleRNNCell:function(e){return new fx(e)},convLstm2d:function(e){return new Sx(e)},convLstm2dCell:function(e){return new Ix(e)},rnn:function(e){return new px(e)},stackedRNNCells:function(e){return new wx(e)},bidirectional:function(e){return new bw(e)},timeDistributed:function(e){return new yw(e)},globalMaxPool1d:$w,globalMaxPool2d:Cw,maxPool1d:Tw,maxPool2d:Ew,Layer:zg,RNN:px,RNNCell:dx,input:pb,gaussianNoise:function(e){return new Hx(e)},gaussianDropout:function(e){return new jx(e)},alphaDropout:function(e){return new qx(e)},masking:function(e){return new Dx(e)}});var Rw=Object.freeze({__proto__:null,binaryAccuracy:function(e,t){return py(e,t)},binaryCrossentropy:function(e,t){return yy(e,t)},sparseCategoricalAccuracy:function(e,t){return by(e,t)},categoricalAccuracy:function(e,t){return dy(e,t)},categoricalCrossentropy:function(e,t){return xy(e,t)},precision:function(e,t){return my(e,t)},recall:function(e,t){return gy(e,t)},cosineProximity:function(e,t){return uy(e,t)},meanAbsoluteError:function(e,t){return ny(e,t)},meanAbsolutePercentageError:function(e,t){return sy(e,t)},MAPE:function(e,t){return sy(e,t)},mape:function(e,t){return sy(e,t)},meanSquaredError:function(e,t){return ty(e,t)},MSE:function(e,t){return ty(e,t)},mse:function(e,t){return ty(e,t)}}),Fw=Object.freeze({__proto__:null,modelFromJSON:async function(e,t){"modelTopology"in e||(e={modelTopology:e});let n=(e=e).modelTopology;null!=n.model_config&&(n=n.model_config);const s=Qg(Fy(n),t);if(null!=e.weightsManifest){const t=await ja(e.weightsManifest,e.pathPrefix,s.weights.map((e=>e.originalName))),n={};for(const e of s.weights)n[e.originalName]=t[e.originalName];s.loadWeights(n),Yi(t)}return s}});var _w=Object.freeze({__proto__:null,l1l2:function(e){return new _b(e)},l1:function(e){return Rb(t=e),new _b({l1:null!=t?t.l1:null,l2:0});var t},l2:function(e){return Rb(t=e),new _b({l2:null!=t?t.l2:null,l1:0});var t}});class Dw extends Hg{constructor(){super(...arguments),this.model=null}setModel(e){if(!(e instanceof lb))throw new Error("model must be a LayersModel, not some other Container");this.model=e}}function Ow(e,t){return e<t}function Mw(e,t){return e>t}class Lw extends Dw{constructor(e){if(super(),null==e&&(e={}),e.restoreBestWeights)throw new zf("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=e.monitor||"val_loss",this.minDelta=Math.abs(e.minDelta||0),this.patience=e.patience||0,this.verbose=e.verbose||0,this.mode=e.mode||"auto",this.baseline=e.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=Ow:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=Mw:this.monitorFunc=Ow,this.monitorFunc===Ow&&(this.minDelta*=-1)}async onTrainBegin(e){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===Ow?1/0:-1/0}async onEpochEnd(e,t){await Vg(t);const n=this.getMonitorValue(t);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=e,this.model.stopTraining=!0)))}async onTrainEnd(e){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(e){null==e&&(e={});const t=e[this.monitor];return null==t&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(e)}`),t}}const zw={earlyStopping:function(e){return new Lw(e)}};var Bw,Pw;!function(e){e[e.DT_INVALID=0]="DT_INVALID",e[e.DT_FLOAT=1]="DT_FLOAT",e[e.DT_DOUBLE=2]="DT_DOUBLE",e[e.DT_INT32=3]="DT_INT32",e[e.DT_UINT8=4]="DT_UINT8",e[e.DT_INT16=5]="DT_INT16",e[e.DT_INT8=6]="DT_INT8",e[e.DT_STRING=7]="DT_STRING",e[e.DT_COMPLEX64=8]="DT_COMPLEX64",e[e.DT_INT64=9]="DT_INT64",e[e.DT_BOOL=10]="DT_BOOL",e[e.DT_QINT8=11]="DT_QINT8",e[e.DT_QUINT8=12]="DT_QUINT8",e[e.DT_QINT32=13]="DT_QINT32",e[e.DT_BFLOAT16=14]="DT_BFLOAT16",e[e.DT_FLOAT_R